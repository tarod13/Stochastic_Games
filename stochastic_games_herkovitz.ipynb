{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stochastic_games_herkovitz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfwiycszBTesGCjMhk3/ef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarod13/Stochastic_Games/blob/master/stochastic_games_herkovitz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5T17o-vEAdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPVt4jdFpH13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta = 0.99\n",
        "\n",
        "M = 5\n",
        "N_P = 2\n",
        "N_A = 2\n",
        "N_S = 3 + N_A * (2*(M+1) + 2)\n",
        "G = [('G')]\n",
        "O = [('O',i) for i in range(1,3)]\n",
        "E1 = [('E1',i,j) for i in range(0,N_A) for j in range(0,M+1)]\n",
        "E2 = [('E2',i,j) for i in range(0,N_A) for j in range(0,M+1)]\n",
        "R1 = [('R1',i) for i in range(0,N_A)]\n",
        "R2 = [('R2',i) for i in range(0,N_A)]\n",
        "S = list(itertools.chain(G, O, E1, E2, R1, R2)) \n",
        "S1 = list(itertools.chain(G, ('O',1), E1, R1))\n",
        "S2 = list(itertools.chain([('O',2), E2, R2]))\n",
        "\n",
        "RG1 = np.array([[3.,0.],[5.,1.]])\n",
        "RG2 = np.array([[3.,5.],[0.,1.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMwIgXp5r-RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class game(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.log_pi1 = nn.ParameterDict()\n",
        "    self.log_pi1['G'] = Parameter(torch.Tensor(N_A,1))\n",
        "    nn.init.zeros_(self.log_pi1['G'])\n",
        "    self.log_pi1[str(('O',1))] = Parameter(torch.Tensor((M+1)*N_A,1))\n",
        "    nn.init.zeros_(self.log_pi1[str(('O',1))])    \n",
        "    self.log_pi1[str(('O',2))] = Parameter(torch.Tensor(1,1))\n",
        "    nn.init.ones_(self.log_pi1[str(('O',2))])\n",
        "\n",
        "    for state in E1:\n",
        "      self.log_pi1[str(state)] = Parameter(torch.Tensor(2,1))\n",
        "      nn.init.zeros_(self.log_pi1[str(state)])\n",
        "      \n",
        "    for state in E2 + R2:\n",
        "      self.log_pi1[str(state)] = Parameter(torch.Tensor(1,1))\n",
        "      nn.init.ones_(self.log_pi1[str(state)])\n",
        "\n",
        "    for state in R1:\n",
        "      self.log_pi1[str(state)] = Parameter(torch.Tensor(N_A,1))\n",
        "      nn.init.zeros_(self.log_pi1[str(state)])\n",
        "\n",
        "    self.log_pi2 = nn.ParameterDict()\n",
        "    self.log_pi2['G'] = Parameter(torch.Tensor(N_A,1))\n",
        "    nn.init.zeros_(self.log_pi2['G'])\n",
        "    self.log_pi2[str(('O',1))] = Parameter(torch.Tensor(1,1))\n",
        "    nn.init.ones_(self.log_pi2[str(('O',1))])\n",
        "    self.log_pi2[str(('O',2))] = Parameter(torch.Tensor((M+1)*N_A,1))\n",
        "    nn.init.zeros_(self.log_pi2[str(('O',2))])\n",
        "\n",
        "    for state in E2:\n",
        "      self.log_pi2[str(state)] = Parameter(torch.Tensor(2,1))\n",
        "      nn.init.zeros_(self.log_pi2[str(state)])\n",
        "      \n",
        "    for state in E1 + R1:\n",
        "      self.log_pi2[str(state)] = Parameter(torch.Tensor(1,1))\n",
        "      nn.init.ones_(self.log_pi2[str(state)])\n",
        "\n",
        "    for state in R2:\n",
        "      self.log_pi2[str(state)] = Parameter(torch.Tensor(N_A,1))\n",
        "      nn.init.zeros_(self.log_pi2[str(state)])\n",
        "\n",
        "    self.v = Parameter(torch.Tensor(N_S,2))\n",
        "    nn.init.zeros_(self.v)    \n",
        "\n",
        "    self.sqrt_lambda_nash1 = nn.ParameterDict()\n",
        "    self.sqrt_lambda_nash2 = nn.ParameterDict()  # dual vars. for Nash inequalities\n",
        "    for s in S:\n",
        "        self.sqrt_lambda_nash1[str(s)] = Parameter(torch.Tensor(self.log_pi1[str(s)].detach().size()))\n",
        "        self.sqrt_lambda_nash2[str(s)] = Parameter(torch.Tensor(self.log_pi2[str(s)].detach().size()))\n",
        "        nn.init.zeros_(self.sqrt_lambda_nash1[str(s)])\n",
        "        #self.sqrt_lambda_nash1[str(s)].data.mul_(1.0)\n",
        "        nn.init.zeros_(self.sqrt_lambda_nash2[str(s)])\n",
        "        #self.sqrt_lambda_nash2[str(s)].data.mul_(1.0)\n",
        "\n",
        "  def forward(self):\n",
        "    return self.pi(), self.v, self.lambda_nash()\n",
        "  \n",
        "  def pi(self):\n",
        "    pi = {'1':{}, '2':{}}\n",
        "    for s in S:\n",
        "      log_pi1 = self.log_pi1[str(s)]\n",
        "      log_pi2 = self.log_pi2[str(s)]\n",
        "      \n",
        "      log_pi1_shift = log_pi1 - log_pi1.max()\n",
        "      log_pi2_shift = log_pi2 - log_pi2.max()\n",
        "\n",
        "      pi1 = torch.exp(log_pi1_shift + 1e-20)\n",
        "      pi2 = torch.exp(log_pi2_shift + 1e-20)\n",
        "      \n",
        "      pi['1'][str(s)] = pi1 / pi1.sum()\n",
        "      pi['2'][str(s)] = pi2 / pi2.sum()\n",
        "    return pi\n",
        "  \n",
        "  def lambda_nash(self):\n",
        "    lambda_ = {'1':{}, '2':{}}\n",
        "    for s in S:\n",
        "      lambda_['1'][str(s)] = self.sqrt_lambda_nash1[str(s)].pow(2)\n",
        "      lambda_['2'][str(s)] = self.sqrt_lambda_nash2[str(s)].pow(2)\n",
        "    return lambda_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_3Y5tpQGui5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game_b = game().to('cuda')\n",
        "optimizer = optim.Adam(game_b.parameters(), lr=1e-3)\n",
        "\n",
        "N_A_S = {}\n",
        "with torch.no_grad():\n",
        "  pi, v, lambda_nash = game_b()\n",
        "  for state in S:\n",
        "    N_A1_state = pi['1'][str(state)].shape[0]\n",
        "    N_A2_state = pi['2'][str(state)].shape[0]\n",
        "    N_A_S[str(state)] = {'1': N_A1_state, '2': N_A2_state}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ChZjFIvavkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def offer_accepted(action):\n",
        "  if action == 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "  \n",
        "\n",
        "def other_player(i):\n",
        "  if i == 1:\n",
        "    return 2\n",
        "  elif i == 2:\n",
        "    return 1\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player id'\n",
        "\n",
        "\n",
        "def transition_info(state, actions):\n",
        "  if state == 'G' or 'R1' in state or 'R2' in state:\n",
        "    T = [(('O',1), 0.5), (('O',2), 0.5)]\n",
        "    return ('R', T)\n",
        "  elif 'O' in state:\n",
        "    _, id_player = state\n",
        "    offeral = actions[id_player-1] // N_A\n",
        "    action_requested = actions[id_player-1] % N_A\n",
        "    return ('D', ('E'+str(other_player(id_player)), action_requested, offeral))\n",
        "  elif 'E1' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      return ('D', ('R2', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G'))\n",
        "  elif 'E2' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      return ('D', ('R1', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G'))   \n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pjuaTnWdlwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transition_types = {}\n",
        "for state in S:\n",
        "  if state == 'G' or 'R1' in state or 'R2' in state:\n",
        "    transition_types[str(state)] = (0,0) \n",
        "  else:\n",
        "    transition_types[str(state)] = (1,1) # entries correspond to deterministic behaviour and dependence on actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ANZA1cKdIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rewards(state, actions):\n",
        "  if state == 'G':\n",
        "    r1 = RG1[actions[0], actions[1]]\n",
        "    r2 = RG2[actions[0], actions[1]]\n",
        "  elif 'O' in state:\n",
        "    r1 = r2 = 0.0\n",
        "  elif 'E1' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      r1 = offeral\n",
        "      r2 = -offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'E2' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      r1 = -offeral\n",
        "      r2 = offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'R1' in state:\n",
        "    _, i = state\n",
        "    r1 = RG1[actions[0], i]\n",
        "    r2 = RG2[actions[0], i]\n",
        "  elif 'R2' in state:\n",
        "    _, i = state\n",
        "    r1 = RG1[i, actions[1]]\n",
        "    r2 = RG2[i, actions[1]]\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state'\n",
        "\n",
        "  return r1, r2\n",
        "\n",
        "\n",
        "def reward_matrices(s):\n",
        "  if s == 'G':\n",
        "    RM1 = RG1.copy()\n",
        "    RM2 = RG2.copy()\n",
        "  else:\n",
        "    N_A1 = N_A_S[str(s)]['1']\n",
        "    N_A2 = N_A_S[str(s)]['2']\n",
        "    RM1 = np.zeros((N_A1,N_A2))\n",
        "    RM2 = np.zeros((N_A1,N_A2))\n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        r1, r2 = rewards(s, [a1,a2])\n",
        "        RM1[a1,a2] = r1\n",
        "        RM2[a1,a2] = r2\n",
        "  return RM1, RM2  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSMeXOcIT0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RM = {}\n",
        "RM['1'] = {}\n",
        "RM['2'] = {}\n",
        "for s in S:\n",
        "  RM1, RM2 = reward_matrices(s)\n",
        "  RM['1'][str(s)] = torch.FloatTensor(RM1).to('cuda')\n",
        "  RM['2'][str(s)] = torch.FloatTensor(RM2).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMPtyopfi3fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_state_index(state):\n",
        "  return S.index(state)\n",
        "\n",
        "def player_dim(i):\n",
        "  if i == 1:\n",
        "    return 'i'\n",
        "  elif i == 2:\n",
        "    return 'j'\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player id'\n",
        "\n",
        "def players():\n",
        "  return range(1,2+1)\n",
        "\n",
        "def players_str():\n",
        "  return iter(['1','2']) \n",
        "\n",
        "def state_player_pairs():\n",
        "  return itertools.product(S, players())\n",
        "\n",
        "S_str = [str(s) for s in S]\n",
        "def state_player_str_pairs():\n",
        "  return itertools.product(S_str, players_str())\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def next_value_matrices(s, v): # TODO: consider other 2 cases\n",
        "  det, dep = transition_types[str(s)]\n",
        "  N_A1 = N_A_S[str(s)]['1']\n",
        "  N_A2 = N_A_S[str(s)]['2']\n",
        "  vs = torch.zeros((N_A1,N_A2,2)).to('cuda')\n",
        "  if det and dep:\n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        _, next_state = transition_info(s, [a1,a2])\n",
        "        vs[a1,a2,:] = v[get_state_index(next_state),:]\n",
        "  elif (not det) and (not dep):\n",
        "    _, transition_dic = transition_info(s, [])\n",
        "    next_v = torch.zeros(1,2).to('cuda')\n",
        "    for next_state, transition_prob in transition_dic:\n",
        "      next_v = next_v + v[get_state_index(next_state),:].view(1,-1) * transition_prob      \n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        vs[a1,a2,:] = next_v.view(-1)\n",
        "  return vs \n",
        "\n",
        "\n",
        "def transition_matrix(pi): # TODO: consider other 2 cases\n",
        "  transition_matrix = torch.zeros((N_S,N_S)).to('cuda')\n",
        "  for state in S:\n",
        "    strategy_1 = pi['1'][str(state)]\n",
        "    strategy_2 = pi['2'][str(state)]\n",
        "\n",
        "    det, dep = transition_types[str(state)]\n",
        "    N_A1 = strategy_1.shape[0]\n",
        "    N_A2 = strategy_2.shape[0]\n",
        "    id_s = get_state_index(state)\n",
        "\n",
        "    if det and dep:\n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          _, next_state = transition_info(state, [a1,a2])\n",
        "          id_ns = get_state_index(next_state)\n",
        "          transition_prob = strategy_1[a1,0] * strategy_2[a2,0]\n",
        "          transition_matrix[id_s, id_ns] = transition_matrix[id_s, id_ns] + transition_prob\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = transition_info(state, [])\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        transition_matrix[id_s, get_state_index(next_state)] = transition_prob\n",
        "  return transition_matrix\n",
        "\n",
        "\n",
        "def partial_transition_matrices(pi): # TODO: consider other 2 cases\n",
        "  transition_matrices = {}\n",
        "  for s in S:\n",
        "    strategy_1 = pi['1'][str(s)]\n",
        "    strategy_2 = pi['2'][str(s)]\n",
        "    N_A1 = strategy_1.shape[0]\n",
        "    N_A2 = strategy_2.shape[0]\n",
        "    transition_matrices[str(s)] = {\n",
        "        '1': torch.zeros((N_A1,N_S)).to('cuda'),\n",
        "        '2': torch.zeros((N_A2,N_S)).to('cuda')\n",
        "    }\n",
        "\n",
        "    det, dep = transition_types[str(s)]\n",
        "    if det and dep:\n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          _, next_state = transition_info(s, [a1,a2])\n",
        "          id_ns = get_state_index(next_state)\n",
        "          transition_prob1 = strategy_2[a2,0]\n",
        "          transition_prob2 = strategy_1[a1,0]\n",
        "          transition_matrices[str(s)]['1'][a1, id_ns] = (\n",
        "              transition_matrices[str(s)]['1'][a1, id_ns] + transition_prob1)\n",
        "          transition_matrices[str(s)]['2'][a2, id_ns] = (\n",
        "              transition_matrices[str(s)]['2'][a2, id_ns] + transition_prob2)\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = transition_info(s, [])\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        transition_matrices[str(s)]['1'][:, get_state_index(next_state)] = transition_prob\n",
        "        transition_matrices[str(s)]['2'][:, get_state_index(next_state)] = transition_prob\n",
        "  return transition_matrices\n",
        "\n",
        "\n",
        "def expected_reward(RM, pi):\n",
        "  r_mean = torch.zeros((N_S,2)).to('cuda')\n",
        "  for i in range(1,2+1):\n",
        "    RM_i = RM[str(i)]\n",
        "    for s in S:\n",
        "      strategy_1 = pi['1'][str(s)]\n",
        "      strategy_2 = pi['2'][str(s)]\n",
        "      r_mean_1 = torch.einsum('ij,ik->jk', RM_i[str(s)], strategy_1)\n",
        "      r_mean[get_state_index(s),i-1] = (r_mean_1 * strategy_2).sum()\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def partial_expected_reward(RM, pi):\n",
        "  r_mean = {}\n",
        "  for player_id in range(1,2+1):  \n",
        "    r_mean[str(player_id)] = {}  \n",
        "    for s in S:\n",
        "      N_A = N_A_S[str(s)][str(player_id)]\n",
        "      RM_i = RM[str(player_id)]\n",
        "      other_player_id = other_player(player_id)\n",
        "      strategy = pi[str(other_player_id)][str(s)].view(-1)\n",
        "      formula = 'ij,'+player_dim(other_player_id)+'->'+player_dim(player_id)\n",
        "      r_mean[str(player_id)][str(s)] = torch.einsum(formula, RM_i[str(s)], strategy).view(-1,1)\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def bellman_projection(RM, pi, v):\n",
        "  r_mean = expected_reward(RM, pi)\n",
        "  next_v = torch.zeros((N_S,2)).to('cuda')\n",
        "  for s in S:\n",
        "    next_state_action_value = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    next_value_1 = torch.einsum('ijk,i->jk', next_state_action_value, strategy_1)\n",
        "    next_v[get_state_index(s),:] = torch.einsum('jk,j->k', next_value_1, strategy_2)\n",
        "  return r_mean + beta * next_v\n",
        "\n",
        "\n",
        "def bellman_partial_projection(RM, pi, v):\n",
        "  r_mean = partial_expected_reward(RM, pi)\n",
        "  next_v = {'1':{}, '2':{}}\n",
        "  for s in S:\n",
        "    next_state_action_value = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    # mean next value when considering the strategy of the other player. Output: array of size m^i(s)\n",
        "    next_v['1'][str(s)] = torch.einsum('ij,j->i', next_state_action_value[:,:,0], strategy_2).view(-1,1)\n",
        "    next_v['2'][str(s)] = torch.einsum('ij,i->j', next_state_action_value[:,:,1], strategy_1).view(-1,1)\n",
        "  bellman_projection_dic = {'1':{}, '2':{}}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    bellman_projection_dic[str(player_id)][str(s)] = r_mean[str(player_id)][str(s)] + beta * next_v[str(player_id)][str(s)]\n",
        "  return bellman_projection_dic\n",
        "\n",
        "\n",
        "def cost_vector_fixed_policies(pi):\n",
        "  P = transition_matrix(pi)\n",
        "  cost_vector = (1 - beta * P.sum(0)).view(-1,1).detach().cpu().numpy() / N_S\n",
        "  return cost_vector\n",
        "\n",
        "def reward_baselines(RM, pi):\n",
        "  r_mean = expected_reward(RM, pi)\n",
        "  r_baseline = r_mean.mean(0).view(-1,1).detach().cpu().numpy()\n",
        "  return r_baseline\n",
        "\n",
        "def restriction_matrices_fixed_policies(pi):\n",
        "  transition_matrices = partial_transition_matrices(pi)\n",
        "  restriction_matrices = {'1':[], '2':[]}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    temp_matrix = - beta * transition_matrices[str(s)][str(player_id)]\n",
        "    temp_matrix[:, get_state_index(s)] = temp_matrix[:, get_state_index(s)] + 1\n",
        "    restriction_matrices[str(player_id)].append(temp_matrix)\n",
        "  for player in players_str():\n",
        "    restriction_matrices[player] = -torch.cat(restriction_matrices[player], dim=0).detach().cpu().numpy()\n",
        "  return restriction_matrices\n",
        "\n",
        "def restriction_vectors_fixed_policies(RM, pi, alpha=0.1):\n",
        "  r_mean = partial_expected_reward(RM, pi)\n",
        "  restriction_vectors = {'1':[], '2':[]}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    restriction_vectors[player].append(r_mean[player][s].view(-1,1))\n",
        "  for player in players_str():\n",
        "    restriction_vectors[player] = -(torch.cat(restriction_vectors[player], dim=0)+alpha).detach().cpu().numpy()\n",
        "  return restriction_vectors\n",
        "\n",
        "\n",
        "def parameters_fixed_policies(game_, alpha):\n",
        "  pi = game_()[0]\n",
        "  c = cost_vector_fixed_policies(pi)\n",
        "  f0 = reward_baselines(RM, pi)\n",
        "  A_ub = restriction_matrices_fixed_policies(pi)\n",
        "  b_ub = restriction_vectors_fixed_policies(RM, pi, alpha)\n",
        "  return c, f0, A_ub, b_ub\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_nash_restrictions(pi, v):\n",
        "  q_estimated = bellman_partial_projection(RM, pi, v) # Dic. with an array of 'q'-values for each agent \n",
        "  g_nash = {'1':{}, '2':{}}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    g_nash[str(player_id)][str(s)] = q_estimated[str(player_id)][str(s)] - v[get_state_index(s), player_id-1]\n",
        "  return g_nash\n",
        "\n",
        "\n",
        "def lagrangian_nash(pi, v, lambda_nash, c=1):\n",
        "  # Calculation of original target function: Bellman approximation error \n",
        "  v_estimated = bellman_projection(RM, pi, v)\n",
        "  f_bellman = (v - v_estimated).sum()\n",
        "\n",
        "  # Calculation of restrictions\n",
        "  g_nash = calculate_nash_restrictions(pi, v)\n",
        "\n",
        "  # Calculation of shifted restrictions  \n",
        "  nash_restriction_products = {'1':{}, '2':{}}  \n",
        "  for s, player_id in state_player_str_pairs():\n",
        "    nash_restriction_products[player_id][s] = (g_nash[player_id][s].view(-1,1) * lambda_nash[player_id][s].view(-1,1)) \n",
        "                                                #+ c * g_nash[player_id][s].view(-1,1).pow(2) * lambda_nash[player_id][s].view(-1,1).detach())\n",
        "    \n",
        "  # Calculation of 2-norms  \n",
        "  nash_restriction_sum = 0.0\n",
        "  for s, player_id in state_player_str_pairs():\n",
        "    nash_restriction_sum = nash_restriction_sum + nash_restriction_products[player_id][s].sum()\n",
        "    \n",
        "  # Calculation of augmented lagrangian\n",
        "  lagrangian = f_bellman + nash_restriction_sum\n",
        "  \n",
        "  return lagrangian, f_bellman\n",
        "\n",
        "\n",
        "def check_KKT_conditions(pi, v, lambda_nash, tol=1e-8):\n",
        "  with torch.no_grad():\n",
        "    # Calculate restrictions\n",
        "    g_nash = calculate_nash_restrictions(pi, v)\n",
        "\n",
        "    g_nash_satisfied = True\n",
        "    product_zero_satisfied = True\n",
        "    \n",
        "    max_g_nash = -np.infty\n",
        "    max_product_zero = -np.infty\n",
        "    \n",
        "    for s, player_id in state_player_str_pairs():\n",
        "      g_nash_satisfied = g_nash_satisfied and torch.all(g_nash[player_id][s] <= 0)\n",
        "      max_g_nash = max(max_g_nash, g_nash[player_id][s].max())\n",
        "\n",
        "      lambda_g_nash_product = g_nash[player_id][s].view(-1) * lambda_nash[player_id][s].view(-1)\n",
        "      product_zero_satisfied = product_zero_satisfied and torch.all(lambda_g_nash_product.abs() <= tol)\n",
        "      max_product_zero = max(max_product_zero, lambda_g_nash_product.abs().max())\n",
        "\n",
        "    return g_nash_satisfied, max_g_nash, product_zero_satisfied, max_product_zero\n",
        "\n",
        "\n",
        "def check_nash_conditions(pi, v, lambda_nash):\n",
        "  with torch.no_grad():\n",
        "    # Calculate restrictions\n",
        "    g_nash = calculate_nash_restrictions(pi, v)\n",
        "    g_nash_satisfied = True\n",
        "    max_g_nash = -np.infty\n",
        "    \n",
        "    for s, player_id in state_player_str_pairs():\n",
        "      g_nash_satisfied = g_nash_satisfied and torch.all(g_nash[player_id][s] <= 0)\n",
        "      max_g_nash = max(max_g_nash, g_nash[player_id][s].max())\n",
        "\n",
        "    return g_nash_satisfied, max_g_nash\n",
        "  \n",
        "\n",
        "def optimize_game(game_, n_epochs, optimizers, params_dual, \n",
        "                  print_each=50, save_each=10):\n",
        "  losses = []\n",
        "  costs = []\n",
        "  delta_cost = 0.0\n",
        "  previous_cost = np.infty\n",
        "  opti_primal, opti_dual = optimizers\n",
        "  \n",
        "  for epoch in range(0, n_epochs):\n",
        "    pi, v, lambda_nash = game_()\n",
        "    loss, cost = lagrangian_nash(pi, v, lambda_nash)\n",
        "    delta_cost = cost.item() - previous_cost\n",
        "\n",
        "    previous_cost = cost.item()\n",
        "    torch.save(game_.state_dict(), './game_temp.pth')\n",
        "\n",
        "    opti_primal.zero_grad()\n",
        "    opti_dual.zero_grad()    \n",
        "    loss.backward()\n",
        "    \n",
        "    for p in params_dual:\n",
        "      if p.grad is not None:\n",
        "        p.grad.data.mul_(-1)\n",
        "\n",
        "    opti_primal.step()\n",
        "    opti_dual.step()\n",
        "\n",
        "    if (epoch + 1) % print_each == 0:\n",
        "      with torch.no_grad():\n",
        "        nash_satisfied, max_nash_restrictions, KKT_zero_product_satisfied, max_KKT_product = check_KKT_conditions(pi, v, lambda_nash)\n",
        "      print(\"Epoch: {}, Loss: {:.3f}, f: {:.3f}, nash: {}, max g: {:.3f}, KKT product: {}, max product: {:.3e}\".format(\n",
        "          epoch+1,loss.item(),cost.item(), nash_satisfied, max_nash_restrictions,\n",
        "          KKT_zero_product_satisfied, max_KKT_product))\n",
        "\n",
        "    if (epoch + 1) % save_each == 0:\n",
        "      losses.append(loss.item())\n",
        "      costs.append(cost.item())\n",
        "  return losses, costs\n",
        "\n",
        "def calculate_initial_v(game_):\n",
        "  v0 = np.zeros((N_S,2))\n",
        "  c, f0, A_ub, b_ub = parameters_fixed_policies(game_, alpha=10)\n",
        "  for player_id in players():\n",
        "    temp_res = linprog(c, A_ub=A_ub[str(player_id)], b_ub=b_ub[str(player_id)])\n",
        "    v0[:,player_id-1] = temp_res.x\n",
        "  return v0\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def nash_error_gradient(pi):\n",
        "  P = transition_matrix(pi)\n",
        "  nash_error_grad = (torch.eye(N_S).to('cuda') - torch.t(P)).sum(1, keepdim=True)\n",
        "  return nash_error_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbiikizT9KyT",
        "colab_type": "text"
      },
      "source": [
        "#**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLlRZTiQAgWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game_b = game().to('cuda')\n",
        "params_primal = [game_b.v] + list(game_b.log_pi1.parameters()) + list(game_b.log_pi2.parameters())\n",
        "params_dual = list(game_b.sqrt_lambda_nash1.parameters()) + list(game_b.sqrt_lambda_nash2.parameters())\n",
        "optim_primal = optim.Adam(params_primal, lr=1e-3)\n",
        "optim_dual = optim.Adam(params_dual, lr=1e-4)\n",
        "optimizers = [optim_primal, optim_dual]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnKgBgmSQuEE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "0e0dc0af-4f82-4c37-9e13-cac8d1dd8ee9"
      },
      "source": [
        "v0 = calculate_initial_v(game_b)\n",
        "v0 = torch.FloatTensor(v0).to('cuda')\n",
        "game_b.v.data.add_(v0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1163.1011, 1163.1011],\n",
              "        [1160.3406, 1161.0554],\n",
              "        [1161.0554, 1160.3406],\n",
              "        [1161.4701, 1161.9601],\n",
              "        [1161.4701, 1161.4601],\n",
              "        [1161.4701, 1160.9601],\n",
              "        [1161.9851, 1160.4601],\n",
              "        [1162.9851, 1159.9601],\n",
              "        [1163.9851, 1159.4601],\n",
              "        [1161.4701, 1159.9801],\n",
              "        [1161.4701, 1159.4801],\n",
              "        [1162.4701, 1158.9801],\n",
              "        [1163.4701, 1158.4801],\n",
              "        [1164.4701, 1157.9801],\n",
              "        [1165.4701, 1157.4801],\n",
              "        [1161.9601, 1161.4701],\n",
              "        [1161.4601, 1161.4701],\n",
              "        [1160.9601, 1161.4701],\n",
              "        [1160.4601, 1161.9851],\n",
              "        [1159.9601, 1162.9851],\n",
              "        [1159.4601, 1163.9851],\n",
              "        [1159.9801, 1161.4701],\n",
              "        [1159.4801, 1161.4701],\n",
              "        [1158.9801, 1162.4701],\n",
              "        [1158.4801, 1163.4701],\n",
              "        [1157.9801, 1164.4701],\n",
              "        [1157.4801, 1165.4701],\n",
              "        [1164.0911, 1160.5911],\n",
              "        [1160.0911, 1162.0911],\n",
              "        [1160.5911, 1164.0911],\n",
              "        [1162.0911, 1160.0911]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYaXbFwlYldO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.5\n",
        "gamma_0 = 0.5\n",
        "eta = 1e-2\n",
        "nu = 1/0.9999\n",
        "rho_0 = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eDk9g9ScGFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  pi,v,_ = game_b()\n",
        "  grad_f_v = nash_error_gradient(pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8LcBEke_dRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses, costs = optimize_game(game_b, 20000, optimizers, params_dual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeRE3bhKtsYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses2, costs2 = optimize_game(game_b, 23000, optimizers, params_dual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYtRrLbKyHJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dloss = np.array(losses[1:])-np.array(losses[:-1])\n",
        "dcost = np.array(costs[1:])-np.array(costs[:-1])\n",
        "\n",
        "fig, ax = plt.subplots(2,2, figsize=(12,8))\n",
        "ax[0,0].plot(np.array(losses))\n",
        "ax[0,1].plot(np.array(costs))\n",
        "ax[1,0].plot(np.array(dloss))\n",
        "ax[1,1].plot(np.array(dcost))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(np.argmax(dloss), np.argmax(dcost))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snsijRf_2Vb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  pi, v = game_b()\n",
        "lambda_nash = update_nash_dual_variables(pi, v, lambda_nash)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8xVRb-v2e3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses2, costs2 = optimize_policy_values(game_b, [lambda_nash], 2000, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmR8ZqICDnIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dloss2 = np.array(losses2[1:])-np.array(losses2[:-1])\n",
        "dcost2 = np.array(costs2[1:])-np.array(costs2[:-1])\n",
        "\n",
        "fig, ax = plt.subplots(2,2, figsize=(12,8))\n",
        "ax[0,0].plot(np.array(losses2))\n",
        "ax[0,1].plot(np.array(costs2))\n",
        "ax[1,0].plot(np.array(dloss2))\n",
        "ax[1,1].plot(np.array(dcost2))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(np.argmax(dloss2), np.argmax(dcost2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmX6sVj0I5WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(game_b.state_dict(), './game.pth')\n",
        "pickle.dump(lambda_nash, open('./lambda_nash.p', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVd34_FA491I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  pi, v = game_b()\n",
        "lambda_nash = update_nash_dual_variables(pi, v, lambda_nash)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}