{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stochastic_games_herkovitz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarod13/Stochastic_Games/blob/master/stochastic_games_herkovitz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5T17o-vEAdt"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPVt4jdFpH13"
      },
      "source": [
        "beta = 0.99\n",
        "\n",
        "M = 5\n",
        "N_P = 2\n",
        "N_A = 2\n",
        "N_S = 3 + N_A * (2*(M+1) + 2)\n",
        "G = [('G',)]\n",
        "O = [('O',i) for i in range(1,3)]\n",
        "E1 = [('E1',i,j) for i in range(0,N_A) for j in range(0,M+1)]\n",
        "E2 = [('E2',i,j) for i in range(0,N_A) for j in range(0,M+1)]\n",
        "R1 = [('R1',i) for i in range(0,N_A)]\n",
        "R2 = [('R2',i) for i in range(0,N_A)]\n",
        "S = list(itertools.chain(G, O, E1, E2, R1, R2)) \n",
        "S1 = list(itertools.chain(G, ('O',1), E1, R1))\n",
        "S2 = list(itertools.chain([('O',2), E2, R2]))\n",
        "\n",
        "RG1 = np.array([[3.,0.],[5.,1.]])\n",
        "RG2 = np.array([[3.,5.],[0.,1.]])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMwIgXp5r-RT"
      },
      "source": [
        "class game(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.log_pi1 = nn.ParameterDict()\n",
        "    self.log_pi1[str(('G',))] = Parameter(torch.Tensor(N_A,1))\n",
        "    nn.init.zeros_(self.log_pi1[str(('G',))])\n",
        "    self.log_pi1[str(('O',1))] = Parameter(torch.Tensor((M+1)*N_A,1))\n",
        "    nn.init.zeros_(self.log_pi1[str(('O',1))])    \n",
        "    self.log_pi1[str(('O',2))] = Parameter(torch.Tensor(1,1))\n",
        "    nn.init.ones_(self.log_pi1[str(('O',2))])\n",
        "\n",
        "    for state in E1:\n",
        "      self.log_pi1[str(state)] = Parameter(torch.Tensor(2,1))\n",
        "      nn.init.zeros_(self.log_pi1[str(state)])\n",
        "      \n",
        "    for state in E2 + R2:\n",
        "      self.log_pi1[str(state)] = Parameter(torch.Tensor(1,1))\n",
        "      nn.init.ones_(self.log_pi1[str(state)])\n",
        "\n",
        "    for state in R1:\n",
        "      self.log_pi1[str(state)] = Parameter(torch.Tensor(N_A,1))\n",
        "      nn.init.zeros_(self.log_pi1[str(state)])\n",
        "\n",
        "    self.log_pi2 = nn.ParameterDict()\n",
        "    self.log_pi2[str(('G',))] = Parameter(torch.Tensor(N_A,1))\n",
        "    nn.init.zeros_(self.log_pi2[str(('G',))])\n",
        "    self.log_pi2[str(('O',1))] = Parameter(torch.Tensor(1,1))\n",
        "    nn.init.ones_(self.log_pi2[str(('O',1))])\n",
        "    self.log_pi2[str(('O',2))] = Parameter(torch.Tensor((M+1)*N_A,1))\n",
        "    nn.init.zeros_(self.log_pi2[str(('O',2))])\n",
        "\n",
        "    for state in E2:\n",
        "      self.log_pi2[str(state)] = Parameter(torch.Tensor(2,1))\n",
        "      nn.init.zeros_(self.log_pi2[str(state)])\n",
        "      \n",
        "    for state in E1 + R1:\n",
        "      self.log_pi2[str(state)] = Parameter(torch.Tensor(1,1))\n",
        "      nn.init.ones_(self.log_pi2[str(state)])\n",
        "\n",
        "    for state in R2:\n",
        "      self.log_pi2[str(state)] = Parameter(torch.Tensor(N_A,1))\n",
        "      nn.init.zeros_(self.log_pi2[str(state)])\n",
        "\n",
        "    self.v = Parameter(torch.Tensor(N_S,2))\n",
        "    nn.init.zeros_(self.v)    \n",
        "\n",
        "    self.sqrt_lambda_nash1 = nn.ParameterDict()\n",
        "    self.sqrt_lambda_nash2 = nn.ParameterDict()  # dual vars. for Nash inequalities\n",
        "    for s in S:\n",
        "        self.sqrt_lambda_nash1[str(s)] = Parameter(torch.Tensor(self.log_pi1[str(s)].detach().size()))\n",
        "        self.sqrt_lambda_nash2[str(s)] = Parameter(torch.Tensor(self.log_pi2[str(s)].detach().size()))\n",
        "        nn.init.zeros_(self.sqrt_lambda_nash1[str(s)])\n",
        "        #self.sqrt_lambda_nash1[str(s)].data.mul_(1.0)\n",
        "        nn.init.zeros_(self.sqrt_lambda_nash2[str(s)])\n",
        "        #self.sqrt_lambda_nash2[str(s)].data.mul_(1.0)\n",
        "\n",
        "    for param in self.parameters():  \n",
        "      nn.init.uniform_(param)\n",
        "\n",
        "  def forward(self):\n",
        "    return self.pi(), self.v, self.lambda_nash()\n",
        "  \n",
        "  def pi(self):\n",
        "    pi = {'1':{}, '2':{}}\n",
        "    for s in S:\n",
        "      log_pi1 = self.log_pi1[str(s)]\n",
        "      log_pi2 = self.log_pi2[str(s)]\n",
        "      \n",
        "      log_pi1_shift = log_pi1 - log_pi1.max()\n",
        "      log_pi2_shift = log_pi2 - log_pi2.max()\n",
        "\n",
        "      pi1 = torch.exp(log_pi1_shift + 1e-20)\n",
        "      pi2 = torch.exp(log_pi2_shift + 1e-20)\n",
        "      \n",
        "      pi['1'][str(s)] = pi1 / pi1.sum()\n",
        "      pi['2'][str(s)] = pi2 / pi2.sum()\n",
        "    return pi\n",
        "  \n",
        "  def lambda_nash(self):\n",
        "    lambda_ = {'1':{}, '2':{}}\n",
        "    for s in S:\n",
        "      lambda_['1'][str(s)] = self.sqrt_lambda_nash1[str(s)].pow(2)\n",
        "      lambda_['2'][str(s)] = self.sqrt_lambda_nash2[str(s)].pow(2)\n",
        "    return lambda_"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_3Y5tpQGui5"
      },
      "source": [
        "game_b = game().to('cuda')\n",
        "optimizer = optim.Adam(game_b.parameters(), lr=1e-3)\n",
        "\n",
        "N_A_S = {}\n",
        "with torch.no_grad():\n",
        "  pi, v, lambda_nash = game_b()\n",
        "  for state in S:\n",
        "    N_A1_state = pi['1'][str(state)].shape[0]\n",
        "    N_A2_state = pi['2'][str(state)].shape[0]\n",
        "    N_A_S[str(state)] = {'1': N_A1_state, '2': N_A2_state}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ChZjFIvavkh"
      },
      "source": [
        "def offer_accepted(action):\n",
        "  if action == 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "  \n",
        "\n",
        "def other_player(i):\n",
        "  if i == 1:\n",
        "    return 2\n",
        "  elif i == 2:\n",
        "    return 1\n",
        "  elif i == '1':\n",
        "    return '2'\n",
        "  elif i == '2':\n",
        "    return '1'\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player id'\n",
        "\n",
        "\n",
        "def transition_info(state, actions):\n",
        "  if 'G' in state or 'R1' in state or 'R2' in state:\n",
        "    T = [(('O',1), 0.5), (('O',2), 0.5)]\n",
        "    return ('R', T)\n",
        "  elif 'O' in state:\n",
        "    _, id_player = state\n",
        "    offeral = actions[id_player-1] // N_A\n",
        "    action_requested = actions[id_player-1] % N_A\n",
        "    return ('D', ('E'+str(other_player(id_player)), action_requested, offeral))\n",
        "  elif 'E1' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      return ('D', ('R2', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G',))\n",
        "  elif 'E2' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      return ('D', ('R1', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G',))   \n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state' "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pjuaTnWdlwu"
      },
      "source": [
        "transition_types = {}\n",
        "for state in S:\n",
        "  if 'G' in state or 'R1' in state or 'R2' in state:\n",
        "    transition_types[str(state)] = (0,0) \n",
        "  else:\n",
        "    transition_types[str(state)] = (1,1) # entries correspond to deterministic behaviour and dependence on actions"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ANZA1cKdIE"
      },
      "source": [
        "def rewards(state, actions):\n",
        "  if 'G' in state:\n",
        "    r1 = RG1[actions[0], actions[1]]\n",
        "    r2 = RG2[actions[0], actions[1]]\n",
        "  elif 'O' in state:\n",
        "    r1 = r2 = 0.0\n",
        "  elif 'E1' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      r1 = offeral\n",
        "      r2 = -offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'E2' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      r1 = -offeral\n",
        "      r2 = offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'R1' in state:\n",
        "    _, i = state\n",
        "    r1 = RG1[actions[0], i]\n",
        "    r2 = RG2[actions[0], i]\n",
        "  elif 'R2' in state:\n",
        "    _, i = state\n",
        "    r1 = RG1[i, actions[1]]\n",
        "    r2 = RG2[i, actions[1]]\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state'\n",
        "\n",
        "  return r1, r2\n",
        "\n",
        "\n",
        "def reward_matrices(s):\n",
        "  if 'G' in s:\n",
        "    RM1 = RG1.copy()\n",
        "    RM2 = RG2.copy()\n",
        "  else:\n",
        "    N_A1 = N_A_S[str(s)]['1']\n",
        "    N_A2 = N_A_S[str(s)]['2']\n",
        "    RM1 = np.zeros((N_A1,N_A2))\n",
        "    RM2 = np.zeros((N_A1,N_A2))\n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        r1, r2 = rewards(s, [a1,a2])\n",
        "        RM1[a1,a2] = r1\n",
        "        RM2[a1,a2] = r2\n",
        "  return RM1, RM2  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSMeXOcIT0Z"
      },
      "source": [
        "RM = {}\n",
        "RM['1'] = {}\n",
        "RM['2'] = {}\n",
        "for s in S:\n",
        "  RM1, RM2 = reward_matrices(s)\n",
        "  RM['1'][str(s)] = torch.FloatTensor(RM1).to('cuda')\n",
        "  RM['2'][str(s)] = torch.FloatTensor(RM2).to('cuda')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-oou8F-3U7D"
      },
      "source": [
        "def get_state_index(state):\n",
        "  if state in S:\n",
        "    return S.index(state)\n",
        "  elif state in S_str:\n",
        "    return S_str.index(state)\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state'\n",
        "\n",
        "def player_dim(i):\n",
        "  if i in [1, '1']:\n",
        "    return 'i'\n",
        "  elif i in [2, '2']:\n",
        "    return 'j'\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player id'\n",
        "  \n",
        "def get_player_id(player):\n",
        "  if player == '1':\n",
        "    return 0\n",
        "  elif player == '2':\n",
        "    return 1\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player'\n",
        "\n",
        "def players():\n",
        "  return range(1,2+1)\n",
        "\n",
        "def players_str():\n",
        "  return iter(['1','2']) \n",
        "\n",
        "def state_player_pairs():\n",
        "  return itertools.product(S, players())\n",
        "\n",
        "S_str = [str(s) for s in S]\n",
        "def state_player_str_pairs():\n",
        "  return itertools.product(S_str, players_str())\n",
        "\n",
        "\n",
        "def player_consistent_reward_matrices():\n",
        "  consistent_RM = {'1':{}, '2':{}}\n",
        "  for s in S_str:\n",
        "    consistent_RM['1'][s] = RM['1'][s].clone()\n",
        "    consistent_RM['2'][s] = torch.t(RM['2'][s].clone())  \n",
        "  return consistent_RM "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HFMfG9l3gso"
      },
      "source": [
        "def next_value_matrices(s, v): # TODO: consider other 2 cases\n",
        "  det, dep = transition_types[str(s)]\n",
        "  N_A1 = N_A_S[str(s)]['1']\n",
        "  N_A2 = N_A_S[str(s)]['2']\n",
        "  vs = torch.zeros((N_A1,N_A2,2)).to('cuda')\n",
        "  if det and dep:\n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        _, next_state = transition_info(s, [a1,a2])\n",
        "        vs[a1,a2,:] = v[get_state_index(next_state),:]\n",
        "  elif (not det) and (not dep):\n",
        "    _, transition_dic = transition_info(s, [])\n",
        "    next_v = torch.zeros(1,2).to('cuda')\n",
        "    for next_state, transition_prob in transition_dic:\n",
        "      next_v = next_v + v[get_state_index(next_state),:].view(1,-1) * transition_prob      \n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        vs[a1,a2,:] = next_v.view(-1)\n",
        "  return vs\n",
        "\n",
        "def next_value_dictionary(v):\n",
        "  next_v_dic = {}\n",
        "  for s in S:\n",
        "    next_v_dic[str(s)] = next_value_matrices(s, v)\n",
        "  return next_v_dic\n",
        "\n",
        "\n",
        "def transition_matrix(pi): # TODO: consider other 2 cases\n",
        "  transition_matrix = torch.zeros((N_S,N_S)).to('cuda')\n",
        "  for state in S:\n",
        "    strategy_1 = pi['1'][str(state)]\n",
        "    strategy_2 = pi['2'][str(state)]\n",
        "\n",
        "    det, dep = transition_types[str(state)]\n",
        "    N_A1 = strategy_1.shape[0]\n",
        "    N_A2 = strategy_2.shape[0]\n",
        "    id_s = get_state_index(state)\n",
        "\n",
        "    if det and dep:\n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          _, next_state = transition_info(state, [a1,a2])\n",
        "          id_ns = get_state_index(next_state)\n",
        "          transition_prob = strategy_1[a1,0] * strategy_2[a2,0]\n",
        "          transition_matrix[id_s, id_ns] = transition_matrix[id_s, id_ns] + transition_prob\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = transition_info(state, [])\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        transition_matrix[id_s, get_state_index(next_state)] = transition_prob\n",
        "  return transition_matrix\n",
        "\n",
        "\n",
        "def partial_transition_matrices(pi): # TODO: consider other 2 cases\n",
        "  # Create dictionary of transition matrices for each state given\n",
        "  # the strategy of the other player \n",
        "  transition_matrices = {}\n",
        "  for s in S:\n",
        "    strategy_1 = pi['1'][str(s)]\n",
        "    strategy_2 = pi['2'][str(s)]\n",
        "    N_A1 = strategy_1.shape[0]\n",
        "    N_A2 = strategy_2.shape[0]\n",
        "    transition_matrices[str(s)] = {\n",
        "        '1': torch.zeros((N_A1,N_S)).to('cuda'),\n",
        "        '2': torch.zeros((N_A2,N_S)).to('cuda')\n",
        "    }\n",
        "\n",
        "    # Fill matrices with transition probabilities depending on the type\n",
        "    # of transition, i.e., if deterministic or random and independent or\n",
        "    # not on the actions\n",
        "    det, dep = transition_types[str(s)]\n",
        "    if det and dep:\n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          _, next_state = transition_info(s, [a1,a2])\n",
        "          id_ns = get_state_index(next_state)\n",
        "          transition_prob1 = strategy_2[a2,0]\n",
        "          transition_prob2 = strategy_1[a1,0]\n",
        "          transition_matrices[str(s)]['1'][a1, id_ns] = (\n",
        "              transition_matrices[str(s)]['1'][a1, id_ns] + transition_prob1)\n",
        "          transition_matrices[str(s)]['2'][a2, id_ns] = (\n",
        "              transition_matrices[str(s)]['2'][a2, id_ns] + transition_prob2)\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = transition_info(s, [])\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        transition_matrices[str(s)]['1'][:, get_state_index(next_state)] = transition_prob\n",
        "        transition_matrices[str(s)]['2'][:, get_state_index(next_state)] = transition_prob\n",
        "  return transition_matrices\n",
        "\n",
        "\n",
        "def expected_reward(RM, pi):\n",
        "  r_mean = torch.zeros((N_S,2)).to('cuda')\n",
        "  for i in range(1,2+1):\n",
        "    RM_i = RM[str(i)]\n",
        "    for s in S:\n",
        "      strategy_1 = pi['1'][str(s)]\n",
        "      strategy_2 = pi['2'][str(s)]\n",
        "      r_mean_1 = torch.einsum('ij,ik->jk', RM_i[str(s)], strategy_1)\n",
        "      r_mean[get_state_index(s),i-1] = (r_mean_1 * strategy_2).sum()\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def partial_expected_reward_other(RM, pi):\n",
        "  r_mean = {}\n",
        "  for player in players_str():\n",
        "    r_mean[player] = {}\n",
        "\n",
        "  for s, player in state_player_str_pairs():\n",
        "      N_A = N_A_S[s][player]\n",
        "      RM_i = RM[player]\n",
        "      other_player_ = other_player(player)\n",
        "      strategy = pi[other_player_][s].view(-1)\n",
        "      formula = 'ij,'+player_dim(other_player_)+'->'+player_dim(player)\n",
        "      r_mean[player][s] = torch.einsum(formula, RM_i[s], strategy).view(-1,1)\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def partial_expected_reward(RM, pi):\n",
        "  # Create reward dictionary for each combination of players\n",
        "  r_mean = {}\n",
        "  for player in players_str():  \n",
        "    r_mean[player] = {'1':{}, '2':{}}\n",
        "\n",
        "  # Calculate expected reward for combination of players wrt the policy of one of the players\n",
        "  for s, player in state_player_str_pairs():\n",
        "    other_player_ = other_player(player) # Player used to calculate expected reward\n",
        "    for second_player in players_str():  \n",
        "      N_A = N_A_S[s][second_player]\n",
        "      RM_i = RM[second_player] # Reward matrix for one of the players  \n",
        "      strategy = pi[other_player_][s].view(-1)\n",
        "      formula = 'ij,'+player_dim(other_player_)+'->'+player_dim(player)\n",
        "      r_mean[player][second_player][s] = torch.einsum(formula, RM_i[s], strategy).view(-1,1)\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def bellman_projection(RM, pi, v):\n",
        "  r_mean = expected_reward(RM, pi)\n",
        "  next_v = torch.zeros((N_S,2)).to('cuda')\n",
        "  for s in S:\n",
        "    next_state_value_matrix = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    next_value_1 = torch.einsum('ijk,i->jk', next_state_value_matrix, strategy_1)\n",
        "    next_v[get_state_index(s),:] = torch.einsum('jk,j->k', next_value_1, strategy_2)\n",
        "  return r_mean + beta * next_v\n",
        "\n",
        "\n",
        "def bellman_partial_projection_other(RM, pi, v):\n",
        "  r_mean = partial_expected_reward_other(RM, pi)\n",
        "  next_v = {'1':{}, '2':{}}\n",
        "  for s in S:\n",
        "    next_state_value_matrix = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    # mean next value when considering the strategy of the other player. Output: array of size m^i(s)\n",
        "    next_v['1'][str(s)] = torch.einsum('ij,j->i', next_state_value_matrix[:,:,0], strategy_2).view(-1,1)\n",
        "    next_v['2'][str(s)] = torch.einsum('ij,i->j', next_state_value_matrix[:,:,1], strategy_1).view(-1,1)\n",
        "  bellman_projection_dic = {'1':{}, '2':{}}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    bellman_projection_dic[str(player_id)][str(s)] = r_mean[str(player_id)][str(s)] + beta * next_v[str(player_id)][str(s)]\n",
        "  return bellman_projection_dic\n",
        "\n",
        "\n",
        "def partial_next_values(pi, v):\n",
        "  # Create next-value dictionary for each player combination\n",
        "  next_v = {'1':{}, '2':{}}\n",
        "  for player in players_str():\n",
        "    next_v[player] = {'1':{}, '2':{}}\n",
        "\n",
        "  # Fill dictionary \n",
        "  for s in S:\n",
        "    next_state_value_matrix = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    \n",
        "    # Calculate mean next value when considering the strategy of one of the players. Output: array of size m^i(s)\n",
        "    for player_id in players():\n",
        "      next_v['1'][str(player_id)][str(s)] = torch.einsum('ij,j->i', next_state_value_matrix[:,:,player_id-1], strategy_2).view(-1,1)\n",
        "      next_v['2'][str(player_id)][str(s)] = torch.einsum('ij,i->j', next_state_value_matrix[:,:,player_id-1], strategy_1).view(-1,1)  \n",
        "  return next_v\n",
        "\n",
        "\n",
        "def bellman_partial_projection(RM, pi, v):\n",
        "  r_mean = partial_expected_reward(RM, pi)\n",
        "  next_v = partial_next_values(pi, v)\n",
        "\n",
        "  bellman_projection_dic = {'1':{}, '2':{}}\n",
        "  for player in players_str():\n",
        "    bellman_projection_dic[player] = {'1':{}, '2':{}}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    for second_player in players_str():\n",
        "      bellman_projection_dic[player][second_player][s] = (\n",
        "          r_mean[player][second_player][s] + beta * next_v[player][second_player][s]\n",
        "          )\n",
        "  return bellman_projection_dic\n",
        "\n",
        "\n",
        "def reward_baselines(RM, pi):\n",
        "  r_mean = expected_reward(RM, pi)\n",
        "  r_baseline = r_mean.mean(0).view(-1,1).detach().cpu().numpy()\n",
        "  return r_baseline"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-uCtqdo35yW"
      },
      "source": [
        "def cost_vector_fixed_policies(pi):\n",
        "  P = transition_matrix(pi)\n",
        "  cost_vector = (1 - beta * P.sum(0)).view(-1,1).detach().cpu().numpy() / N_S\n",
        "  return cost_vector\n",
        "\n",
        "def restriction_matrices_fixed_policies(pi):\n",
        "  transition_matrices = partial_transition_matrices(pi)\n",
        "  restriction_matrices = {'1':[], '2':[]}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    temp_matrix = - beta * transition_matrices[str(s)][str(player_id)]\n",
        "    temp_matrix[:, get_state_index(s)] = temp_matrix[:, get_state_index(s)] + 1\n",
        "    restriction_matrices[str(player_id)].append(temp_matrix)\n",
        "  for player in players_str():\n",
        "    restriction_matrices[player] = -torch.cat(restriction_matrices[player], dim=0).detach().cpu().numpy()\n",
        "  return restriction_matrices\n",
        "\n",
        "def restriction_vectors_fixed_policies(RM, pi, alpha=0.1):\n",
        "  r_mean = partial_expected_reward_other(RM, pi)\n",
        "  restriction_vectors = {'1':[], '2':[]}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    restriction_vectors[player].append(r_mean[player][s].view(-1,1))\n",
        "  for player in players_str():\n",
        "    restriction_vectors[player] = -(torch.cat(restriction_vectors[player], dim=0)+alpha).detach().cpu().numpy()\n",
        "  return restriction_vectors\n",
        "\n",
        "\n",
        "def parameters_fixed_policies(game_, alpha):\n",
        "  pi = game_()[0]\n",
        "  c = cost_vector_fixed_policies(pi)\n",
        "  f0 = reward_baselines(RM, pi)\n",
        "  A_ub = restriction_matrices_fixed_policies(pi)\n",
        "  b_ub = restriction_vectors_fixed_policies(RM, pi, alpha)\n",
        "  return c, f0, A_ub, b_ub\n",
        "\n",
        "\n",
        "def calculate_initial_v(game_):\n",
        "  v0 = np.zeros((N_S,2))\n",
        "  c, f0, A_ub, b_ub = parameters_fixed_policies(game_, alpha=10)\n",
        "  for player_id in players():\n",
        "    temp_res = linprog(c, A_ub=A_ub[str(player_id)], b_ub=b_ub[str(player_id)])\n",
        "    v0[:,player_id-1] = temp_res.x\n",
        "  return v0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbMxPbhy3s_p"
      },
      "source": [
        "def calculate_nash_restrictions(pi, v):\n",
        "  q_estimated = bellman_partial_projection_other(RM, pi, v) # Dic. with an array of 'q'-values for each agent \n",
        "  g_nash = {'1':{}, '2':{}}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    g_nash[str(player_id)][str(s)] = q_estimated[str(player_id)][str(s)] - v[get_state_index(s), player_id-1]\n",
        "  return g_nash\n",
        "\n",
        "\n",
        "def calculate_bellman_error(pi, v):\n",
        "  # Calculation of original target function: Bellman approximation error \n",
        "  v_estimated = bellman_projection(RM, pi, v)\n",
        "  f_bellman = (v - v_estimated).sum()\n",
        "  return f_bellman\n",
        "\n",
        "\n",
        "def lagrangian_nash(pi, v, lambda_nash, c=1):\n",
        "  # Calculation of original target function: Bellman approximation error \n",
        "  v_estimated = bellman_projection(RM, pi, v)\n",
        "  f_bellman = (v - v_estimated).sum()\n",
        "\n",
        "  # Calculation of restrictions\n",
        "  g_nash = calculate_nash_restrictions(pi, v)\n",
        "\n",
        "  # Calculation of shifted restrictions  \n",
        "  nash_restriction_products = {'1':{}, '2':{}}  \n",
        "  for s, player_id in state_player_str_pairs():\n",
        "    nash_restriction_products[player_id][s] = (g_nash[player_id][s].view(-1,1) * lambda_nash[player_id][s].view(-1,1)) \n",
        "                                                #+ c * g_nash[player_id][s].view(-1,1).pow(2) * lambda_nash[player_id][s].view(-1,1).detach())    \n",
        "  # Calculation of 2-norms  \n",
        "  nash_restriction_sum = 0.0\n",
        "  for s, player_id in state_player_str_pairs():\n",
        "    nash_restriction_sum = nash_restriction_sum + nash_restriction_products[player_id][s].sum()\n",
        "    \n",
        "  # Calculation of augmented lagrangian\n",
        "  lagrangian = f_bellman + nash_restriction_sum  \n",
        "  return lagrangian, f_bellman\n",
        "\n",
        "\n",
        "def check_KKT_conditions(pi, v, lambda_nash, tol=1e-8):\n",
        "  with torch.no_grad():\n",
        "    # Calculate restrictions\n",
        "    g_nash = calculate_nash_restrictions(pi, v)\n",
        "\n",
        "    g_nash_satisfied = True\n",
        "    product_zero_satisfied = True\n",
        "    \n",
        "    max_g_nash = -np.infty\n",
        "    max_product_zero = -np.infty\n",
        "    \n",
        "    for player in players_str():\n",
        "      remaining_duals = lambda_nash[player].clone()\n",
        "      for s in S_str:\n",
        "        NA = N_A_S[s][player]\n",
        "        g_nash_satisfied = g_nash_satisfied and torch.all(g_nash[player][s] <= 0)\n",
        "        max_g_nash = max(max_g_nash, g_nash[player][s].max())\n",
        "\n",
        "        lambda_g_nash_product = g_nash[player][s].view(-1) * remaining_duals[:NA,:].view(-1)\n",
        "        product_zero_satisfied = product_zero_satisfied and torch.all(lambda_g_nash_product.abs() <= tol)\n",
        "        max_product_zero = max(max_product_zero, lambda_g_nash_product.abs().max())\n",
        "        remaining_duals = remaining_duals[NA:,:]\n",
        "    return g_nash_satisfied, max_g_nash, product_zero_satisfied, max_product_zero\n",
        "\n",
        "\n",
        "def check_nash_conditions(pi, v):\n",
        "  with torch.no_grad():\n",
        "    # Calculate restrictions\n",
        "    g_nash = calculate_nash_restrictions(pi, v)\n",
        "    g_nash_satisfied = True\n",
        "    max_g_nash = -np.infty\n",
        "    \n",
        "    for s, player_id in state_player_str_pairs():\n",
        "      g_nash_satisfied = g_nash_satisfied and torch.all(g_nash[player_id][s] <= 0)\n",
        "      max_g_nash = max(max_g_nash, g_nash[player_id][s].max())\n",
        "    return g_nash_satisfied, max_g_nash\n",
        "  \n",
        "\n",
        "# def optimize_game(game_, n_epochs, optimizers, params_dual, \n",
        "#                   print_each=50, save_each=10):\n",
        "#   losses = []\n",
        "#   costs = []\n",
        "#   delta_cost = 0.0\n",
        "#   previous_cost = np.infty\n",
        "#   opti_primal, opti_dual = optimizers\n",
        "  \n",
        "#   for epoch in range(0, n_epochs):\n",
        "#     pi, v, lambda_nash = game_()\n",
        "#     loss, cost = lagrangian_nash(pi, v, lambda_nash)\n",
        "#     delta_cost = cost.item() - previous_cost\n",
        "\n",
        "#     previous_cost = cost.item()\n",
        "#     torch.save(game_.state_dict(), './game_temp.pth')\n",
        "\n",
        "#     opti_primal.zero_grad()\n",
        "#     opti_dual.zero_grad()    \n",
        "#     loss.backward()\n",
        "    \n",
        "#     for p in params_dual:\n",
        "#       if p.grad is not None:\n",
        "#         p.grad.data.mul_(-1)\n",
        "\n",
        "#     opti_primal.step()\n",
        "#     opti_dual.step()\n",
        "\n",
        "#     if (epoch + 1) % print_each == 0:\n",
        "#       with torch.no_grad():\n",
        "#         nash_satisfied, max_nash_restrictions, KKT_zero_product_satisfied, max_KKT_product = check_KKT_conditions(pi, v, lambda_nash)\n",
        "#       print(\"Epoch: {}, Loss: {:.3f}, f: {:.3f}, nash: {}, max g: {:.3f}, KKT product: {}, max product: {:.3e}\".format(\n",
        "#           epoch+1,loss.item(),cost.item(), nash_satisfied, max_nash_restrictions,\n",
        "#           KKT_zero_product_satisfied, max_KKT_product))\n",
        "\n",
        "#     if (epoch + 1) % save_each == 0:\n",
        "#       losses.append(loss.item())\n",
        "#       costs.append(cost.item())\n",
        "#   return losses, costs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMPtyopfi3fC"
      },
      "source": [
        "def bellman_error_gradients(pi, v):\n",
        "  P = transition_matrix(pi)\n",
        "  bellman_error_grad_v = (torch.eye(N_S).to('cuda') - torch.t(P)).sum(1, keepdim=True)\n",
        "\n",
        "  q_individual = bellman_partial_projection(RM, pi, v)\n",
        "  bellman_error_grad_pi = {'1':{}, '2':{}}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    bellman_error_grad_pi[player][s] = 0.0\n",
        "    for second_player in players_str():\n",
        "      bellman_error_grad_pi[player][s] = bellman_error_grad_pi[player][s] - q_individual[player][second_player][s]  \n",
        "  return bellman_error_grad_v, bellman_error_grad_pi\n",
        "\n",
        "def nash_restriction_gradients(pi, v):\n",
        "  P_partial = partial_transition_matrices(pi)\n",
        "  next_value_dic = next_value_dictionary(v)\n",
        "  consistent_RM = player_consistent_reward_matrices()\n",
        "\n",
        "  nash_restriction_grad_v = {'1':{}, '2':{}}\n",
        "  nash_restriction_grad_pi = {'1':{}, '2':{}}\n",
        "\n",
        "  for s, player in state_player_str_pairs():\n",
        "    Delta = torch.zeros_like(P_partial[s][player])\n",
        "    Delta[:, get_state_index(s)] = 1.0\n",
        "    nash_restriction_grad_v[player][s] = beta * P_partial[s][player]  - Delta\n",
        "  \n",
        "    other_player_ = other_player(player)\n",
        "    next_value_matrix = next_value_dic[s][:,:,get_player_id(other_player_)]\n",
        "    r_matrix = consistent_RM[other_player_][s]\n",
        "    if player == '1':\n",
        "      next_value_matrix = torch.t(next_value_matrix)\n",
        "    nash_restriction_grad_pi[player][s] = r_matrix + beta * next_value_matrix\n",
        "  return nash_restriction_grad_v, nash_restriction_grad_pi\n",
        "\n",
        "\n",
        "def calculate_jacobian_pi_log_pi(pi):\n",
        "  jacobian_pi_log_pi_dic = {'1':{}, '2':{}}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    p = pi[player][s].view(-1)\n",
        "    jacobian_pi_log_pi_dic[player][s] = torch.diag(p) - torch.einsum('i,j->ij',p,p)\n",
        "  return jacobian_pi_log_pi_dic\n",
        "\n",
        "\n",
        "def calculate_jacobian_pi_pi_nolast():\n",
        "  jacobian_pi_pi_nolast_dic = {'1':{}, '2':{}}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    NA = N_A_S[s][player]\n",
        "    if NA > 1:\n",
        "      jacobian_pi_pi_nolast_dic[player][s] = torch.eye(NA, m=NA-1).to('cuda')\n",
        "      jacobian_pi_pi_nolast_dic[player][s][-1,:] = -1.0\n",
        "    else:\n",
        "      jacobian_pi_pi_nolast_dic[player][s] = None\n",
        "  return jacobian_pi_pi_nolast_dic\n",
        "\n",
        "\n",
        "def calculate_nash_log_pi_gradients(pi, grad_f_pi, grad_g_pi):\n",
        "  grad_f_log_pi = {'1':{}, '2':{}}\n",
        "  grad_g_log_pi = {'1':{}, '2':{}}\n",
        "\n",
        "  jacobian_pi_log_pi_dic = calculate_jacobian_pi_log_pi(pi)\n",
        "\n",
        "  for s, player in state_player_str_pairs():\n",
        "    gradient_f = grad_f_pi[player][s]\n",
        "    jacobian_g = grad_g_pi[player][s]\n",
        "    jacobian_pi = jacobian_pi_log_pi_dic[player][s]\n",
        "\n",
        "    grad_f_log_pi[player][s] = torch.einsum('ij,iw->jw', jacobian_pi, gradient_f)\n",
        "    grad_g_log_pi[player][s] = torch.einsum('ij,jk->ik', jacobian_g, jacobian_pi)\n",
        "  return grad_f_log_pi, grad_g_log_pi\n",
        "\n",
        "def calculate_descent_direction(g, grad_f_v, grad_f_lpi, grad_g_v, grad_g_lpi):\n",
        "  duals_0 = {}\n",
        "  grad_gi_vi_matrix = {}\n",
        "  A_system = {}\n",
        "  b_system = {} \n",
        "\n",
        "  d0_v = torch.zeros((N_S,2)).to('cuda')  \n",
        "  d0_lpi = {'1':{}, '2':{}}\n",
        "  norm_2_d0 = 0.0\n",
        "\n",
        "  for player in players_str():\n",
        "    J_gi_vi = torch.cat(list(grad_g_v[player].values()))\n",
        "    grad_gi_vi_matrix[player] = J_gi_vi.clone()\n",
        "    A_system[player] = (r_value*torch.diag(torch.cat(list(g[player].values())).view(-1)) - \n",
        "                        torch.einsum('as,bs->ab', J_gi_vi, J_gi_vi))\n",
        "    b_system[player] = torch.einsum('as,sw->aw', J_gi_vi, grad_f_v)\n",
        "\n",
        "    A_system_lpi = []\n",
        "    b_system_lpi = []\n",
        "    other_player_ = other_player(player)\n",
        "    for s in S_str:\n",
        "      A_system_lpi.append(torch.einsum('as,bs->ab', grad_g_lpi[other_player_][s], grad_g_lpi[other_player_][s]))\n",
        "      b_system_lpi.append(torch.einsum('as,sw->aw', grad_g_lpi[other_player_][s], grad_f_lpi[other_player_][s]))\n",
        "    A_system[player] = A_system[player] - torch.block_diag(*A_system_lpi)\n",
        "    b_system[player] = b_system[player] + torch.cat(b_system_lpi)\n",
        "\n",
        "    duals_0[player], _ = torch.solve(b_system[player], A_system[player])    \n",
        "    \n",
        "    d0_v[:,get_player_id(player)] = -torch.einsum('as,a->s', J_gi_vi, duals_0[player].view(-1)) - grad_f_v.view(-1)\n",
        "    norm_2_d0 += d0_v[:,get_player_id(player)].pow(2).sum()\n",
        "\n",
        "    remaining_duals = duals_0[player].clone()\n",
        "    for s in S_str:\n",
        "      NA = N_A_S[s][player]\n",
        "      d0_lpi[other_player_][s] = -torch.einsum('ba,bw->aw', grad_g_lpi[other_player_][s], remaining_duals[:NA,:]) - grad_f_lpi[other_player_][s]\n",
        "      norm_2_d0 += d0_lpi[other_player_][s].pow(2).sum()\n",
        "      remaining_duals = remaining_duals[NA:,:]\n",
        "  return d0_v, d0_lpi, norm_2_d0, duals_0, A_system, b_system\n",
        "\n",
        "\n",
        "def calculate_feasible_direction(d0_v, d0_lpi, norm_2_d0, duals_0, A, b, grad_g_v, grad_g_lpi, rho):\n",
        "  div = duals_0['1'].sum() + duals_0['2'].sum()\n",
        "  if div > 0:\n",
        "    rho_1 = (1-alpha) / div\n",
        "    if rho_1 < rho:\n",
        "      rho = 0.5 * rho_1\n",
        "  \n",
        "  duals = {}  \n",
        "  d_v = d0_v.clone() \n",
        "  d_lpi = {'1':{}, '2':{}}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    d_lpi[player][s] = d0_lpi[player][s].clone()\n",
        "  \n",
        "  for player in players_str():\n",
        "    dual_corrections = -rho * norm_2_d0 * torch.solve(torch.ones_like(b[player]), A[player])[0]\n",
        "    J_gi_vi = torch.cat(list(grad_g_v[player].values()))\n",
        "    d_v[:,get_player_id(player)] -= torch.einsum('as,a->s', J_gi_vi, dual_corrections.view(-1))\n",
        "\n",
        "    other_player_ = other_player(player)\n",
        "    remaining_corrections = dual_corrections.clone()\n",
        "    for s in S_str:\n",
        "      NA = N_A_S[s][player]\n",
        "      d_lpi[other_player_][s] = -torch.einsum('ba,bw->aw', grad_g_lpi[other_player_][s], remaining_corrections[:NA,:])      \n",
        "      remaining_corrections = remaining_corrections[NA:,:]\n",
        "    \n",
        "    duals[player] = duals_0[player] + dual_corrections\n",
        "  return d_v, d_lpi, duals, rho\n",
        "\n",
        "\n",
        "def copy_game(game_original):\n",
        "  game_copy = game().to('cuda')\n",
        "  game_copy.load_state_dict(game_original.state_dict())\n",
        "  return game_copy\n",
        "\n",
        "\n",
        "def feasible_gradient_descent(game_0, pi, v, g, grad_f_v, grad_f_pi, d_v, d_lpi, duals,\n",
        "                              max_steps=1000, verbose=False):\n",
        "  f = calculate_bellman_error(pi, v)\n",
        "  decrement = torch.einsum('ij,i->j', d_v, grad_f_v.view(-1)).sum()\n",
        "  for s, player in state_player_str_pairs():\n",
        "    decrement += (d_lpi[player][s] * grad_f_pi[player][s]).sum()\n",
        "\n",
        "  found_feasible_step_size = False\n",
        "  step_size = 1.0\n",
        "  n_step = 0\n",
        "  max_steps = max(1, max_steps)\n",
        "  while (not found_feasible_step_size) and (n_step < max_steps):\n",
        "    game_temp = copy_game(game_0)\n",
        "    game_temp.v.data.add_(step_size * d_v)\n",
        "    for s in S_str:\n",
        "      game_temp.log_pi1[s].data.add_(step_size * d_lpi['1'][s])\n",
        "      game_temp.log_pi2[s].data.add_(step_size * d_lpi['2'][s])\n",
        "\n",
        "    pi_temp, v_temp, _ = game_temp()\n",
        "    f_temp = calculate_bellman_error(pi_temp, v_temp)\n",
        "    if f_temp <= f + step_size * eta * decrement:\n",
        "      feasible_step = True\n",
        "      g_temp = calculate_nash_restrictions(pi_temp, v_temp)\n",
        "      for player in players_str():\n",
        "        gammas = gamma_0 * torch.ones_like(duals[player])\n",
        "        gammas[duals[player]< 0] = 1.0\n",
        "        remaining_gammas = gammas.clone()\n",
        "        for s in S_str:\n",
        "          NA = N_A_S[s][player]\n",
        "          if not torch.all(g_temp[player][s] <= remaining_gammas[:NA,:] * g[player][s]):\n",
        "            feasible_step = False\n",
        "            break\n",
        "        if not feasible_step:\n",
        "          break\n",
        "      if feasible_step:\n",
        "        found_feasible_step_size = True\n",
        "    step_size /= nu\n",
        "    n_step += 1\n",
        "    if verbose:\n",
        "      print(\"Step: {}, Found feasible step size: {}\".format(n_step, found_feasible_step_size))\n",
        "  return game_temp, found_feasible_step_size, n_step, f_temp\n",
        "\n",
        "\n",
        "def optimize_game(game_0, rho, n_epochs=100):\n",
        "  game_new = copy_game(game_0)\n",
        "  pi_0, v_0, _ = game_0()\n",
        "  f_0 = calculate_bellman_error(pi_0, v_0)\n",
        "  with torch.no_grad():\n",
        "    for epoch in range(0, n_epochs):\n",
        "      pi, v, _ = game_new()\n",
        "      g_nash = calculate_nash_restrictions(pi, v)\n",
        "      grad_f_v, grad_f_pi = bellman_error_gradients(pi, v)\n",
        "      grad_g_v, grad_g_pi = nash_restriction_gradients(pi, v)\n",
        "      grad_f_log_pi, grad_g_log_pi = calculate_nash_log_pi_gradients(pi, grad_f_pi, grad_g_pi)\n",
        "      d0_v, d0_lpi, norm_2_d0, duals_0, A_system, b_system = calculate_descent_direction(g_nash, grad_f_v, grad_f_log_pi, grad_g_v, grad_g_log_pi)\n",
        "      d_v, d_lpi, duals, rho = calculate_feasible_direction(d0_v, d0_lpi, norm_2_d0, duals_0, A_system, b_system, grad_g_v, grad_g_log_pi, rho)\n",
        "      g_nash_satisfied, max_g_nash, product_zero_satisfied, max_product_zero = check_KKT_conditions(pi, v, duals)\n",
        "      game_temp, found_feasible_step_size, n_step, f_new = feasible_gradient_descent(game_new, pi, v, g_nash, grad_f_v, grad_f_pi, d_v, d_lpi, duals)\n",
        "      \n",
        "      if found_feasible_step_size:\n",
        "        game_new = game_temp\n",
        "        delta_f = (f_new - f_0) / f_0 * 100\n",
        "        print('Epoch: {}, f: {:.3e}, delta f: {:.3e}%, nash satisfied: {}, max g: {:.3e}, dual nash satisfied: {}, max product: {:.3e}, rho: {:.3e}, norm2 d0:{:.3e}'.format(\n",
        "            epoch, f_new.item(), delta_f.item(), g_nash_satisfied, max_g_nash, product_zero_satisfied, max_product_zero, rho, norm_2_d0))\n",
        "      else:\n",
        "        break\n",
        "        \n",
        "  return game_new     "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbiikizT9KyT"
      },
      "source": [
        "#**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLlRZTiQAgWJ"
      },
      "source": [
        "game_b = game().to('cuda')\n",
        "params_primal = [game_b.v] + list(game_b.log_pi1.parameters()) + list(game_b.log_pi2.parameters())\n",
        "params_dual = list(game_b.sqrt_lambda_nash1.parameters()) + list(game_b.sqrt_lambda_nash2.parameters())\n",
        "optim_primal = optim.Adam(params_primal, lr=1e-3)\n",
        "optim_dual = optim.Adam(params_dual, lr=1e-4)\n",
        "optimizers = [optim_primal, optim_dual]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnKgBgmSQuEE",
        "outputId": "2c7a457c-caa9-4f99-ba0b-7129a24064a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "v0 = calculate_initial_v(game_b)\n",
        "v0 = torch.FloatTensor(v0).to('cuda')\n",
        "game_b.v.data.add_(v0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1170.7245, 1163.5232],\n",
              "        [1167.5430, 1161.5166],\n",
              "        [1167.8507, 1160.7441],\n",
              "        [1169.0156, 1162.8157],\n",
              "        [1168.8639, 1161.6923],\n",
              "        [1169.0306, 1161.5918],\n",
              "        [1168.7157, 1161.1423],\n",
              "        [1169.5321, 1160.6948],\n",
              "        [1171.2921, 1159.4099],\n",
              "        [1168.4246, 1161.3650],\n",
              "        [1169.1949, 1159.9166],\n",
              "        [1168.4863, 1159.5914],\n",
              "        [1169.8118, 1157.4054],\n",
              "        [1171.0175, 1158.4017],\n",
              "        [1171.5082, 1157.9583],\n",
              "        [1168.6451, 1162.1965],\n",
              "        [1168.4462, 1161.6926],\n",
              "        [1168.0032, 1161.6946],\n",
              "        [1167.3015, 1162.8699],\n",
              "        [1167.7131, 1163.6956],\n",
              "        [1167.0376, 1164.9009],\n",
              "        [1167.0580, 1162.2310],\n",
              "        [1166.4336, 1161.8843],\n",
              "        [1165.7365, 1163.2469],\n",
              "        [1164.8076, 1163.5886],\n",
              "        [1164.0450, 1165.1560],\n",
              "        [1164.2004, 1166.2593],\n",
              "        [1170.6833, 1161.0632],\n",
              "        [1167.2362, 1162.8953],\n",
              "        [1167.2474, 1164.5168],\n",
              "        [1168.0742, 1160.5702]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYaXbFwlYldO"
      },
      "source": [
        "alpha = 0.1\n",
        "gamma_0 = 0.1\n",
        "eta = 0.1\n",
        "nu = 1/0.98\n",
        "rho_0 = 1.0\n",
        "rho = rho_0\n",
        "r_value = 1.0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GpIbXdaVkt9",
        "outputId": "5bb2fb5c-aa83-410f-cb73-9cecf002a103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "game_b2 = optimize_game(game_b, rho, n_epochs=15)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, f: 5.622e+02, delta f: -1.347e+01%, nash satisfied: True, max g: -9.322e+00, dual nash satisfied: False, max product: 5.588e+00, rho: 2.397e-02, norm2 d0:6.991e+01\n",
            "Epoch: 1, f: 5.268e+02, delta f: -1.892e+01%, nash satisfied: True, max g: -6.413e+00, dual nash satisfied: False, max product: 6.417e+00, rho: 2.397e-02, norm2 d0:8.646e+01\n",
            "Epoch: 2, f: 5.063e+02, delta f: -2.208e+01%, nash satisfied: True, max g: -5.127e+00, dual nash satisfied: False, max product: 6.731e+00, rho: 2.397e-02, norm2 d0:8.911e+01\n",
            "Epoch: 3, f: 4.736e+02, delta f: -2.711e+01%, nash satisfied: True, max g: -4.723e+00, dual nash satisfied: False, max product: 6.322e+00, rho: 2.397e-02, norm2 d0:7.738e+01\n",
            "Epoch: 4, f: 4.262e+02, delta f: -3.440e+01%, nash satisfied: True, max g: -4.537e+00, dual nash satisfied: False, max product: 5.191e+00, rho: 2.397e-02, norm2 d0:5.962e+01\n",
            "Epoch: 5, f: 3.724e+02, delta f: -4.269e+01%, nash satisfied: True, max g: -3.564e+00, dual nash satisfied: False, max product: 3.853e+00, rho: 2.397e-02, norm2 d0:4.322e+01\n",
            "Epoch: 6, f: 3.212e+02, delta f: -5.056e+01%, nash satisfied: True, max g: -2.444e+00, dual nash satisfied: False, max product: 2.754e+00, rho: 2.397e-02, norm2 d0:3.082e+01\n",
            "Epoch: 7, f: 2.794e+02, delta f: -5.700e+01%, nash satisfied: True, max g: -1.467e+00, dual nash satisfied: False, max product: 1.967e+00, rho: 2.397e-02, norm2 d0:2.222e+01\n",
            "Epoch: 8, f: 2.499e+02, delta f: -6.154e+01%, nash satisfied: True, max g: -7.998e-01, dual nash satisfied: False, max product: 1.391e+00, rho: 2.397e-02, norm2 d0:1.689e+01\n",
            "Epoch: 9, f: 2.311e+02, delta f: -6.443e+01%, nash satisfied: True, max g: -2.448e-01, dual nash satisfied: False, max product: 1.355e+00, rho: 2.397e-02, norm2 d0:1.422e+01\n",
            "Epoch: 10, f: 2.184e+02, delta f: -6.638e+01%, nash satisfied: True, max g: -1.941e-01, dual nash satisfied: False, max product: 1.363e+00, rho: 2.397e-02, norm2 d0:1.278e+01\n",
            "Epoch: 11, f: 2.145e+02, delta f: -6.698e+01%, nash satisfied: True, max g: -6.531e-02, dual nash satisfied: False, max product: 1.137e+00, rho: 1.194e-02, norm2 d0:1.173e+01\n",
            "Epoch: 12, f: 2.140e+02, delta f: -6.706e+01%, nash satisfied: True, max g: -6.958e-03, dual nash satisfied: False, max product: 1.147e+00, rho: 1.194e-02, norm2 d0:1.137e+01\n",
            "Epoch: 13, f: 2.140e+02, delta f: -6.706e+01%, nash satisfied: True, max g: -7.324e-04, dual nash satisfied: False, max product: 1.148e+00, rho: 1.194e-02, norm2 d0:1.133e+01\n",
            "Epoch: 14, f: 2.140e+02, delta f: -6.706e+01%, nash satisfied: True, max g: -2.441e-04, dual nash satisfied: False, max product: 1.148e+00, rho: 1.194e-02, norm2 d0:1.132e+01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Rp2_lu55dj",
        "outputId": "cb9b0f2b-7783-4719-88e0-dc5fe9b71585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "torch.eye(4,3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}