{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stochastic_games_RGD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarod13/Stochastic_Games/blob/master/stochastic_games_RGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5T17o-vEAdt"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "from scipy.linalg import orth\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "import copy\n",
        "\n",
        "device = 'cpu'"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SMCGNsteY1a"
      },
      "source": [
        "# import inspect\n",
        "# inspect.getsource(game.feasible_gradient_descent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsaLjXwX-aKX"
      },
      "source": [
        "# Game Class Definition\n",
        "\n",
        "First, we will define a general class for stochastic games that requires the tuple $\\langle N,S,A,R,T,\\beta \\rangle$ to define each game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diVFarepD_cX"
      },
      "source": [
        "class multi_player_game(nn.Module):\n",
        "  def __init__(self, N, S, A, R, T, beta, device, dtype=torch.float64, name='StochasticGame-v0'):\n",
        "    super().__init__()\n",
        "\n",
        "    # Store game defining parameters\n",
        "    self.N = N\n",
        "    self.S_no_string = S.copy()\n",
        "    self.S = [str(s) for s in S]\n",
        "    self.A = A.copy()\n",
        "    self.R = {}\n",
        "    for player in self.players():\n",
        "      self.R[player] = {}\n",
        "      for s_key, val in R[player].items():\n",
        "        self.R[player][s_key] = val.clone().to(device)\n",
        "    self.transition_map = T[0]\n",
        "    self.transition_type = T[1]\n",
        "    self.beta = beta\n",
        "    self.device = device\n",
        "    self.name = name\n",
        "    self.dtype = dtype\n",
        "\n",
        "    # Define useful constants\n",
        "    self.N_S = len(S)    \n",
        "    self.N_A_S_vector = {'1':[], '2':[]}\n",
        "    self.N_A_S_reduced_vector = {'1':[], '2':[]}\n",
        "    for player in self.players():\n",
        "      for s in self.S:\n",
        "        n_actions_player_in_s = A[player][s]      \n",
        "        self.N_A_S_vector[player].append(n_actions_player_in_s)\n",
        "        if self.more_than_one_action(player, s):\n",
        "          self.N_A_S_reduced_vector[player].append(n_actions_player_in_s)\n",
        "    \n",
        "    self.N_A_total = {}\n",
        "    self.N_A_reduced = {}\n",
        "    self.N_S_reduced = {}\n",
        "    for player in self.players():\n",
        "      if dtype == torch.float64:\n",
        "        self.N_A_S_vector[player] = torch.DoubleTensor(self.N_A_S_vector[player]).to(device).view(-1,1)\n",
        "        self.N_A_S_reduced_vector[player] = torch.DoubleTensor(self.N_A_S_reduced_vector[player]).to(device).view(-1,1)\n",
        "      else:\n",
        "        self.N_A_S_vector[player] = torch.FloatTensor(self.N_A_S_vector[player]).to(device).view(-1,1)\n",
        "        self.N_A_S_reduced_vector[player] = torch.FloatTensor(self.N_A_S_reduced_vector[player]).to(device).view(-1,1)\n",
        "      self.N_A_total[player] = int(self.N_A_S_vector[player].sum().item())\n",
        "      self.N_A_reduced[player] = int(self.N_A_S_reduced_vector[player].sum().item())\n",
        "      self.N_S_reduced[player] = self.N_A_S_reduced_vector[player].view(-1).shape[0]\n",
        "\n",
        "    self.n_restrictions = 0\n",
        "    for player in self.players():\n",
        "      self.n_restrictions += self.N_A_total[player] + self.N_A_reduced[player] + self.N_S_reduced[player]\n",
        "    self.n_vars = 2*self.N_S + self.N_A_reduced['1'] + self.N_A_reduced['2']\n",
        "    self.restriction_types = torch.BoolTensor((self.N_A_total['1'] + self.N_A_total['2'] + \n",
        "                               self.N_A_reduced['1'] + self.N_A_reduced['2']) * [False] + \n",
        "                              (self.N_S_reduced['1']+self.N_S_reduced['2']) * [True]) \n",
        "\n",
        "    # Define useful variables\n",
        "    self.n_save = 0\n",
        "\n",
        "    # Define policy and value parameter tensors\n",
        "    for player in self.players():\n",
        "      setattr(self, 'pi'+player, nn.ParameterDict())\n",
        "    \n",
        "    self.pi = {}\n",
        "    for player in self.players():\n",
        "      self.pi[player] = getattr(self, 'pi'+player)\n",
        "      for s in self.S: \n",
        "        n_actions_player_in_s = A[player][s]\n",
        "        if dtype == torch.float64:\n",
        "          self.pi[player][s] = Parameter(torch.Tensor(n_actions_player_in_s,1).double().to(device))\n",
        "        else:\n",
        "          self.pi[player][s] = Parameter(torch.Tensor(n_actions_player_in_s,1).to(device))\n",
        "        if n_actions_player_in_s >= 2:\n",
        "          nn.init.constant_(self.pi[player][s], 1.0/n_actions_player_in_s)\n",
        "        else:\n",
        "          nn.init.constant_(self.pi[player][s], 1.0)\n",
        "    \n",
        "    if dtype == torch.float64:\n",
        "      self.v = Parameter(torch.Tensor(self.N_S,2).double().to(device))\n",
        "    else:\n",
        "      self.v = Parameter(torch.Tensor(self.N_S,2).to(device))\n",
        "    self.set_feasible_value(alpha=1.0e-1)\n",
        "\n",
        "  def forward(self):\n",
        "    return self.pi, self.v\n",
        "  \n",
        "  #-----------------------------------------------------------------------------\n",
        "  # Useful methods\n",
        "  def get_state_index(self, state):\n",
        "    if state in self.S:\n",
        "      return self.S.index(state)\n",
        "    elif state in self.S_no_string:\n",
        "      return self.S_no_string.index(state)\n",
        "    else:\n",
        "      assert 0 == 1, 'Invalid state'\n",
        "  \n",
        "  # TODO: consider more than 2 players (is this method really necessary?)\n",
        "  @staticmethod \n",
        "  def player_dim(i):\n",
        "    if i in [0, '1']:\n",
        "      return 'i'\n",
        "    elif i in [1, '2']:\n",
        "      return 'j'\n",
        "    else:\n",
        "      assert 0 == 1, 'Invalid player id'\n",
        "\n",
        "  def get_player_id(self, player):\n",
        "    player_id = int(player)-1\n",
        "    if player_id >= 0 and player_id < self.N:\n",
        "      return player_id\n",
        "    else:\n",
        "      assert 0 == 1, 'Invalid player'\n",
        "  \n",
        "  # TODO: consider more than 2 players (is this method really necessary?)\n",
        "  @staticmethod\n",
        "  def other_player(i):\n",
        "    if i == 0:\n",
        "      return 1\n",
        "    elif i == 1:\n",
        "      return 0\n",
        "    elif i == '1':\n",
        "      return '2'\n",
        "    elif i == '2':\n",
        "      return '1'\n",
        "    else:\n",
        "      assert 0 == 1, 'Invalid player id'\n",
        "\n",
        "  def players_id(self):\n",
        "    return range(0, self.N)\n",
        "\n",
        "  def players(self):\n",
        "    return iter([str(player_id+1) for player_id in self.players_id()]) \n",
        "\n",
        "  def state_player_id_pairs(self):\n",
        "    return itertools.product(self.S, self.players_id())\n",
        "\n",
        "  def state_player_pairs(self):\n",
        "    return itertools.product(self.S, self.players())\n",
        "\n",
        "  def player_consistent_reward_matrices(self):\n",
        "    consistent_R = {'1':{}, '2':{}}\n",
        "    for s in self.S:\n",
        "      consistent_R['1'][s] = self.R['1'][s].clone()\n",
        "      consistent_R['2'][s] = torch.t(self.R['2'][s].clone())  \n",
        "    return consistent_R \n",
        "\n",
        "  def more_than_one_action(self, player, s):\n",
        "    if self.A[player][s] > 1:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  \n",
        "  def pi2vec(self):\n",
        "    pi_vector = {}\n",
        "    for player in self.players():\n",
        "      pi_list = []\n",
        "      for s in self.S:\n",
        "        if self.more_than_one_action(player, s):\n",
        "          pi_list.append(self.pi[player][s])\n",
        "      pi_vector[player] = torch.cat(pi_list, dim=0)\n",
        "    return pi_vector\n",
        "  \n",
        "  def pi_sum(self):\n",
        "    sum_vector = {}\n",
        "    for player in self.players():\n",
        "      sum_list = []\n",
        "      for s in self.S:\n",
        "        if self.more_than_one_action(player, s):\n",
        "          sum_list.append(self.pi[player][s].sum(0, keepdim=True))        \n",
        "      sum_vector[player] = torch.cat(sum_list, dim=0)\n",
        "    return sum_vector\n",
        "\n",
        "  def perturb_pi(self, hard_constraints, alpha=1e-2):\n",
        "    for player in self.players():\n",
        "      for s in self.S:\n",
        "        if self.more_than_one_action(player, s):\n",
        "          sum_pi = self.pi[player][s].sum()\n",
        "          pi = self.pi[player][s].clamp(1e-3,1.0-1e-3)\n",
        "          pi = pi / pi.sum() * sum_pi\n",
        "          self.pi[player][s].data.copy_(pi.clone())\n",
        "    g_vector = self.calculate_restriction_vector(hard_constraints)\n",
        "    if not torch.all(g_vector <= 0.0):\n",
        "      self.set_feasible_value(alpha=alpha)\n",
        "\n",
        "  def pi_negentropy(self):\n",
        "    H_vector = {}\n",
        "    for player in self.players():\n",
        "      H_list = []\n",
        "      for s in self.S:\n",
        "        if self.more_than_one_action(player, s):\n",
        "          p = self.pi[player][s]\n",
        "          H = (-p * torch.log(p)).sum(0, keepdim=True)\n",
        "          Hmax = np.log(self.A[player][s])\n",
        "          coef = (1+1e-3)*Hmax - H\n",
        "          H_list.append(coef*torch.ones_like(p))        \n",
        "      H_vector[player] = torch.cat(H_list, dim=0)\n",
        "    return H_vector\n",
        "  \n",
        "  def mask_inequality_restrictions(self, vec):\n",
        "    masked_vector = vec.clone()\n",
        "    masked_vector[-self.N_S_reduced['1']-self.N_S_reduced['2']:,:] = 0.0\n",
        "    return masked_vector\n",
        "\n",
        "  def mask_equality_restrictions(self, vec):\n",
        "    masked_vector = vec.clone()\n",
        "    masked_vector[:-self.N_S_reduced['1']-self.N_S_reduced['2'],:] = 0.0\n",
        "    return masked_vector\n",
        "  \n",
        "  def vec2dic(self, d_vector, duals_vector):\n",
        "    d_v = torch.zeros((self.N_S,2), dtype=self.dtype).to(self.device)  \n",
        "    d_pi = {'1':{}, '2':{}}\n",
        "    duals = {}\n",
        "\n",
        "    # Store gradients related to v in the proper shape\n",
        "    for i in range(0,2):\n",
        "      d_v[:,i] = d_vector[i*self.N_S:(i+1)*self.N_S,:].view(-1)\n",
        "    \n",
        "    # Store gradients related to pi in dictionaries for each player and state\n",
        "    for player in self.players():\n",
        "      y0 = 2*self.N_S + self.get_player_id(player) * self.N_A_reduced['1']\n",
        "      yf = 2*self.N_S + self.N_A_reduced['1']  + self.get_player_id(player) * self.N_A_reduced['2'] \n",
        "      d_pi_vector = d_vector[y0:yf,:]\n",
        "      n = 0\n",
        "      for s in self.S:      \n",
        "        if self.more_than_one_action(player, s):\n",
        "          NA = self.A[player][s]\n",
        "          d_pi[player][s] = d_pi_vector[n:n+NA,:]\n",
        "          n = n + NA\n",
        "        else:\n",
        "          d_pi[player][s] = None      \n",
        "\n",
        "    # Separate duals for each player  \n",
        "    for player in self.players():\n",
        "      i = self.get_player_id(player)\n",
        "      n_restrictions_player = self.N_A_total[player] + self.N_A_reduced[player] + self.N_S_reduced[player]\n",
        "      duals[player] = torch.zeros((n_restrictions_player,1), dtype=self.dtype).to(self.device)\n",
        "      \n",
        "      y0 = self.get_player_id(player) * self.N_A_total['1']\n",
        "      yf = self.N_A_total['1']  + self.get_player_id(player) * self.N_A_total['2'] \n",
        "      duals[player][:self.N_A_total[player],:] = duals_vector[y0:yf,:]\n",
        "      \n",
        "      y0 = self.N_A_total['1'] + self.N_A_total['2'] + self.get_player_id(player) * self.N_A_reduced['1']\n",
        "      yf = self.N_A_total['1'] + self.N_A_total['2'] + self.N_A_reduced['1'] + self.get_player_id(player) * self.N_A_reduced['2'] \n",
        "      duals[player][self.N_A_total[player]:self.N_A_total[player]+self.N_A_reduced[player],:] = duals_vector[y0:yf,:]\n",
        "\n",
        "      y0 = (self.N_A_total['1'] + self.N_A_total['2'] + self.N_A_reduced['1'] + self.N_A_reduced['2'] \n",
        "            + self.get_player_id(player) * self.N_S_reduced['1'])\n",
        "      yf = (self.N_A_total['1'] + self.N_A_total['2'] + self.N_A_reduced['1'] + self.N_A_reduced['2'] \n",
        "            + self.N_S_reduced['1'] + self.get_player_id(player) * self.N_S_reduced['2']) \n",
        "      duals[player][-self.N_S_reduced[player]:,:] = duals_vector[y0:yf,:]\n",
        "\n",
        "    return d_v, d_pi, duals\n",
        "\n",
        "  def vec2dic_noduals(self, d_vector):\n",
        "    d_v = torch.zeros((self.N_S,2), dtype=self.dtype).to(self.device)  \n",
        "    d_pi = {'1':{}, '2':{}}\n",
        "    duals = {}\n",
        "\n",
        "    # Store gradients related to v in the proper shape\n",
        "    for i in range(0,2):\n",
        "      d_v[:,i] = d_vector[i*self.N_S:(i+1)*self.N_S,:].view(-1)\n",
        "    \n",
        "    # Store gradients related to pi in dictionaries for each player and state\n",
        "    for player in self.players():\n",
        "      y0 = 2*self.N_S + self.get_player_id(player) * self.N_A_reduced['1']\n",
        "      yf = 2*self.N_S + self.N_A_reduced['1']  + self.get_player_id(player) * self.N_A_reduced['2'] \n",
        "      d_pi_vector = d_vector[y0:yf,:]\n",
        "      n = 0\n",
        "      for s in self.S:      \n",
        "        if self.more_than_one_action(player, s):\n",
        "          NA = self.A[player][s]\n",
        "          d_pi[player][s] = d_pi_vector[n:n+NA,:]\n",
        "          n = n + NA\n",
        "        else:\n",
        "          d_pi[player][s] = None      \n",
        "\n",
        "    return d_v, d_pi\n",
        "\n",
        "  def update_c_vector(self, c_vector, duals_0_vector):\n",
        "    new_c_vector = c_vector.clone()\n",
        "    entries_to_update = c_vector < -1.2*duals_0_vector\n",
        "    new_c_vector[entries_to_update] = -2*duals_0_vector[entries_to_update]\n",
        "    new_c_vector = self.mask_equality_restrictions(new_c_vector)\n",
        "    return new_c_vector\n",
        "\n",
        "  def copy_game(self):\n",
        "    # Ttuple = (self.transition_map, self.transition_type)\n",
        "    # game_copy = multi_player_game(self.N, self.S_no_string, self.A, self.R, Ttuple, self.beta, self.device).to(device)\n",
        "\n",
        "    game_copy = copy.deepcopy(self)\n",
        "    # with torch.no_grad():\n",
        "    #   for param_copy, param in zip(game_copy.parameters(), self.parameters()):\n",
        "    #     param_copy.copy_(param)\n",
        "    return game_copy\n",
        "  \n",
        "  def save(self):\n",
        "    torch.save(self.state_dict(), './'+self.name+'_gamesave_'+str(self.n_save)+'.pth')\n",
        "    print('Game '+str(self.n_save)+' saved succesfully')\n",
        "    self.n_save += 1\n",
        "  \n",
        "  #-----------------------------------------------------------------------------\n",
        "  # Methods related with the transition matrices\n",
        "\n",
        "  def transition_matrix(self): # TODO: consider other 2 cases\n",
        "    transition_matrix = torch.zeros((self.N_S,self.N_S), dtype=self.dtype).to(self.device)\n",
        "    for state in self.S_no_string:\n",
        "      strategy_1 = self.pi['1'][str(state)]\n",
        "      strategy_2 = self.pi['2'][str(state)]\n",
        "\n",
        "      det, dep = self.transition_type[str(state)]\n",
        "      N_A1 = self.A['1'][str(state)]\n",
        "      N_A2 = self.A['2'][str(state)]\n",
        "      id_s = self.get_state_index(state)\n",
        "\n",
        "      if det and dep:\n",
        "        for a1 in range(0,N_A1):\n",
        "          for a2 in range(0,N_A2):\n",
        "            _, next_state = self.transition_map(state, [a1,a2])\n",
        "            id_ns = self.get_state_index(next_state)\n",
        "            transition_prob = strategy_1[a1,0] * strategy_2[a2,0]\n",
        "            transition_matrix[id_s, id_ns] = transition_matrix[id_s, id_ns] + transition_prob\n",
        "      elif (not det) and (not dep):\n",
        "        _, transition_dic = self.transition_map(state, [])\n",
        "        for next_state, transition_prob in transition_dic:\n",
        "          transition_matrix[id_s, self.get_state_index(next_state)] = (\n",
        "              transition_prob * strategy_1.sum() * strategy_2.sum())\n",
        "    return transition_matrix\n",
        "\n",
        "  def partial_transition_matrices(self): # TODO: consider other 2 cases\n",
        "    # Create dictionary of transition matrices for each state given\n",
        "    # the strategy of the other player \n",
        "    transition_matrices = {}\n",
        "    for state in self.S_no_string:\n",
        "      s = str(state)\n",
        "      strategy_1 = self.pi['1'][s]\n",
        "      strategy_2 = self.pi['2'][s]\n",
        "      N_A1 = self.A['1'][s]\n",
        "      N_A2 = self.A['2'][s]\n",
        "      transition_matrices[s] = {\n",
        "          '1': torch.zeros((N_A1,self.N_S), dtype=self.dtype).to(self.device),\n",
        "          '2': torch.zeros((N_A2,self.N_S), dtype=self.dtype).to(self.device)\n",
        "      }\n",
        "\n",
        "      # Fill matrices with transition probabilities depending on the type\n",
        "      # of transition, i.e., if deterministic or random and independent or\n",
        "      # not on the actions\n",
        "      det, dep = self.transition_type[s]\n",
        "      if det and dep:\n",
        "        for a1 in range(0,N_A1):\n",
        "          for a2 in range(0,N_A2):\n",
        "            _, next_state = self.transition_map(state, [a1,a2])\n",
        "            id_ns = self.get_state_index(next_state)\n",
        "            transition_prob1 = strategy_2[a2,0]\n",
        "            transition_prob2 = strategy_1[a1,0]\n",
        "            transition_matrices[s]['1'][a1, id_ns] = (\n",
        "                transition_matrices[s]['1'][a1, id_ns] + transition_prob1)\n",
        "            transition_matrices[s]['2'][a2, id_ns] = (\n",
        "                transition_matrices[s]['2'][a2, id_ns] + transition_prob2)\n",
        "      elif (not det) and (not dep):\n",
        "        _, transition_dic = self.transition_map(state, [])\n",
        "        for next_state, transition_prob in transition_dic:\n",
        "          transition_matrices[s]['1'][:, self.get_state_index(next_state)] = (\n",
        "              transition_prob * strategy_2.sum())\n",
        "          transition_matrices[s]['2'][:, self.get_state_index(next_state)] = (\n",
        "              transition_prob * strategy_1.sum())\n",
        "    return transition_matrices\n",
        "\n",
        "  def transition_matrix_given_sai(self, player, state, ai): # TODO: consider other 2 cases\n",
        "    other_player = self.other_player(player)\n",
        "    N_Ami = self.A[other_player][str(state)]\n",
        "    transition_matrix = torch.zeros((self.N_S,N_Ami), dtype=self.dtype).to(self.device)\n",
        "    \n",
        "    det, dep = self.transition_type[str(state)]\n",
        "    \n",
        "    if det and dep:\n",
        "      for ami in range(0,N_Ami):\n",
        "          action = [0, 0]\n",
        "          action[self.get_player_id(player)] = ai\n",
        "          action[self.get_player_id(other_player)] = ami\n",
        "          _, next_state = self.transition_map(state, action)\n",
        "          id_ns = self.get_state_index(next_state)\n",
        "          transition_matrix[id_ns, ami] = 1.0\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = self.transition_map(state, [])\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        transition_matrix[self.get_state_index(next_state), :] = transition_prob\n",
        "    return transition_matrix\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # Methods related with the expected reward and value function calculation\n",
        "\n",
        "  # TODO: consider more than 2 players  \n",
        "  def expected_reward(self):\n",
        "    r_mean = torch.zeros((self.N_S,2), dtype=self.dtype).to(self.device)\n",
        "    for player_id in self.players_id():\n",
        "      player = str(player_id+1)\n",
        "      RM_i = self.R[player]\n",
        "      for s in self.S:\n",
        "        strategy_1 = self.pi['1'][s]\n",
        "        strategy_2 = self.pi['2'][s]\n",
        "        r_mean_1 = torch.einsum('ij,ik->jk', RM_i[s], strategy_1)\n",
        "        r_mean[self.get_state_index(s), player_id] = (r_mean_1 * strategy_2).sum()\n",
        "    return r_mean\n",
        "  \n",
        "  def partial_expected_reward(self):\n",
        "    # Create reward dictionary for each combination of players\n",
        "    r_mean = {}\n",
        "    for player in self.players():  \n",
        "      r_mean[player] = {'1':{}, '2':{}}\n",
        "\n",
        "    # Calculate expected reward for combination of players wrt the policy of one of the players\n",
        "    for s, player in self.state_player_pairs():\n",
        "      other_player = self.other_player(player) # Player used to calculate expected reward\n",
        "      for second_player in self.players():  \n",
        "        N_A = self.A[second_player][s]\n",
        "        RM_i = self.R[second_player] # Reward matrix for one of the players  \n",
        "        strategy = self.pi[other_player][s].view(-1)\n",
        "        formula = 'ij,'+self.player_dim(other_player)+'->'+self.player_dim(player)\n",
        "        r_mean[player][second_player][s] = torch.einsum(formula, RM_i[s], strategy).view(-1,1)\n",
        "    return r_mean\n",
        "  \n",
        "  def partial_expected_reward_other(self):\n",
        "    r_mean = {}\n",
        "    for player in self.players():\n",
        "      r_mean[player] = {}\n",
        "    for s, player in self.state_player_pairs():\n",
        "        RM_i = self.R[player]\n",
        "        other_player = self.other_player(player)\n",
        "        strategy = self.pi[other_player][s].view(-1)\n",
        "        formula = 'ij,'+self.player_dim(other_player)+'->'+self.player_dim(player)\n",
        "        r_mean[player][s] = torch.einsum(formula, RM_i[s], strategy).view(-1,1)\n",
        "    return r_mean\n",
        "  \n",
        "  # TODO: consider other 2 cases or transition matrices\n",
        "  # TODO: consider more than 2 players\n",
        "  def next_value_matrices(self, state):\n",
        "    det, dep = self.transition_type[str(state)]\n",
        "    N_A1 = self.A['1'][str(state)]\n",
        "    N_A2 = self.A['2'][str(state)]\n",
        "    vs = torch.zeros((N_A1,N_A2,2), dtype=self.dtype).to(self.device)\n",
        "    if det and dep: \n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          _, next_state = self.transition_map(state, [a1,a2])\n",
        "          vs[a1,a2,:] = self.v[self.get_state_index(next_state),:]\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = self.transition_map(state, [])\n",
        "      next_v = torch.zeros((1,2), dtype=self.dtype).to(self.device)\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        next_v = next_v + self.v[self.get_state_index(next_state),:].view(1,-1) * transition_prob      \n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          vs[a1,a2,:] = next_v.view(-1)\n",
        "    return vs\n",
        "  \n",
        "  # TODO: consider more than 2 players\n",
        "  def partial_next_values(self):\n",
        "    # Create next-value dictionary for each player combination\n",
        "    next_v = {}\n",
        "    for player in self.players():\n",
        "      next_v[player] = {'1':{}, '2':{}}\n",
        "\n",
        "    # Fill dictionary \n",
        "    for state in self.S_no_string:\n",
        "      next_state_value_matrix = self.next_value_matrices(state)\n",
        "      strategy_1 = self.pi['1'][str(state)]\n",
        "      strategy_2 = self.pi['2'][str(state)]\n",
        "      \n",
        "      # Calculate mean next value when considering the strategy of one of the players. Output: array of size m^i(s)\n",
        "      for player_id in self.players_id():\n",
        "        next_v['1'][str(player_id+1)][str(state)] = torch.einsum('ij,jk->ik', next_state_value_matrix[:,:,player_id], strategy_2)\n",
        "        next_v['2'][str(player_id+1)][str(state)] = torch.einsum('ji,jk->ik', next_state_value_matrix[:,:,player_id], strategy_1)  \n",
        "    return next_v\n",
        "  \n",
        "  def next_value_dictionary(self):\n",
        "    next_v_dic = {}\n",
        "    for state in self.S_no_string:\n",
        "      next_v_dic[str(state)] = self.next_value_matrices(state)\n",
        "    return next_v_dic\n",
        "  \n",
        "  # TODO: consider more than 2 players\n",
        "  def bellman_projection(self):\n",
        "    r_mean = self.expected_reward()\n",
        "    next_v = torch.zeros_like(self.v)\n",
        "    for s in self.S_no_string:\n",
        "      next_state_value_matrix = self.next_value_matrices(s)\n",
        "      strategy_1 = self.pi['1'][str(s)].squeeze(1)\n",
        "      strategy_2 = self.pi['2'][str(s)].squeeze(1)\n",
        "      next_value_1 = torch.einsum('ijk,i->jk', next_state_value_matrix, strategy_1)\n",
        "      next_v[self.get_state_index(s),:] = torch.einsum('jk,j->k', next_value_1, strategy_2)\n",
        "    return r_mean + self.beta * next_v\n",
        "\n",
        "  # TODO: consider more than 2 players\n",
        "  def bellman_partial_projection(self):\n",
        "    r_mean = self.partial_expected_reward()\n",
        "    next_v = self.partial_next_values()\n",
        "\n",
        "    bellman_projection_dic = {'1':{}, '2':{}}\n",
        "    for player in self.players():\n",
        "      bellman_projection_dic[player] = {'1':{}, '2':{}}\n",
        "    for s, player in self.state_player_pairs():\n",
        "      for second_player in self.players():\n",
        "        bellman_projection_dic[player][second_player][s] = (\n",
        "            r_mean[player][second_player][s] + self.beta * next_v[player][second_player][s])\n",
        "    return bellman_projection_dic\n",
        "\n",
        "  # TODO: consider more than 2 players\n",
        "  def bellman_partial_projection_other(self):\n",
        "    r_mean = self.partial_expected_reward_other()\n",
        "    next_v = {'1':{}, '2':{}}\n",
        "    for state in self.S_no_string:\n",
        "      next_state_value_matrix = self.next_value_matrices(state)\n",
        "      strategy_1 = self.pi['1'][str(state)]\n",
        "      strategy_2 = self.pi['2'][str(state)]\n",
        "      # mean next value when considering the strategy of the other player. Output: array of size m^i(s)\n",
        "      next_v['1'][str(state)] = torch.einsum('ij,jk->ik', next_state_value_matrix[:,:,0], strategy_2)\n",
        "      next_v['2'][str(state)] = torch.einsum('ji,jk->ik', next_state_value_matrix[:,:,1], strategy_1)\n",
        "    bellman_projection_dic = {'1':{}, '2':{}}\n",
        "    for s, player in self.state_player_pairs():\n",
        "      bellman_projection_dic[player][s] = r_mean[player][s] + self.beta * next_v[player][s]\n",
        "    return bellman_projection_dic\n",
        "  \n",
        "  #-----------------------------------------------------------------------------\n",
        "  # Methods related with the linear optimization problem to find a feasible point (v given pi)\n",
        "\n",
        "  def reward_baselines(self):\n",
        "    r_mean = self.expected_reward()\n",
        "    r_baseline = r_mean.mean(0).view(-1,1).detach().cpu().numpy()\n",
        "    return r_baseline\n",
        "\n",
        "  def cost_vector_fixed_policies(self):\n",
        "    P = self.transition_matrix()\n",
        "    cost_vector = (1 - self.beta * P.sum(0)).view(-1,1).detach().cpu().numpy() / self.N_S\n",
        "    return cost_vector\n",
        "\n",
        "  def restriction_matrices_fixed_policies(self):\n",
        "    transition_matrices = self.partial_transition_matrices()\n",
        "    restriction_matrices = {'1':[], '2':[]}\n",
        "    for player in self.players():\n",
        "      for s in self.S:\n",
        "        temp_matrix = - self.beta * transition_matrices[s][player]\n",
        "        temp_matrix[:, self.get_state_index(s)] = temp_matrix[:, self.get_state_index(s)] + 1\n",
        "        restriction_matrices[player].append(temp_matrix.clone())\n",
        "      restriction_matrices[player] = -torch.cat(restriction_matrices[player], dim=0).detach().cpu().numpy()\n",
        "    return restriction_matrices\n",
        "\n",
        "  def restriction_vectors_fixed_policies(self, alpha):\n",
        "    r_mean = self.partial_expected_reward_other()\n",
        "    restriction_vectors = {'1':[], '2':[]}\n",
        "    for player in self.players():\n",
        "      for s in self.S:\n",
        "        restriction_vectors[player].append(r_mean[player][s].view(-1,1))\n",
        "      restriction_vectors[player] = -(torch.cat(restriction_vectors[player], dim=0)+alpha).detach().cpu().numpy()\n",
        "    return restriction_vectors\n",
        "\n",
        "  def parameters_fixed_policies(self, alpha):\n",
        "    cost = self.cost_vector_fixed_policies()\n",
        "    f0 = self.reward_baselines()\n",
        "    A_ub = self.restriction_matrices_fixed_policies()\n",
        "    b_ub = self.restriction_vectors_fixed_policies(alpha)\n",
        "    return cost, f0, A_ub, b_ub\n",
        "\n",
        "  def calculate_feasible_value(self, alpha=0.1):\n",
        "    v0 = np.zeros((self.N_S,2), dtype=np.float64)\n",
        "    cost, f0, A_ub, b_ub = self.parameters_fixed_policies(alpha)\n",
        "    for player_id in self.players_id():\n",
        "      temp_res = linprog(cost, A_ub=A_ub[str(player_id+1)], b_ub=b_ub[str(player_id+1)])\n",
        "      v0[:,player_id] = temp_res.x\n",
        "    return v0\n",
        "\n",
        "  def set_feasible_value(self, alpha=0.1):\n",
        "    v0 = self.calculate_feasible_value(alpha)\n",
        "    if self.dtype == torch.float64:\n",
        "      v0 = torch.DoubleTensor(v0).clone().to(self.device)\n",
        "    else:\n",
        "      v0 = torch.FloatTensor(v0).clone().to(self.device)\n",
        "    nn.init.zeros_(self.v)\n",
        "    with torch.no_grad():\n",
        "      self.v.data.add_(v0)  \n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # Methods related with the associated non-linear optimization problem\n",
        "\n",
        "  # Calculation of original target function: Bellman approximation error\n",
        "  def calculate_bellman_error(self):     \n",
        "    v_estimated = self.bellman_projection()\n",
        "    f_bellman = (self.v - v_estimated).sum()\n",
        "    return f_bellman\n",
        "\n",
        "  def calculate_nash_restrictions(self, hard_constraints):\n",
        "    q_estimated = self.bellman_partial_projection_other() # Dic. with an array of 'q'-values for each agent \n",
        "    g_nash = {'1':{}, '2':{}}\n",
        "    coef = 1.0\n",
        "    for s, player in self.state_player_pairs():\n",
        "      if not hard_constraints:\n",
        "        coef = self.pi[player][s].sum()\n",
        "      g_nash[player][s] = coef * q_estimated[player][s] - self.v[self.get_state_index(s), self.get_player_id(player)].item()\n",
        "    return g_nash\n",
        "  \n",
        "  def calculate_restriction_vector(self, hard_constraints):\n",
        "    g_nash = self.calculate_nash_restrictions(hard_constraints)\n",
        "    pi_vector = self.pi2vec()\n",
        "    pi_sum_vector = self.pi_sum()\n",
        "    g_list = []\n",
        "    for player in self.players():\n",
        "      for s in self.S:\n",
        "        g_list.append(g_nash[player][s])\n",
        "    for player in self.players():\n",
        "      g_list.append(-pi_vector[player])\n",
        "    for player in self.players():\n",
        "      g_list.append(pi_sum_vector[player]-1)\n",
        "    g_vector = torch.cat(g_list, dim=0)\n",
        "    return g_vector\n",
        "  \n",
        "  # Methods to calculate gradients\n",
        "  def bellman_error_gradients(self):\n",
        "    P = self.transition_matrix()\n",
        "    bellman_error_grad_v = (torch.eye(self.N_S).to(self.device) - self.beta*torch.t(P)).sum(1, keepdim=True)\n",
        "\n",
        "    q_individual = self.bellman_partial_projection()\n",
        "    bellman_error_grad_pi = {'1':{}, '2':{}}\n",
        "    for s, player in self.state_player_pairs():\n",
        "      bellman_error_grad_pi[player][s] = 0.0\n",
        "      for second_player in self.players():\n",
        "        bellman_error_grad_pi[player][s] = bellman_error_grad_pi[player][s] - q_individual[player][second_player][s]  \n",
        "    return bellman_error_grad_v, bellman_error_grad_pi\n",
        "\n",
        "\n",
        "  def project_bellman_error_gradients(self):\n",
        "    bellman_error_grad_pi = self.bellman_error_gradients()[1]\n",
        "    for s, player in self.state_player_pairs():\n",
        "      p = bellman_error_grad_pi[player][s]\n",
        "      one_vector = torch.ones_like(p)\n",
        "      projected_p = p - p.mean()*one_vector\n",
        "      bellman_error_grad_pi[player][s] = projected_p\n",
        "    return bellman_error_grad_pi\n",
        "\n",
        "  def nash_restriction_gradients(self, hard_constraints):\n",
        "    P_partial = self.partial_transition_matrices()\n",
        "    next_value_dic = self.next_value_dictionary()\n",
        "    consistent_R = self.player_consistent_reward_matrices()\n",
        "\n",
        "    nash_restriction_grad_v = {'1':{}, '2':{}}\n",
        "    nash_restriction_grad_pi = {'1':{}, '2':{}}\n",
        "\n",
        "    coef = 1.0\n",
        "    for s, player in self.state_player_pairs():\n",
        "      if not hard_constraints:\n",
        "        coef = self.pi[player][s].sum()\n",
        "      Delta = torch.zeros_like(P_partial[s][player])\n",
        "      Delta[:, self.get_state_index(s)] = 1.0\n",
        "      nash_restriction_grad_v[player][s] = coef * self.beta * P_partial[s][player] - Delta\n",
        "    \n",
        "      other_player = self.other_player(player)\n",
        "      if not hard_constraints:\n",
        "        coef = self.pi[other_player][s].sum()\n",
        "      next_value_matrix = next_value_dic[s][:,:,self.get_player_id(other_player)]\n",
        "      r_matrix = consistent_R[other_player][s] # the r_matrix and the next_value_matrix need to have as rows the actions of the other player\n",
        "      if player == '1':\n",
        "        next_value_matrix = torch.t(next_value_matrix)\n",
        "      nash_restriction_grad_pi[player][s] = coef * (r_matrix + self.beta * next_value_matrix)\n",
        "    return nash_restriction_grad_v, nash_restriction_grad_pi\n",
        "  \n",
        "  # TODO: consider more than 2 players\n",
        "  def bellman_error_gradient_vector(self):\n",
        "    grad_f_v, grad_f_pi = self.bellman_error_gradients() \n",
        "    grad_f_vector = torch.zeros((self.n_vars, 1), dtype=self.dtype).to(self.device)\n",
        "    \n",
        "    for i in range(0,2):\n",
        "      grad_f_vector[i*self.N_S:(i+1)*self.N_S,:] = grad_f_v.clone()\n",
        "    \n",
        "    for player in self.players():\n",
        "      grad_f_pi_list = []\n",
        "      for s in self.S:\n",
        "        if self.more_than_one_action(player, s):\n",
        "          grad_f_pi_list.append(grad_f_pi[player][s].clone())  \n",
        "      grad_f_pi_vector = torch.cat(grad_f_pi_list, dim=0)\n",
        "      y0 = 2*self.N_S + self.get_player_id(player) * self.N_A_reduced['1']\n",
        "      yf = 2*self.N_S + self.N_A_reduced['1'] + self.get_player_id(player) * self.N_A_reduced['2']\n",
        "      grad_f_vector[y0:yf,:] = grad_f_pi_vector\n",
        "\n",
        "    return grad_f_vector\n",
        "\n",
        "  def nash_restrictions_pi_mi_jacobian_matrix(self, player, grad_g_pi):\n",
        "    other_player = self.other_player(player)\n",
        "    grad_g_pi_list = []\n",
        "    s_not_added_list = []\n",
        "    for s in self.S:\n",
        "      if self.more_than_one_action(player, s):\n",
        "        grad = grad_g_pi[player][s].clone()\n",
        "        if len(s_not_added_list) == 0:\n",
        "          grad_g_pi_list.append(grad)\n",
        "        else:\n",
        "          n_null_rows = 0\n",
        "          for s_ in s_not_added_list:\n",
        "            n_null_rows += self.A[other_player][s_]\n",
        "          n_rows = n_null_rows + grad.shape[0]\n",
        "          n_cols = grad.shape[1]\n",
        "          null_rows_and_grad_g_pi = torch.zeros((n_rows, n_cols), dtype=self.dtype).to(self.device)\n",
        "          null_rows_and_grad_g_pi[-n_rows+n_null_rows:,:] = grad\n",
        "          grad_g_pi_list.append(null_rows_and_grad_g_pi)\n",
        "          s_not_added_list = []\n",
        "      else:\n",
        "        s_not_added_list.append(s)\n",
        "    if len(s_not_added_list) != 0:\n",
        "      n_null_rows = 0\n",
        "      for s_ in s_not_added_list:\n",
        "        n_null_rows += self.A[other_player][s_]\n",
        "      n_rows = n_null_rows + grad_g_pi_list[-1].shape[0]\n",
        "      n_cols = grad_g_pi_list[-1].shape[1]\n",
        "      null_rows_and_grad_g_pi = torch.zeros((n_rows, n_cols), dtype=self.dtype).to(self.device)\n",
        "      null_rows_and_grad_g_pi[:n_rows-n_null_rows,:] = grad_g_pi_list[-1].clone()\n",
        "      grad_g_pi_list[-1] = null_rows_and_grad_g_pi\n",
        "    \n",
        "    J_gmi_pi = torch.block_diag(*grad_g_pi_list)\n",
        "    return J_gmi_pi\n",
        "\n",
        "  def nash_restrictions_pi_i_jacobian_matrix(self, player, g_nash_pure):    \n",
        "    grad_g_pi_list = []\n",
        "    s_not_added_list = []\n",
        "    player_id = self.get_player_id(player)\n",
        "    for s in self.S:\n",
        "      if self.more_than_one_action(player, s):\n",
        "        NA = self.A[player][s]\n",
        "        ones = torch.ones((NA, NA), dtype=self.dtype).to(self.device)\n",
        "        s_id = self.get_state_index(s)\n",
        "        g = g_nash_pure[player][s].view(-1) + self.v[s_id, player_id].item() \n",
        "        grad = torch.einsum('ij,jk->ik', torch.diag(g), ones)\n",
        "        if len(s_not_added_list) == 0:\n",
        "          grad_g_pi_list.append(grad)\n",
        "        else:\n",
        "          n_null_rows = 0\n",
        "          for s_ in s_not_added_list:\n",
        "            n_null_rows += self.A[player][s_]\n",
        "          n_rows = n_null_rows + grad.shape[0]\n",
        "          n_cols = grad.shape[1]\n",
        "          null_rows_and_grad_g_pi = torch.zeros((n_rows, n_cols), dtype=self.dtype).to(self.device)\n",
        "          null_rows_and_grad_g_pi[-n_rows+n_null_rows:,:] = grad\n",
        "          grad_g_pi_list.append(null_rows_and_grad_g_pi)\n",
        "          s_not_added_list = []\n",
        "      else:\n",
        "        s_not_added_list.append(s)\n",
        "    if len(s_not_added_list) != 0:\n",
        "      n_null_rows = 0\n",
        "      for s_ in s_not_added_list:\n",
        "        n_null_rows += self.A[player][s_]\n",
        "      n_rows = n_null_rows + grad_g_pi_list[-1].shape[0]\n",
        "      n_cols = grad_g_pi_list[-1].shape[1]\n",
        "      null_rows_and_grad_g_pi = torch.zeros((n_rows, n_cols), dtype=self.dtype).to(self.device)\n",
        "      null_rows_and_grad_g_pi[:n_rows-n_null_rows,:] = grad_g_pi_list[-1].clone()\n",
        "      grad_g_pi_list[-1] = null_rows_and_grad_g_pi\n",
        "    \n",
        "    J_gi_pi = torch.block_diag(*grad_g_pi_list)\n",
        "    return J_gi_pi\n",
        "\n",
        "  def build_grad_tensors(self, hard_constraints):           \n",
        "    grad_f_vector = self.bellman_error_gradient_vector()\n",
        "    grad_g_v, grad_g_pi = self.nash_restriction_gradients(hard_constraints)\n",
        "    grad_g_matrix = torch.zeros((self.n_restrictions, self.n_vars), dtype=self.dtype).to(self.device)\n",
        "    \n",
        "    if not hard_constraints:\n",
        "      g_nash_pure = self.calculate_nash_restrictions(hard_constraints=True)\n",
        "\n",
        "    for player in self.players():\n",
        "      other_player = self.other_player(player)\n",
        "\n",
        "      # fill jacobian with restriction gradients for v\n",
        "      grad_g_v_list = [grad_g_v[player][s].clone() for s in self.S]\n",
        "      J_gi_vi = torch.cat(grad_g_v_list, dim=0)\n",
        "      y0 = self.get_player_id(player) * self.N_A_total['1']\n",
        "      yf = self.N_A_total['1'] + self.get_player_id(player) * self.N_A_total['2']\n",
        "      x0 = self.get_player_id(player) * self.N_S\n",
        "      xf = x0 + self.N_S\n",
        "      grad_g_matrix[y0:yf,x0:xf] = J_gi_vi\n",
        "\n",
        "      # fill jacobian with nash restriction gradients for pi \n",
        "      J_gmi_pi = self.nash_restrictions_pi_mi_jacobian_matrix(player, grad_g_pi)\n",
        "      y0 = self.get_player_id(other_player) * self.N_A_total['1']\n",
        "      yf = self.N_A_total['1'] + self.get_player_id(other_player) * self.N_A_total['2']\n",
        "      x0 = 2*self.N_S + self.get_player_id(player) * self.N_A_reduced['1']\n",
        "      xf = 2*self.N_S + self.N_A_reduced['1'] + self.get_player_id(player) * self.N_A_reduced['2']\n",
        "      grad_g_matrix[y0:yf,x0:xf] = J_gmi_pi\n",
        "\n",
        "      if not hard_constraints:\n",
        "        J_gi_pi = self.nash_restrictions_pi_i_jacobian_matrix(player, g_nash_pure)\n",
        "        y0 = self.get_player_id(player) * self.N_A_total['1']\n",
        "        yf = self.N_A_total['1'] + self.get_player_id(player) * self.N_A_total['2']\n",
        "        grad_g_matrix[y0:yf,x0:xf] = J_gi_pi\n",
        "\n",
        "      # fill jacobian with positivity restriction gradients for pi \n",
        "      J_gpi_plus = -torch.eye(self.N_A_reduced[player]).to(self.device)\n",
        "      y0 = self.N_A_total['1'] + self.N_A_total['2'] + self.get_player_id(player) * self.N_A_reduced['1']\n",
        "      yf = (self.N_A_total['1'] + self.N_A_total['2'] + self.N_A_reduced['1'] \n",
        "            + self.get_player_id(player) * self.N_A_reduced['2'])\n",
        "      grad_g_matrix[y0:yf,x0:xf] = J_gpi_plus\n",
        "\n",
        "      # fill jacobian with unitary sum restriction gradients for pi\n",
        "      grad_gpi_one_list = []\n",
        "      for s in self.S:\n",
        "        NA = self.A[player][s]\n",
        "        if self.more_than_one_action(player, s):\n",
        "          grad_gpi_one_list.append(torch.ones((1,NA), dtype=self.dtype).to(self.device))\n",
        "\n",
        "      J_gpi_one = torch.block_diag(*grad_gpi_one_list)\n",
        "      y0 = (self.N_A_total['1'] + self.N_A_total['2'] + self.N_A_reduced['1'] \n",
        "            + self.N_A_reduced['2'] + self.get_player_id(player) * self.N_S_reduced['1'])\n",
        "      yf = (self.N_A_total['1'] + self.N_A_total['2'] + self.N_A_reduced['1'] + self.N_A_reduced['2'] \n",
        "            + self.N_S_reduced['1'] + self.get_player_id(player) * self.N_S_reduced['2'])\n",
        "      grad_g_matrix[y0:yf,x0:xf] = J_gpi_one\n",
        "\n",
        "    return grad_f_vector, grad_g_matrix\n",
        "  \n",
        "\n",
        "  def project_grad_on_equality_manifolds(self):\n",
        "    grad_f_vector, grad_g_matrix = self.build_grad_tensors(True)\n",
        "    N_S_reduced = self.N_S_reduced['1'] + self.N_S_reduced['2']\n",
        "    grad_g_matrix_eq = grad_g_matrix[-N_S_reduced:,:].T.detach().numpy()\n",
        "    if dtype == torch.float64:\n",
        "      grad_g_orth_basis = torch.DoubleTensor(orth(grad_g_matrix_eq)).to(self.device)\n",
        "    else:\n",
        "      grad_g_orth_basis = torch.FloatTensor(orth(grad_g_matrix_eq)).to(self.device)\n",
        "    dots = torch.einsum('ji,jk->ik', grad_g_orth_basis, grad_f_vector)\n",
        "    projection = torch.einsum('ji,ik->jk', grad_g_orth_basis, dots)\n",
        "    grad_f_vector_projected = grad_f_vector - projection\n",
        "    return grad_f_vector_projected\n",
        "\n",
        "\n",
        "  def calculate_descent_direction(self, g_vector, grad_f_vector, grad_g_matrix, r_value=1.0):\n",
        "    g_vector_ineq = self.mask_inequality_restrictions(g_vector)\n",
        "    g_vector_eq = self.mask_equality_restrictions(g_vector)\n",
        "    g_diag_matrix = torch.diag(r_value * g_vector_ineq.view(-1))\n",
        "                                            \n",
        "    A_matrix = g_diag_matrix - torch.einsum('ik,jk->ij', grad_g_matrix, grad_g_matrix) \n",
        "    b_vector = torch.einsum('ij,jk->ik', grad_g_matrix, grad_f_vector) - r_value * g_vector_eq \n",
        "    duals_0_vector = torch.solve(b_vector, A_matrix)[0]\n",
        "\n",
        "    d0_vector = - grad_f_vector - torch.einsum('ij,ik->jk', grad_g_matrix, duals_0_vector)\n",
        "    norm_2_d0 = d0_vector.pow(2).sum().item() \n",
        "    return d0_vector, norm_2_d0, duals_0_vector, A_matrix, b_vector\n",
        "\n",
        "  def calculate_feasible_direction(self, g_vector, c_vector, duals_0_vector, \n",
        "                                  norm_2_d0, A_matrix, b_vector, rho,\n",
        "                                  grad_f_vector, grad_g_matrix,\n",
        "                                  alpha=0.5, r_value=1.0):\n",
        "    mWe = torch.solve(torch.ones_like(b_vector), A_matrix)[0]\n",
        "    g_vector_eq = self.mask_equality_restrictions(g_vector)\n",
        "    dot = (r_value * g_vector_eq.view(-1) * mWe.view(-1)).sum()\n",
        "\n",
        "    new_rho = rho\n",
        "    div = (duals_0_vector.sum() + c_vector.sum() + dot).item()\n",
        "    if div > 0:\n",
        "        rho_1 = (1.0-alpha) / div\n",
        "        if rho_1 < rho:\n",
        "          new_rho = 0.5 * rho_1\n",
        "\n",
        "    duals_vector = torch.solve(b_vector - new_rho * norm_2_d0, A_matrix)[0]\n",
        "    d_vector = - grad_f_vector - torch.einsum('ij,ik->jk', grad_g_matrix, duals_vector)\n",
        "    return d_vector, duals_vector, new_rho, mWe\n",
        "\n",
        "  @staticmethod\n",
        "  def calculate_auxiliary_bellman_error(f, g_vector, c_vector):\n",
        "    return (f - (c_vector.view(-1) * g_vector.view(-1)).sum()).item()\n",
        "\n",
        "  def feasible_gradient_descent_v2(self, f, d_v, d_pi, hard_constraints, max_steps=1200,\n",
        "                                   verbose=False, on=True, alpha_0=1.0, eps=1e-6):\n",
        "    # Procedure to find feasible step size \n",
        "    found_feasible_step_size = False\n",
        "    step_size = alpha_0\n",
        "    n_step = 0\n",
        "    max_steps = max(1, max_steps)\n",
        "    step_size_hist = []\n",
        "    while (not found_feasible_step_size) and (n_step < max_steps):\n",
        "      step_size_hist.append(step_size)\n",
        "      game_temp = self.copy_game()\n",
        "      # Update parameters performing step in feasible descent direction \n",
        "      game_temp.v.data.add_(step_size * d_v)\n",
        "      for s, player in self.state_player_pairs():\n",
        "        if game_temp.more_than_one_action(player, s):\n",
        "          game_temp.pi[player][s].data.add_(step_size * d_pi[player][s])\n",
        "\n",
        "      f_temp = game_temp.calculate_bellman_error()\n",
        "      g_vector_temp = game_temp.calculate_restriction_vector(hard_constraints)\n",
        "      \n",
        "      theta_decreased = f_temp <= f - eps\n",
        "      NAt = self.N_A_total['1'] + self.N_A_total['2']\n",
        "      g_v_valid = torch.all(g_vector_temp[:NAt] <= 0.0).item()\n",
        "      g_pi_valid = torch.all(g_vector_temp[NAt:] <= 0.0).item()\n",
        "      g_valid = g_v_valid and g_pi_valid\n",
        "      \n",
        "      # Check if the current step size results in a feasible descent direction\n",
        "      if theta_decreased and g_valid:\n",
        "        found_feasible_step_size = True\n",
        "      else:        \n",
        "        step_size /= nu\n",
        "      n_step += 1\n",
        "      if verbose:        \n",
        "        print(\"Step: {}, Step size: {:.3e}, Found feasible: {}\".format(\n",
        "            n_step, step_size, found_feasible_step_size))\n",
        "        print(theta_decreased, g_v_valid, g_pi_valid)\n",
        "        print(\"Step: {}, f: {:.3e}, f new: {:.3e}, theta: {:.3e}, theta new: {:.3e}\".format(\n",
        "            n_step, f.item(), f_temp.item(), theta, theta_temp))\n",
        "      \n",
        "    if found_feasible_step_size and on:\n",
        "      self.v.data.add_(step_size * d_v)\n",
        "      for s, player in self.state_player_pairs():\n",
        "        if self.more_than_one_action(player, s):\n",
        "          self.pi[player][s].data.add_(step_size * d_pi[player][s])\n",
        "\n",
        "    return found_feasible_step_size, n_step, f_temp, step_size, step_size_hist\n",
        "\n",
        "  def active_restrictions(self, eps=1e-4): \n",
        "    active = self.calculate_restriction_vector(True).T > -eps\n",
        "    return active\n",
        "  \n",
        "  def inactive_restrictions(self):\n",
        "    return torch.bitwise_not(self.active_restrictions())\n",
        "\n",
        "  def linear_backtracking(self, f, grad_f_v, grad_f_pi, init_step_size=1e-3,\n",
        "                          max_steps=40, nu=1.1, verbose=False, on=True):\n",
        "    \n",
        "    # Procedure to find feasible step size \n",
        "    active_mask = self.active_restrictions()\n",
        "    found_feasible_step_size = False\n",
        "    step_size = init_step_size\n",
        "    n_step = 0\n",
        "    max_steps = max(1, max_steps)\n",
        "    while (not found_feasible_step_size) and (n_step < max_steps):\n",
        "      game_temp = self.copy_game()\n",
        "      # Update parameters performing step in feasible descent direction \n",
        "      game_temp.v.data.add_(-step_size * grad_f_v)\n",
        "      for s, player in self.state_player_pairs():\n",
        "        if game_temp.more_than_one_action(player, s):\n",
        "          game_temp.pi[player][s].data.add_(-step_size * grad_f_pi[player][s])\n",
        "      \n",
        "      f_temp = game_temp.calculate_bellman_error()\n",
        "      f_decreased = f - f_temp > 0.0\n",
        "      \n",
        "      inactive = game_temp.inactive_restrictions()\n",
        "      no_new_restrictions = torch.all(inactive.bitwise_or(active_mask))\n",
        "\n",
        "      # Check if the current step size results in a feasible descent direction\n",
        "      if f_decreased and no_new_restrictions:\n",
        "        found_feasible_step_size = True\n",
        "      else:        \n",
        "        step_size /= nu\n",
        "      n_step += 1\n",
        "      if verbose:        \n",
        "        print(\"Step: {}, Step size: {:.3e}, Found feasible: {}\".format(\n",
        "            n_step, step_size, found_feasible_step_size))\n",
        "        print(theta_decreased, g_v_valid, g_pi_valid)\n",
        "        print(\"Step: {}, f: {:.3e}, f new: {:.3e}, theta: {:.3e}, theta new: {:.3e}\".format(\n",
        "            n_step, f.item(), f_temp.item(), theta, theta_temp))\n",
        "\n",
        "    return step_size, n_step, f_temp, found_feasible_step_size \n",
        "  \n",
        "  def perform_gradient_descent_step(self, grad_f_v, grad_f_pi, step_size):\n",
        "    self.v.data.add_(-step_size * grad_f_v)\n",
        "    for s, player in self.state_player_pairs():\n",
        "      if self.more_than_one_action(player, s):\n",
        "        self.pi[player][s].data.add_(-step_size * grad_f_pi[player][s])\n",
        "\n",
        "  def feasible_gradient_descent(self, f, g_vector, c_vector, d_vector,\n",
        "                                grad_f_vector, grad_g_matrix, d_v, d_pi, \n",
        "                                duals_vector, hard_constraints, bound_step_size, max_steps=1200, \n",
        "                                eta=0.1, nu=1.1, gamma_0=0.5, verbose=False, on=True):\n",
        "    \n",
        "    # Calculate loss function\n",
        "    theta = self.calculate_auxiliary_bellman_error(f, g_vector, c_vector)\n",
        "    \n",
        "    # Calculate required decrement in the loss function\n",
        "    grad_theta_vector = grad_f_vector - torch.einsum('ij,ik->jk', grad_g_matrix, c_vector)\n",
        "    decrement = (d_vector.view(-1) * grad_theta_vector.view(-1)).sum().item()\n",
        "    \n",
        "    # Procedure to find feasible step size \n",
        "    found_feasible_step_size = False\n",
        "    step_size = 1.0\n",
        "    n_step = 0\n",
        "    max_steps = max(1, max_steps)\n",
        "    step_size_hist = []\n",
        "    while (not found_feasible_step_size) and (n_step < max_steps):\n",
        "      step_size_hist.append(step_size)\n",
        "      game_temp = self.copy_game()\n",
        "      # Update parameters performing step in feasible descent direction \n",
        "      game_temp.v.data.add_(step_size * d_v)\n",
        "      for s, player in self.state_player_pairs():\n",
        "        if game_temp.more_than_one_action(player, s):\n",
        "          game_temp.pi[player][s].data.add_(step_size * d_pi[player][s])\n",
        "\n",
        "      f_temp = game_temp.calculate_bellman_error()\n",
        "      g_vector_temp = game_temp.calculate_restriction_vector(hard_constraints)\n",
        "      theta_temp = game_temp.calculate_auxiliary_bellman_error(f_temp, g_vector_temp, c_vector)      \n",
        "      \n",
        "      gamma = gamma_0 * torch.ones_like(duals_vector)\n",
        "      gamma[duals_vector < 0] = 1.0\n",
        "      gamma[-self.N_S_reduced['1']-self.N_S_reduced['2']:,:] = 0.0\n",
        "\n",
        "      dtheta = theta_temp-theta\n",
        "      theta_decreased = ((dtheta / eta) <= (step_size * decrement))\n",
        "      NAt = self.N_A_total['1'] + self.N_A_total['2']\n",
        "      g_v_valid = torch.all(g_vector_temp[:NAt] <= (g_vector * gamma)[:NAt]).item()\n",
        "      g_pi_valid = torch.all(g_vector_temp[NAt:] <= (g_vector * gamma)[NAt:]).item()\n",
        "      g_valid = g_v_valid and g_pi_valid\n",
        "      \n",
        "      # Check if the current step size results in a feasible descent direction\n",
        "      if theta_decreased and g_valid:\n",
        "        found_feasible_step_size = True\n",
        "      else:        \n",
        "        step_size /= nu\n",
        "      n_step += 1\n",
        "      if verbose:        \n",
        "        print(\"Step: {}, Step size: {:.3e}, Found feasible: {}\".format(\n",
        "            n_step, step_size, found_feasible_step_size))\n",
        "        print(theta_decreased, g_v_valid, g_pi_valid)\n",
        "        print(\"Step: {}, f: {:.3e}, f new: {:.3e}, theta: {:.3e}, theta new: {:.3e}\".format(\n",
        "            n_step, f.item(), f_temp.item(), theta, theta_temp))\n",
        "      \n",
        "    if found_feasible_step_size and on:\n",
        "      self.v.data.add_(step_size * d_v)\n",
        "      for s, player in self.state_player_pairs():\n",
        "        if self.more_than_one_action(player, s):\n",
        "          self.pi[player][s].data.add_(step_size * d_pi[player][s])\n",
        "\n",
        "    return found_feasible_step_size, n_step, f_temp, step_size, step_size_hist\n",
        "\n",
        "  def optimize_game(self, hard_constraints=True, max_steps=1200, n_epochs=100, verbose=False, c=1.0, rho_0=1.0, on=True, eta_0=1e-3, nu=1.1, print_each=5, r_value=1.0):\n",
        "    f_0 = self.calculate_bellman_error()\n",
        "    eta = eta_0\n",
        "    rho = rho_0\n",
        "    c_vector = c*torch.ones((self.n_restrictions,1), dtype=self.dtype).to(self.device)\n",
        "    c_vector = self.mask_equality_restrictions(c_vector)\n",
        "    step_sizes_list = []\n",
        "    step_size_hists = []\n",
        "    step_size_bounds_list = []\n",
        "    dual_vectors_list = []\n",
        "    c_vectors_list = []\n",
        "    rhos_list = []\n",
        "    with torch.no_grad():\n",
        "      for epoch in range(0, n_epochs):\n",
        "        f = self.calculate_bellman_error()\n",
        "        g_vector = self.calculate_restriction_vector(hard_constraints)\n",
        "        grad_f_vector, grad_g_matrix = self.build_grad_tensors(hard_constraints)\n",
        "        \n",
        "        d0_vector, norm_2_d0, duals_0_vector, A_matrix, b_vector = self.calculate_descent_direction(g_vector, grad_f_vector, grad_g_matrix, r_value=r_value)\n",
        "        c_vector = self.update_c_vector(c_vector, duals_0_vector)\n",
        "        c_vectors_list.append(c_vector)\n",
        "        \n",
        "        d_vector, duals_vector, new_rho, mWe = self.calculate_feasible_direction(g_vector, c_vector, duals_0_vector, \n",
        "                                                                            norm_2_d0, A_matrix, b_vector, rho, \n",
        "                                                                            grad_f_vector, grad_g_matrix, r_value=r_value)\n",
        "        dual_vectors_list.append(duals_vector)\n",
        "        rhos_list.append(new_rho)\n",
        "        d_v, d_pi, duals = self.vec2dic(d_vector, duals_vector)\n",
        "\n",
        "        g_nash_satisfied, max_g_nash, prod_zero_satisfied, max_prod_zero = self.check_nash_KKT_conditions(duals, hard_constraints)\n",
        "        pi_KKT = self.check_pi_KKT_conditions(duals)\n",
        "        g_pi_plus_satisfied, max_g_pi_plus, prod_zero_plus_satisfied, max_prod_zero_plus = pi_KKT[:4] \n",
        "        g_pi_one_satisfied, max_g_pi_one, prod_zero_one_satisfied, max_prod_zero_one = pi_KKT[4:]\n",
        "\n",
        "        bound_step_size = self.calculate_bound_step_size(duals_vector, c_vector, new_rho, norm_2_d0, grad_g_matrix, mWe)\n",
        "        step_size_bounds_list.append(bound_step_size)\n",
        "\n",
        "        found_feasible_step_size, n_step, f_new, step_size, step_size_hist = self.feasible_gradient_descent(f, g_vector, c_vector, d_vector, \n",
        "                                                                                  grad_f_vector, grad_g_matrix, d_v, d_pi, \n",
        "                                                                                  duals_vector, hard_constraints, bound_step_size, verbose=verbose, \n",
        "                                                                                  max_steps=max_steps, on=on, eta=eta, nu=nu) # TODO: check if passing wrong grad_f\n",
        "        step_sizes_list.append(step_size)\n",
        "        step_size_hists.append(step_size_hist)\n",
        "        if found_feasible_step_size:\n",
        "          #eta /= 10.0\n",
        "          rho = new_rho\n",
        "          delta_f = (f_new - f_0) / f_0 * 100\n",
        "        else:\n",
        "          break\n",
        "        \n",
        "        if ((epoch + 1) % print_each == 0):\n",
        "          print('Epoch: {}, f: {:.5e}, delta f: {:.5e}%, nash satisfied: {}, max g: {:.3e}, dual nash satisfied: {}, max product: {:.3e}, rho: {:.3e}, norm2 d0:{:.3e}, ss: {:.3e}'.format(\n",
        "              epoch, f_new.item(), delta_f.item(), g_nash_satisfied, max_g_nash, prod_zero_satisfied, max_prod_zero, rho, norm_2_d0, step_size))\n",
        "          \n",
        "          print('Epoch: {}, >0 satisfied: {}, max g>0: {:.3e}, dual >0 satisfied: {}, max product >0: {:.3e}, =1 satisfied: {}, max g=1: {:.3e}, dual =1 satisfied: {}, max product =1: {:.3e}'.format(\n",
        "            epoch, g_pi_plus_satisfied, max_g_pi_plus, prod_zero_plus_satisfied, max_prod_zero_plus, g_pi_one_satisfied, max_g_pi_one, prod_zero_one_satisfied, max_prod_zero_one))\n",
        "                  \n",
        "    self.save()\n",
        "    if self.dtype == torch.float64:\n",
        "      step_sizes = torch.DoubleTensor(step_sizes_list).to(self.device)\n",
        "      step_size_bounds = torch.DoubleTensor(step_size_bounds_list).to(self.device)\n",
        "      rhos = torch.DoubleTensor(rhos_list).to(self.device)\n",
        "    else:\n",
        "      step_sizes = torch.FloatTensor(step_sizes_list).to(self.device)\n",
        "      step_size_bounds = torch.FloatTensor(step_size_bounds_list).to(self.device)\n",
        "      rhos = torch.FloatTensor(rhos_list).to(self.device)\n",
        "    return step_sizes, step_size_bounds, dual_vectors_list, rhos, c_vectors_list, step_size_hists\n",
        "  \n",
        "  def calculate_max_eigen_g_hessian(self):\n",
        "    mu_list = []\n",
        "    grad_g_v_dic, grad_g_pi_dic = self.nash_restriction_gradients(True)\n",
        "    dim = self.N_S + self.N_A_reduced['1'] + self.N_A_reduced['2']\n",
        "    NS = self.N_S\n",
        "    for player in self.players():\n",
        "      other_player = self.other_player(player)\n",
        "      NAi = self.N_A_reduced[player]\n",
        "      NAmi = self.N_A_reduced[other_player]\n",
        "      for state in self.S_no_string:\n",
        "        s = str(state)\n",
        "        pi_sum = self.pi[player][s].sum()\n",
        "        next_v_matrix = grad_g_pi_dic[other_player][s]\n",
        "        grad_g_v_matrix = grad_g_v_dic[player][s]\n",
        "        for ai in range(0, self.A[player][s]):\n",
        "          Tr = self.transition_matrix_given_sai(player, state, ai)\n",
        "          M14 = self.beta * pi_sum * Tr \n",
        "          # next_v = next_v_matrix[ai,:].view(-1)\n",
        "          # ones_1 = torch.ones(NAi,NAmi).to(self.device)\n",
        "          # M34 = torch.einsum('ij,jk->ik', ones_1, torch.diag(next_v))\n",
        "          Ri = self.R[player][s].clone()\n",
        "          if player == '2':\n",
        "            Ri = Ri.T\n",
        "          v_player = self.v[:,self.get_player_id(player)].view(-1,1)\n",
        "          next_v = Ri[ai,:].view(1,-1) + self.beta * torch.einsum('ji,jk->ki', Tr, v_player)\n",
        "          ones_1 = torch.ones((NAi,1), dtype=self.dtype).to(self.device)\n",
        "          M34 = torch.einsum('ij,jk->ik', ones_1, next_v)\n",
        "          \n",
        "          # p = grad_g_v_matrix[ai,:].view(-1)\n",
        "          # id_s = self.get_state_index(s)\n",
        "          # p[id_s] += 1\n",
        "          # ones_2 = torch.ones(NS,NAi).to(self.device)\n",
        "          # M13 = torch.einsum('ij,jk->ik', torch.diag(p), ones_2)\n",
        "          p = torch.einsum('ij,jk->ik', Tr, self.pi[other_player][s]) #grad_g_v_matrix[ai,:].view(-1)\n",
        "          ones_2 = torch.ones((NAi,1), dtype=self.dtype).to(self.device)\n",
        "          M13 = self.beta * torch.einsum('ij,kj->ik', p, ones_2)\n",
        "\n",
        "          # print(M13.shape, M14.shape, M34.shape)\n",
        "\n",
        "          M = torch.zeros((dim, dim), dtype=self.dtype).to(self.device)\n",
        "          M[:NS,NS:NS+NAi] = M13.clone()\n",
        "          M[:NS,NS+NAi:] = M14.clone()\n",
        "          M[NS:NS+NAi,:NS] = M13.T.clone()\n",
        "          M[NS:NS+NAi,NS+NAi:] = M34.clone()\n",
        "          M[NS+NAi:,:NS] = M14.T.clone()\n",
        "          M[NS+NAi:,NS:NS+NAi] = M34.T.clone()\n",
        "\n",
        "          M_eigval, M_eigvec = torch.eig(M)\n",
        "          mu_list.append(M_eigval.max())\n",
        "    mu_vector = torch.DoubleTensor(mu_list).view(-1,1).to(self.device)\n",
        "    return mu_vector\n",
        "\n",
        "  def calculate_bound_step_size(self, duals_vector, c_vector, rho, d0_2norm, \n",
        "                                grad_g_matrix, mWe, gamma_0=0.5, r_value=1.0):\n",
        "    beta_1_tau = 1 + rho * d0_2norm**0.5 * torch.einsum('ji,jk->ik', grad_g_matrix, mWe).pow(2).sum()**0.5\n",
        "    beta_tau = beta_1_tau**(-2)\n",
        "    if self.N_S == 1:\n",
        "      mu_vector = self.calculate_max_eigen_g_hessian()\n",
        "    else:\n",
        "      mu_vector = torch.ones_like(duals_vector)\n",
        "    tau_lambda = (1-gamma_0) / (r_value * (duals_vector + c_vector).max())\n",
        "    tau_mu = 2 * beta_tau * rho / mu_vector.max()\n",
        "    tau = np.infty\n",
        "    if duals_vector.max() > 0.0:\n",
        "      tau = tau_lambda.item()\n",
        "      if mu_vector.max() > 0.0:\n",
        "        tau = min(tau, tau_mu.item())\n",
        "    else:\n",
        "      if mu_vector.max() > 0.0:\n",
        "        tau = tau_mu.item()\n",
        "    return tau\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # Methods related with the KKT conditions\n",
        "\n",
        "  def check_nash_KKT_conditions(self, lambda_nash, hard_constraints, tol=1e-8):\n",
        "    with torch.no_grad():\n",
        "      # Calculate restrictions\n",
        "      g_nash = self.calculate_nash_restrictions(hard_constraints)\n",
        "\n",
        "      g_nash_satisfied = True\n",
        "      product_zero_satisfied = True\n",
        "      \n",
        "      max_g_nash = -np.infty\n",
        "      max_product_zero = -np.infty\n",
        "      \n",
        "      for player in self.players():\n",
        "        remaining_duals = lambda_nash[player].clone()\n",
        "        for s in self.S:\n",
        "          if self.more_than_one_action(player, s):\n",
        "            NA = self.A[player][s]\n",
        "            g_nash_satisfied = g_nash_satisfied and torch.all(g_nash[player][s] <= 0)\n",
        "            max_g_nash = max(max_g_nash, g_nash[player][s].max().item())\n",
        "\n",
        "            lambda_g_nash_product = g_nash[player][s].view(-1) * remaining_duals[:NA,:].view(-1)\n",
        "            product_zero_satisfied = product_zero_satisfied and torch.all(lambda_g_nash_product.abs() <= tol)\n",
        "            max_product_zero = max(max_product_zero, lambda_g_nash_product.abs().max().item())\n",
        "            remaining_duals = remaining_duals[NA:,:]\n",
        "      return g_nash_satisfied, max_g_nash, product_zero_satisfied, max_product_zero\n",
        "\n",
        "  def check_pi_KKT_conditions(self, duals, tol=1e-8):\n",
        "    pi_vector = self.pi2vec()\n",
        "    pi_sum_vector = self.pi_sum()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      g_pi_plus_satisfied = True\n",
        "      g_pi_one_satisfied = True\n",
        "\n",
        "      product_zero_plus_satisfied = True\n",
        "      product_zero_one_satisfied = True\n",
        "\n",
        "      max_g_pi_plus = -np.infty\n",
        "      max_g_pi_one = -np.infty\n",
        "\n",
        "      max_product_zero_plus = -np.infty\n",
        "      max_product_zero_one = -np.infty\n",
        "\n",
        "      for player in self.players():\n",
        "        g_pi_plus_satisfied = g_pi_plus_satisfied and torch.all(pi_vector[player] >= 0)\n",
        "        g_pi_one_satisfied = g_pi_one_satisfied and torch.all(pi_sum_vector[player] <= 1)\n",
        "\n",
        "        max_g_pi_plus = max(max_g_pi_plus, -pi_vector[player].min().item())\n",
        "        max_g_pi_one = max(max_g_pi_one, (pi_sum_vector[player]-1).min().item())\n",
        "\n",
        "        NAt = self.N_A_total[player]\n",
        "        NAr = self.N_A_reduced[player]\n",
        "        lambda_g_pi_plus_product = -pi_vector[player].view(-1) * duals[player][NAt:NAt+NAr,:].view(-1) # TODO: fix for assymetric number of actions\n",
        "        product_zero_plus_satisfied = product_zero_plus_satisfied and torch.all(lambda_g_pi_plus_product.abs() <= tol)\n",
        "        max_product_zero_plus = max(max_product_zero_plus, lambda_g_pi_plus_product.abs().max().item())\n",
        "        lambda_g_pi_one_product = (pi_sum_vector[player].view(-1)-1) * duals[player][NAt+NAr:,:].view(-1)\n",
        "        product_zero_one_satisfied = product_zero_one_satisfied and torch.all(lambda_g_pi_one_product.abs() <= tol)\n",
        "        max_product_zero_one = max(max_product_zero_one, lambda_g_pi_one_product.abs().max().item())\n",
        "    \n",
        "      return (g_pi_plus_satisfied, max_g_pi_plus, product_zero_plus_satisfied, max_product_zero_plus, \n",
        "              g_pi_one_satisfied, max_g_pi_one, product_zero_one_satisfied, max_product_zero_one)   \n",
        "\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  # Methods related with manifold optimization\n"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZNA9Fv1-1-h"
      },
      "source": [
        "# Prisoner's Dilemma With Incentives\n",
        "\n",
        "Now, we will create an instance of a stochastic game that corresponds to the prisoner's dilemma game. This game will include a modification that allows for mutual incentives between the prisoners. For this, we first need to specify the required player, state, and action sets $N, S, A$, and the reward and transition maps $R,T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPVt4jdFpH13"
      },
      "source": [
        "# Define player set N\n",
        "N = 2\n",
        "\n",
        "# Define state set S\n",
        "M = 5\n",
        "G = [('G',)]\n",
        "O = [('O',i) for i in range(1,2+1)]\n",
        "E1 = [('E1',i,j) for i in range(0,2) for j in range(0,M+1)]\n",
        "E2 = [('E2',i,j) for i in range(0,2) for j in range(0,M+1)]\n",
        "R1 = [('R1',i) for i in range(0,2)]\n",
        "R2 = [('R2',i) for i in range(0,2)]\n",
        "S = list(itertools.chain(G.copy(), O.copy(), E1.copy(), E2.copy(), R1.copy(), R2.copy())) \n",
        "\n",
        "# Define action set A^i(s) per player and state\n",
        "A = {'1':{}, '2':{}}\n",
        "for player_id in range(0, N):\n",
        "  player = str(player_id+1)\n",
        "  other_player_id = 1-player_id\n",
        "  other_player = str(other_player_id+1)\n",
        "  A[player][str(('G',))] = 2\n",
        "  A[player][str(('O',player_id+1))] = 2*(M+1)\n",
        "  A[player][str(('O',other_player_id+1))] = 1\n",
        "  for i in range(0,2):\n",
        "    for j in range(0,M+1):\n",
        "      A[player][str(('E'+player,i,j))] = 2\n",
        "      A[player][str(('E'+other_player,i,j))] = 1\n",
        "    A[player][str(('R'+player,i))] = 2\n",
        "    A[player][str(('R'+other_player,i))] = 1\n",
        "\n",
        "# Define discount factor\n",
        "beta = 0.99\n",
        "\n",
        "# Define dtype\n",
        "dtype = torch.float32"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3p_qDLQ0kl3"
      },
      "source": [
        "# Define payoff map R\n",
        "R1 = np.array([[3.,0.],[5.,1.]])\n",
        "R2 = np.array([[3.,5.],[0.,1.]])\n",
        "\n",
        "def offer_accepted(action):\n",
        "  if action == 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def prisoner_dilemma_rewards(state, actions):\n",
        "  if 'G' in state:\n",
        "    r1 = R1[actions[0], actions[1]]\n",
        "    r2 = R2[actions[0], actions[1]]\n",
        "  elif 'O' in state:\n",
        "    r1 = r2 = 0.0\n",
        "  elif 'E1' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      r1 = offeral\n",
        "      r2 = -offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'E2' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      r1 = -offeral\n",
        "      r2 = offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'R1' in state:\n",
        "    _, i = state\n",
        "    r1 = R1[actions[0], i]\n",
        "    r2 = R2[actions[0], i]\n",
        "  elif 'R2' in state:\n",
        "    _, i = state\n",
        "    r1 = R1[i, actions[1]]\n",
        "    r2 = R2[i, actions[1]]\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state'\n",
        "  return r1, r2\n",
        "\n",
        "def prisoner_dilemma_reward_matrices(s):\n",
        "  if 'G' in s:\n",
        "    RM1 = R1.copy()\n",
        "    RM2 = R2.copy()\n",
        "  else:\n",
        "    N_A1 = A['1'][str(s)]\n",
        "    N_A2 = A['2'][str(s)]\n",
        "    if dtype == torch.float64:\n",
        "      RM1 = np.zeros((N_A1,N_A2), dtype=np.float64)\n",
        "      RM2 = np.zeros((N_A1,N_A2), dtype=np.float64)\n",
        "    else:\n",
        "      RM1 = np.zeros((N_A1,N_A2), dtype=np.float32)\n",
        "      RM2 = np.zeros((N_A1,N_A2), dtype=np.float32)\n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        r1, r2 = prisoner_dilemma_rewards(s, [a1,a2])\n",
        "        RM1[a1,a2] = r1\n",
        "        RM2[a1,a2] = r2\n",
        "  return RM1, RM2  \n",
        "\n",
        "R = {'1':{}, '2':{}}\n",
        "for s in S:\n",
        "  RM1, RM2 = prisoner_dilemma_reward_matrices(s)\n",
        "  if dtype == torch.float64:\n",
        "    R['1'][str(s)] = torch.DoubleTensor(RM1).clone()\n",
        "    R['2'][str(s)] = torch.DoubleTensor(RM2).clone()\n",
        "  else:\n",
        "    \n",
        "    R['1'][str(s)] = torch.FloatTensor(RM1).clone()\n",
        "    R['2'][str(s)] = torch.FloatTensor(RM2).clone()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEX15Bq4cyE"
      },
      "source": [
        "# Define transition map\n",
        "def prisoner_dilemma_transition_info(state, actions):\n",
        "  if 'G' in state or 'R1' in state or 'R2' in state:\n",
        "    T = [(('O',1), 0.5), (('O',2), 0.5)]\n",
        "    return ('R', T)\n",
        "  elif 'O' in state:\n",
        "    player_id = state[1]-1\n",
        "    other_player_id = 1 - player_id\n",
        "    other_player = str(other_player_id + 1)\n",
        "    offeral = actions[player_id] // 2\n",
        "    action_requested = actions[player_id] % 2\n",
        "    return ('D', ('E'+other_player, action_requested, offeral))\n",
        "  elif 'E1' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      return ('D', ('R2', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G',))\n",
        "  elif 'E2' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      return ('D', ('R1', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G',))   \n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state' \n",
        "\n",
        "prisoner_dilemma_transition_types = {}\n",
        "for state in S:\n",
        "  if 'G' in state or 'R1' in state or 'R2' in state:\n",
        "    prisoner_dilemma_transition_types[str(state)] = (0,0) \n",
        "  else:\n",
        "    prisoner_dilemma_transition_types[str(state)] = (1,1) # entries correspond to deterministic behaviour and dependence on actions\n",
        "\n",
        "T = (prisoner_dilemma_transition_info, prisoner_dilemma_transition_types)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BNAscRPo2x-"
      },
      "source": [
        "Now we initialize the game and optimize it with the Herskovits algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvdgNUPN510L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c8fc30-4713-4946-b25f-8380a841567c"
      },
      "source": [
        "game = multi_player_game(N, S, A, R, T, beta, device, dtype=dtype).to(device)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLuixBYJl64d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5487afbe-7d68-49fa-f58c-92d9e4690ea7"
      },
      "source": [
        "xx = game.optimize_game(verbose=False, n_epochs=200, print_each=20)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 19, f: 4.05400e+01, delta f: -5.86027e+01%, nash satisfied: True, max g: -2.063e-03, dual nash satisfied: False, max product: 1.298e+01, rho: 2.516e-04, norm2 d0:1.350e+01, ss: 1.372e-02\n",
            "Epoch: 19, >0 satisfied: True, max g>0: -1.065e-04, dual >0 satisfied: False, max product >0: 3.879e-01, =1 satisfied: True, max g=1: -1.322e-01, dual =1 satisfied: False, max product =1: 9.791e+00\n",
            "Epoch: 39, f: 3.31161e+01, delta f: -6.61837e+01%, nash satisfied: True, max g: -2.084e-04, dual nash satisfied: False, max product: 4.636e+01, rho: 1.240e-04, norm2 d0:3.245e+01, ss: 4.809e-03\n",
            "Epoch: 39, >0 satisfied: True, max g>0: -2.084e-05, dual >0 satisfied: False, max product >0: 7.223e-01, =1 satisfied: True, max g=1: -1.003e-01, dual =1 satisfied: False, max product =1: 2.076e+01\n",
            "Epoch: 59, f: 3.17409e+01, delta f: -6.75879e+01%, nash satisfied: True, max g: -9.632e-05, dual nash satisfied: False, max product: 1.130e+02, rho: 1.240e-04, norm2 d0:6.179e+01, ss: 9.513e-04\n",
            "Epoch: 59, >0 satisfied: True, max g>0: -2.471e-05, dual >0 satisfied: False, max product >0: 2.222e+00, =1 satisfied: True, max g=1: -9.093e-02, dual =1 satisfied: False, max product =1: 3.578e+01\n",
            "Game 0 saved succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4grvUW3rowO7",
        "outputId": "22ae03f3-2f0e-4c07-83e6-77a9ff69b8b3"
      },
      "source": [
        "game0 = multi_player_game(N, S, A, R, T, beta, device, dtype=dtype).to(device)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dQXadbzjoig"
      },
      "source": [
        "grad_f_vector, grad_g_matrix = game0.build_grad_tensors(True)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWHp8Mdmjsx3",
        "outputId": "392867cd-3ff5-4d3e-d8c6-f2754316a3f5"
      },
      "source": [
        "print(grad_f_vector.shape, grad_g_matrix.shape)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([146, 1]) torch.Size([230, 146])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9gBjiYhnljZ"
      },
      "source": [
        "f = game0.calculate_bellman_error()\r\n",
        "grad_f_at_v, grad_f_at_pi = game0.bellman_error_gradients()\r\n",
        "grad_f_pr = game0.project_grad_on_equality_manifolds()\r\n",
        "grad_f_at_v_pr, grad_f_at_pi_pr = game0.vec2dic_noduals(grad_f_pr) \r\n",
        "step_size, n_iters_backtracking, f_after, found_step_size = game0.linear_backtracking(f, grad_f_at_v_pr, grad_f_at_pi_pr, max_steps=40)\r\n",
        "if found_step_size:\r\n",
        "  game1 = game0.copy_game()\r\n",
        "  game1.perform_gradient_descent_step(grad_f_at_v_pr, grad_f_at_pi_pr, step_size)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl24FNTDmhwv",
        "outputId": "05b9171e-0ae1-40db-cf4e-a128166d9637"
      },
      "source": [
        "n_iters_backtracking"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVo5Hg3t3Jgn",
        "outputId": "48c6ba87-8210-4198-c6f0-7aff043d4a26"
      },
      "source": [
        "game0.restriction_types.shape"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([230])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRvMDDm0kCNF",
        "outputId": "435d8254-6a5a-4c33-84a3-62598e8c538b"
      },
      "source": [
        "game0.active_restrictions()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PV0Q_XDkn8D",
        "outputId": "19bc5c09-4e12-4af9-b77d-79390d0b5f4a"
      },
      "source": [
        "game1.active_restrictions()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfqFnldFl3p-",
        "outputId": "514f12d3-e49c-4c22-88ea-753a211427fc"
      },
      "source": [
        "torch.all(game1.inactive_restrictions().bitwise_or(game0.active_restrictions()))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyTV85IKW5Zf",
        "outputId": "1c250144-1928-4a3c-d6f0-46e730cf1eb8"
      },
      "source": [
        "game.calculate_bellman_error()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(31.7293, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtld6lw6foi5"
      },
      "source": [
        "grad_f_at_x = game.bellman_error_gradient_vector()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58OPS8w6fvSG"
      },
      "source": [
        "grad_f_at_x.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvR47qJOXdyM"
      },
      "source": [
        "game.bellman_error_gradients()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbQl1oK9hZwT"
      },
      "source": [
        "mask = game.calculate_restriction_vector(True).T > -1e-4"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4Diufa83irA",
        "outputId": "b16c7b63-fe34-487b-b4d8-ef3ca5eb9ddf"
      },
      "source": [
        "mask"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,  True,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False, False,\n",
              "         False,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdFMJR39zpp6",
        "outputId": "737f1c8b-3403-4392-dab4-67ccb1811a8b"
      },
      "source": [
        "mask.bitwise_or(torch.bitwise_not(mask))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "         True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0AvXLx0iY5G"
      },
      "source": [
        "game1 = game.copy_game()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5SFnLi2idhn",
        "outputId": "43a07f96-60e0-4a73-8144-3619822741d5"
      },
      "source": [
        "game1.calculate_bellman_error()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(31.7293, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J01u_GSogPj"
      },
      "source": [
        "We can see that the algorithm does not converge to the global optima, where $f(v^*,\\pi^*)$ becomes 0 and $v^*$ corresponds precisely to the value of the game when the policy is $\\pi^*$. It might be interesting to study why the algorithm does not converge. With this mind, we will consider a simpler case with only one state.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3cBY8Y3oQLv"
      },
      "source": [
        "# Iterated Prisoner's Dilemma\n",
        "\n",
        "Since the version with incentives has 7 states that correspond to more than 100 variables, we will focus now on the original iterated version with only one state.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_dRfGBEoQLx"
      },
      "source": [
        "# Define player set N\n",
        "N = 2\n",
        "\n",
        "# Define state set S\n",
        "G = [('G',)]\n",
        "S = list(itertools.chain(G.copy())) \n",
        "\n",
        "# Define action set A^i(s) per player and state\n",
        "A = {'1':{}, '2':{}}\n",
        "for player_id in range(0, N):\n",
        "  player = str(player_id+1)\n",
        "  A[player][str(('G',))] = 2\n",
        "  \n",
        "# Define discount factor\n",
        "beta = 0.99\n",
        "\n",
        "# Define dtype\n",
        "dtype = torch.float32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQKCVzaAoQL0"
      },
      "source": [
        "# Define payoff map R\n",
        "R1 = np.array([[3.,0.],[5.,1.]])\n",
        "R2 = np.array([[3.,5.],[0.,1.]])\n",
        "\n",
        "def iterated_prisoner_dilemma_rewards(state, actions):\n",
        "  if 'G' in state:\n",
        "    r1 = R1[actions[0], actions[1]]\n",
        "    r2 = R2[actions[0], actions[1]]\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state'\n",
        "  return r1, r2\n",
        "\n",
        "def iterated_prisoner_dilemma_reward_matrices(s):\n",
        "  RM1 = R1.copy()\n",
        "  RM2 = R2.copy()\n",
        "  return RM1, RM2  \n",
        "\n",
        "R = {'1':{}, '2':{}}\n",
        "for s in S:\n",
        "  RM1, RM2 = iterated_prisoner_dilemma_reward_matrices(s)\n",
        "  if dtype == torch.float64:\n",
        "    R['1'][str(s)] = torch.DoubleTensor(RM1).clone()\n",
        "    R['2'][str(s)] = torch.DoubleTensor(RM2).clone()\n",
        "  else:\n",
        "    R['1'][str(s)] = torch.FloatTensor(RM1).clone()\n",
        "    R['2'][str(s)] = torch.FloatTensor(RM2).clone()\n",
        "\n",
        "# Define transition map\n",
        "def iterated_prisoner_dilemma_transition_map(state, actions):\n",
        "  return ('D', ('G',))\n",
        " \n",
        "iterated_prisoner_dilemma_transition_types = {str(('G',)): (1,1)}\n",
        "T = (iterated_prisoner_dilemma_transition_map, iterated_prisoner_dilemma_transition_types)\n",
        "\n",
        "name = 'PrisonersDilemmaIterated-v0'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1oxLxQZoQL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1269b758-252d-4889-f8a5-71918457eb1e"
      },
      "source": [
        "game_iterated = multi_player_game(N, S, A, R, T, beta, device, dtype=dtype, name=name).to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70_DxEsDoQL7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "8ad3b9a7-260f-42d1-a78d-b57ed38e55a8"
      },
      "source": [
        "game_iterated.optimize_game(verbose=False, n_epochs=300, print_each=50, eta_0=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 49, f: 1.65299e+00, delta f: -6.42302e+01%, nash satisfied: True, max g: -1.173e-04, dual nash satisfied: False, max product: 5.270e-02, rho: 1.708e-04, norm2 d0:1.343e+02, ss: 2.673e-02\n",
            "Epoch: 49, >0 satisfied: True, max g>0: -8.131e-05, dual >0 satisfied: False, max product >0: 4.166e-02, =1 satisfied: True, max g=1: -8.607e-02, dual =1 satisfied: False, max product =1: 6.686e+01\n",
            "Epoch: 99, f: 1.62729e+00, delta f: -6.47862e+01%, nash satisfied: True, max g: -6.294e-05, dual nash satisfied: False, max product: 4.357e-02, rho: 5.440e-05, norm2 d0:2.643e+02, ss: 2.009e-02\n",
            "Epoch: 99, >0 satisfied: True, max g>0: -1.963e-05, dual >0 satisfied: False, max product >0: 2.935e-02, =1 satisfied: True, max g=1: -6.139e-02, dual =1 satisfied: False, max product =1: 1.194e+02\n",
            "Epoch: 149, f: 1.58406e+00, delta f: -6.57217e+01%, nash satisfied: True, max g: -4.959e-05, dual nash satisfied: False, max product: 2.594e-02, rho: 1.864e-05, norm2 d0:3.883e+02, ss: 1.134e-02\n",
            "Epoch: 149, >0 satisfied: True, max g>0: -5.607e-06, dual >0 satisfied: False, max product >0: 3.040e-02, =1 satisfied: True, max g=1: -4.722e-02, dual =1 satisfied: False, max product =1: 1.816e+02\n",
            "Epoch: 199, f: 1.45796e+00, delta f: -6.84504e+01%, nash satisfied: True, max g: -1.431e-04, dual nash satisfied: False, max product: 1.202e-01, rho: 1.864e-05, norm2 d0:3.982e+01, ss: 1.046e-03\n",
            "Epoch: 199, >0 satisfied: True, max g>0: -3.035e-04, dual >0 satisfied: False, max product >0: 1.640e-01, =1 satisfied: True, max g=1: -2.915e-02, dual =1 satisfied: False, max product =1: 8.403e+01\n",
            "Epoch: 249, f: 1.39067e+00, delta f: -6.99065e+01%, nash satisfied: True, max g: -4.578e-05, dual nash satisfied: False, max product: 2.251e-02, rho: 5.559e-06, norm2 d0:7.569e+02, ss: 1.158e-16\n",
            "Epoch: 249, >0 satisfied: True, max g>0: -5.065e-06, dual >0 satisfied: False, max product >0: 2.145e-02, =1 satisfied: True, max g=1: -2.418e-02, dual =1 satisfied: False, max product =1: 3.490e+02\n",
            "Epoch: 299, f: 1.39067e+00, delta f: -6.99065e+01%, nash satisfied: True, max g: -4.578e-05, dual nash satisfied: False, max product: 2.251e-02, rho: 5.559e-06, norm2 d0:7.569e+02, ss: 1.158e-16\n",
            "Epoch: 299, >0 satisfied: True, max g>0: -5.065e-06, dual >0 satisfied: False, max product >0: 2.145e-02, =1 satisfied: True, max g=1: -2.418e-02, dual =1 satisfied: False, max product =1: 3.490e+02\n",
            "Game 0 saved succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9MwlBL3t2i2"
      },
      "source": [
        "\"Fortunately\", the algorithm also gets stucked in a local suboptima in the simpler case of only one state. Therefore we can investigate what is happening in this case and try to translate to the other one.\n",
        "\n",
        "First, we will see what is the policy discovered by the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHKhixv7uLYk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "0681b88f-d8ca-4b08-c674-490fcfa246d2"
      },
      "source": [
        "for pname, p in game_iterated.named_parameters():\n",
        "  print(pname, p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v Parameter containing:\n",
            "tensor([[28.7552, 28.7552]], requires_grad=True)\n",
            "pi1.('G',) Parameter containing:\n",
            "tensor([[5.0651e-06],\n",
            "        [9.7582e-01]], requires_grad=True)\n",
            "pi2.('G',) Parameter containing:\n",
            "tensor([[5.0651e-06],\n",
            "        [9.7582e-01]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}