{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stochastic_games_herkovitz_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarod13/Stochastic_Games/blob/master/stochastic_games_herkovitz_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5T17o-vEAdt"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPVt4jdFpH13"
      },
      "source": [
        "beta = 0.99\n",
        "\n",
        "M = 5\n",
        "N_P = 2\n",
        "N_A = 2\n",
        "N_S = 3 + N_P * (2*(M+1) + 2)\n",
        "G = [('G',)]\n",
        "O = [('O',i) for i in range(1,N_P+1)]\n",
        "E1 = [('E1',i,j) for i in range(0,N_P) for j in range(0,M+1)]\n",
        "E2 = [('E2',i,j) for i in range(0,N_P) for j in range(0,M+1)]\n",
        "R1 = [('R1',i) for i in range(0,N_P)]\n",
        "R2 = [('R2',i) for i in range(0,N_P)]\n",
        "S = list(itertools.chain(G, O, E1, E2, R1, R2)) \n",
        "S1 = list(itertools.chain(G, ('O',1), E1, R1))\n",
        "S2 = list(itertools.chain([('O',2), E2, R2]))\n",
        "\n",
        "RG1 = np.array([[3.,0.],[5.,1.]])\n",
        "RG2 = np.array([[3.,5.],[0.,1.]])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7b38kZDjVDF"
      },
      "source": [
        "def get_state_index(state):\n",
        "  if state in S:\n",
        "    return S.index(state)\n",
        "  elif state in S_str:\n",
        "    return S_str.index(state)\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state'\n",
        "\n",
        "def player_dim(i):\n",
        "  if i in [1, '1']:\n",
        "    return 'i'\n",
        "  elif i in [2, '2']:\n",
        "    return 'j'\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player id'\n",
        "  \n",
        "def get_player_id(player):\n",
        "  if player == '1':\n",
        "    return 0\n",
        "  elif player == '2':\n",
        "    return 1\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player'\n",
        "\n",
        "def players():\n",
        "  return range(1,2+1)\n",
        "\n",
        "def players_str():\n",
        "  return iter(['1','2']) \n",
        "\n",
        "def state_player_pairs():\n",
        "  return itertools.product(S, players())\n",
        "\n",
        "S_str = [str(s) for s in S]\n",
        "def state_player_str_pairs():\n",
        "  return itertools.product(S_str, players_str())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbtsHxmLpkaj"
      },
      "source": [
        "class full_game(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.pi1 = nn.ParameterDict()\n",
        "    self.pi1[str(('G',))] = Parameter(torch.Tensor(N_A,1))\n",
        "    nn.init.constant_(self.pi1[str(('G',))], 1.0/N_A)\n",
        "    self.pi1[str(('O',1))] = Parameter(torch.Tensor((M+1)*N_A,1))\n",
        "    nn.init.constant_(self.pi1[str(('O',1))], 1.0/((M+1)*N_A))    \n",
        "    self.pi1[str(('O',2))] = Parameter(torch.Tensor(1,1))\n",
        "    nn.init.constant_(self.pi1[str(('O',2))], 1.0)\n",
        "    \n",
        "    for state in E1:\n",
        "      self.pi1[str(state)] = Parameter(torch.Tensor(2,1))\n",
        "      nn.init.constant_(self.pi1[str(state)], 1.0/2.0)\n",
        "      \n",
        "    for state in E2 + R2:\n",
        "      self.pi1[str(state)] = Parameter(torch.Tensor(1,1))\n",
        "      nn.init.constant_(self.pi1[str(state)], 1.0)\n",
        "\n",
        "    for state in R1:\n",
        "      self.pi1[str(state)] = Parameter(torch.Tensor(N_A,1))\n",
        "      nn.init.constant_(self.pi1[str(state)], 1.0/N_A)\n",
        "\n",
        "    self.pi2 = nn.ParameterDict()\n",
        "    self.pi2[str(('G',))] = Parameter(torch.Tensor(N_A,1))\n",
        "    nn.init.constant_(self.pi2[str(('G',))], 1.0/N_A)\n",
        "    self.pi2[str(('O',1))] = Parameter(torch.Tensor(1,1))\n",
        "    nn.init.constant_(self.pi2[str(('O',1))], 1.0)\n",
        "    self.pi2[str(('O',2))] = Parameter(torch.Tensor((M+1)*N_A,1))\n",
        "    nn.init.constant_(self.pi2[str(('O',2))], 1.0/((M+1)*N_A))\n",
        "\n",
        "    for state in E2:\n",
        "      self.pi2[str(state)] = Parameter(torch.Tensor(2,1))\n",
        "      nn.init.constant_(self.pi2[str(state)], 1.0/2.0)\n",
        "      \n",
        "    for state in E1 + R1:\n",
        "      self.pi2[str(state)] = Parameter(torch.Tensor(1,1))\n",
        "      nn.init.constant_(self.pi2[str(state)], 1.0)\n",
        "\n",
        "    for state in R2:\n",
        "      self.pi2[str(state)] = Parameter(torch.Tensor(N_A,1))\n",
        "      nn.init.constant_(self.pi2[str(state)], 1.0/N_A)\n",
        "\n",
        "    self.v = Parameter(torch.Tensor(N_S,2))\n",
        "    nn.init.zeros_(self.v)    \n",
        "\n",
        "  def forward(self):\n",
        "    return self.pi(), self.v\n",
        "  \n",
        "  def pi(self):\n",
        "    pi = {'1': self.pi1, '2':self.pi2}\n",
        "    return pi\n",
        "  \n",
        "  def pi2vec(self):\n",
        "    pi_vector = {}\n",
        "    for player in players_str():\n",
        "      pi_list = []\n",
        "      for s in S_str:\n",
        "        if player == '1':\n",
        "          pi_list.append(self.pi1[s])\n",
        "        elif player == '2':\n",
        "          pi_list.append(self.pi2[s])\n",
        "      pi_vector[player] = torch.cat(pi_list, dim=0)\n",
        "    return pi_vector\n",
        "  \n",
        "  def pi_sum(self):\n",
        "    sum_vector = {}\n",
        "    for player in players_str():\n",
        "      sum_list = []\n",
        "      for s in S_str:\n",
        "        if player == '1':\n",
        "          sum_list.append(self.pi1[s].sum(0, keepdim=True))\n",
        "        elif player == '2':\n",
        "          sum_list.append(self.pi2[s].sum(0, keepdim=True))\n",
        "      sum_vector[player] = torch.cat(sum_list, dim=0)\n",
        "    return sum_vector\n",
        "\n",
        "def copy_game(game_original):\n",
        "  game_copy = full_game().to('cuda')\n",
        "  game_copy.load_state_dict(game_original.state_dict())\n",
        "  return game_copy"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_3Y5tpQGui5"
      },
      "source": [
        "game_b = full_game().to('cuda')\n",
        "\n",
        "N_A_S = {}\n",
        "N_A_S_tensors = {'1':[], '2':[]}\n",
        "N_A_S_tensors_reduced = {'1':[], '2':[]}\n",
        "N_A_total = {}\n",
        "N_A_reduced = {}\n",
        "N_S_reduced = {}\n",
        "with torch.no_grad():\n",
        "  pi, v = game_b()\n",
        "  for state in S:\n",
        "    N_A1_state = pi['1'][str(state)].shape[0]\n",
        "    N_A2_state = pi['2'][str(state)].shape[0]\n",
        "    N_A_S[str(state)] = {'1': N_A1_state, '2': N_A2_state}\n",
        "    N_A_S_tensors['1'].append(N_A1_state)\n",
        "    N_A_S_tensors['2'].append(N_A2_state)\n",
        "    if N_A1_state >= 2:\n",
        "      N_A_S_tensors_reduced['1'].append(N_A1_state)\n",
        "    if N_A2_state >= 2:\n",
        "      N_A_S_tensors_reduced['2'].append(N_A2_state)\n",
        "for player in players_str():\n",
        "  N_A_S_tensors[player] = torch.FloatTensor(N_A_S_tensors[player]).to('cuda').view(-1,1)\n",
        "  N_A_S_tensors_reduced[player] = torch.FloatTensor(N_A_S_tensors_reduced[player]).to('cuda').view(-1,1)\n",
        "  N_A_total[player] = int(N_A_S_tensors[player].sum().item())\n",
        "  N_A_reduced[player] = int(N_A_S_tensors_reduced[player].sum().item())\n",
        "  N_S_reduced[player] = N_A_S_tensors_reduced[player].view(-1).shape[0]\n",
        "\n",
        "n_restrictions = 0\n",
        "for player in players_str():\n",
        "  n_restrictions += N_A_total[player] + N_A_reduced[player] + N_S_reduced[player]\n",
        "n_vars = 2*N_S + N_A_reduced['1'] + N_A_reduced['2']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ChZjFIvavkh"
      },
      "source": [
        "def more_than_one_action(vec):\n",
        "  if vec.shape[0] > 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "def more_than_one_action_in_s(player,s):\n",
        "  if N_A_S[s][player] > 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def offer_accepted(action):\n",
        "  if action == 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "  \n",
        "\n",
        "def mask_inequality_restrictions(vec):\n",
        "  masked_vector = vec.clone()\n",
        "  masked_vector[-N_S_reduced['1']-N_S_reduced['2']:,:] = 0.0\n",
        "  return masked_vector\n",
        "\n",
        "def mask_equality_restrictions(vec):\n",
        "  masked_vector = vec.clone()\n",
        "  masked_vector[:-N_S_reduced['1']-N_S_reduced['2'],:] = 0.0\n",
        "  return masked_vector\n",
        "\n",
        "\n",
        "def other_player(i):\n",
        "  if i == 1:\n",
        "    return 2\n",
        "  elif i == 2:\n",
        "    return 1\n",
        "  elif i == '1':\n",
        "    return '2'\n",
        "  elif i == '2':\n",
        "    return '1'\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid player id'\n",
        "\n",
        "\n",
        "def transition_info(state, actions):\n",
        "  if 'G' in state or 'R1' in state or 'R2' in state:\n",
        "    T = [(('O',1), 0.5), (('O',2), 0.5)]\n",
        "    return ('R', T)\n",
        "  elif 'O' in state:\n",
        "    _, id_player = state\n",
        "    offeral = actions[id_player-1] // N_A\n",
        "    action_requested = actions[id_player-1] % N_A\n",
        "    return ('D', ('E'+str(other_player(id_player)), action_requested, offeral))\n",
        "  elif 'E1' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      return ('D', ('R2', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G',))\n",
        "  elif 'E2' in state:\n",
        "    _, action_requested, _ = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      return ('D', ('R1', action_requested))\n",
        "    else:\n",
        "      return ('D', ('G',))   \n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state' "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pjuaTnWdlwu"
      },
      "source": [
        "transition_types = {}\n",
        "for state in S:\n",
        "  if 'G' in state or 'R1' in state or 'R2' in state:\n",
        "    transition_types[str(state)] = (0,0) \n",
        "  else:\n",
        "    transition_types[str(state)] = (1,1) # entries correspond to deterministic behaviour and dependence on actions"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ANZA1cKdIE"
      },
      "source": [
        "def rewards(state, actions):\n",
        "  if 'G' in state:\n",
        "    r1 = RG1[actions[0], actions[1]]\n",
        "    r2 = RG2[actions[0], actions[1]]\n",
        "  elif 'O' in state:\n",
        "    r1 = r2 = 0.0\n",
        "  elif 'E1' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[0]):\n",
        "      r1 = offeral\n",
        "      r2 = -offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'E2' in state:\n",
        "    _, _, offeral = state\n",
        "    if offer_accepted(actions[1]):\n",
        "      r1 = -offeral\n",
        "      r2 = offeral\n",
        "    else:\n",
        "      r1 = r2 = 0\n",
        "  elif 'R1' in state:\n",
        "    _, i = state\n",
        "    r1 = RG1[actions[0], i]\n",
        "    r2 = RG2[actions[0], i]\n",
        "  elif 'R2' in state:\n",
        "    _, i = state\n",
        "    r1 = RG1[i, actions[1]]\n",
        "    r2 = RG2[i, actions[1]]\n",
        "  else:\n",
        "    assert 0 == 1, 'Invalid state'\n",
        "\n",
        "  return r1, r2\n",
        "\n",
        "\n",
        "def reward_matrices(s):\n",
        "  if 'G' in s:\n",
        "    RM1 = RG1.copy()\n",
        "    RM2 = RG2.copy()\n",
        "  else:\n",
        "    N_A1 = N_A_S[str(s)]['1']\n",
        "    N_A2 = N_A_S[str(s)]['2']\n",
        "    RM1 = np.zeros((N_A1,N_A2))\n",
        "    RM2 = np.zeros((N_A1,N_A2))\n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        r1, r2 = rewards(s, [a1,a2])\n",
        "        RM1[a1,a2] = r1\n",
        "        RM2[a1,a2] = r2\n",
        "  return RM1, RM2  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSMeXOcIT0Z"
      },
      "source": [
        "RM = {}\n",
        "RM['1'] = {}\n",
        "RM['2'] = {}\n",
        "for s in S:\n",
        "  RM1, RM2 = reward_matrices(s)\n",
        "  RM['1'][str(s)] = torch.FloatTensor(RM1).to('cuda')\n",
        "  RM['2'][str(s)] = torch.FloatTensor(RM2).to('cuda')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-oou8F-3U7D"
      },
      "source": [
        "def player_consistent_reward_matrices():\n",
        "  consistent_RM = {'1':{}, '2':{}}\n",
        "  for s in S_str:\n",
        "    consistent_RM['1'][s] = RM['1'][s].clone()\n",
        "    consistent_RM['2'][s] = torch.t(RM['2'][s].clone())  \n",
        "  return consistent_RM "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HFMfG9l3gso"
      },
      "source": [
        "def next_value_matrices(s, v): # TODO: consider other 2 cases\n",
        "  det, dep = transition_types[str(s)]\n",
        "  N_A1 = N_A_S[str(s)]['1']\n",
        "  N_A2 = N_A_S[str(s)]['2']\n",
        "  vs = torch.zeros((N_A1,N_A2,2)).to('cuda')\n",
        "  if det and dep:\n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        _, next_state = transition_info(s, [a1,a2])\n",
        "        vs[a1,a2,:] = v[get_state_index(next_state),:]\n",
        "  elif (not det) and (not dep):\n",
        "    _, transition_dic = transition_info(s, [])\n",
        "    next_v = torch.zeros(1,2).to('cuda')\n",
        "    for next_state, transition_prob in transition_dic:\n",
        "      next_v = next_v + v[get_state_index(next_state),:].view(1,-1) * transition_prob      \n",
        "    for a1 in range(0,N_A1):\n",
        "      for a2 in range(0,N_A2):\n",
        "        vs[a1,a2,:] = next_v.view(-1)\n",
        "  return vs\n",
        "\n",
        "def next_value_dictionary(v):\n",
        "  next_v_dic = {}\n",
        "  for s in S:\n",
        "    next_v_dic[str(s)] = next_value_matrices(s, v)\n",
        "  return next_v_dic\n",
        "\n",
        "\n",
        "def transition_matrix(pi): # TODO: consider other 2 cases\n",
        "  transition_matrix = torch.zeros((N_S,N_S)).to('cuda')\n",
        "  for state in S:\n",
        "    strategy_1 = pi['1'][str(state)]\n",
        "    strategy_2 = pi['2'][str(state)]\n",
        "\n",
        "    det, dep = transition_types[str(state)]\n",
        "    N_A1 = strategy_1.shape[0]\n",
        "    N_A2 = strategy_2.shape[0]\n",
        "    id_s = get_state_index(state)\n",
        "\n",
        "    if det and dep:\n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          _, next_state = transition_info(state, [a1,a2])\n",
        "          id_ns = get_state_index(next_state)\n",
        "          transition_prob = strategy_1[a1,0] * strategy_2[a2,0]\n",
        "          transition_matrix[id_s, id_ns] = transition_matrix[id_s, id_ns] + transition_prob\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = transition_info(state, [])\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        transition_matrix[id_s, get_state_index(next_state)] = transition_prob\n",
        "  return transition_matrix\n",
        "\n",
        "\n",
        "def partial_transition_matrices(pi): # TODO: consider other 2 cases\n",
        "  # Create dictionary of transition matrices for each state given\n",
        "  # the strategy of the other player \n",
        "  transition_matrices = {}\n",
        "  for s in S:\n",
        "    strategy_1 = pi['1'][str(s)]\n",
        "    strategy_2 = pi['2'][str(s)]\n",
        "    N_A1 = strategy_1.shape[0]\n",
        "    N_A2 = strategy_2.shape[0]\n",
        "    transition_matrices[str(s)] = {\n",
        "        '1': torch.zeros((N_A1,N_S)).to('cuda'),\n",
        "        '2': torch.zeros((N_A2,N_S)).to('cuda')\n",
        "    }\n",
        "\n",
        "    # Fill matrices with transition probabilities depending on the type\n",
        "    # of transition, i.e., if deterministic or random and independent or\n",
        "    # not on the actions\n",
        "    det, dep = transition_types[str(s)]\n",
        "    if det and dep:\n",
        "      for a1 in range(0,N_A1):\n",
        "        for a2 in range(0,N_A2):\n",
        "          _, next_state = transition_info(s, [a1,a2])\n",
        "          id_ns = get_state_index(next_state)\n",
        "          transition_prob1 = strategy_2[a2,0]\n",
        "          transition_prob2 = strategy_1[a1,0]\n",
        "          transition_matrices[str(s)]['1'][a1, id_ns] = (\n",
        "              transition_matrices[str(s)]['1'][a1, id_ns] + transition_prob1)\n",
        "          transition_matrices[str(s)]['2'][a2, id_ns] = (\n",
        "              transition_matrices[str(s)]['2'][a2, id_ns] + transition_prob2)\n",
        "    elif (not det) and (not dep):\n",
        "      _, transition_dic = transition_info(s, [])\n",
        "      for next_state, transition_prob in transition_dic:\n",
        "        transition_matrices[str(s)]['1'][:, get_state_index(next_state)] = transition_prob\n",
        "        transition_matrices[str(s)]['2'][:, get_state_index(next_state)] = transition_prob\n",
        "  return transition_matrices\n",
        "\n",
        "\n",
        "def expected_reward(RM, pi):\n",
        "  r_mean = torch.zeros((N_S,2)).to('cuda')\n",
        "  for i in range(1,2+1):\n",
        "    RM_i = RM[str(i)]\n",
        "    for s in S:\n",
        "      strategy_1 = pi['1'][str(s)]\n",
        "      strategy_2 = pi['2'][str(s)]\n",
        "      r_mean_1 = torch.einsum('ij,ik->jk', RM_i[str(s)], strategy_1)\n",
        "      r_mean[get_state_index(s),i-1] = (r_mean_1 * strategy_2).sum()\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def partial_expected_reward_other(RM, pi):\n",
        "  r_mean = {}\n",
        "  for player in players_str():\n",
        "    r_mean[player] = {}\n",
        "\n",
        "  for s, player in state_player_str_pairs():\n",
        "      N_A = N_A_S[s][player]\n",
        "      RM_i = RM[player]\n",
        "      other_player_ = other_player(player)\n",
        "      strategy = pi[other_player_][s].view(-1)\n",
        "      formula = 'ij,'+player_dim(other_player_)+'->'+player_dim(player)\n",
        "      r_mean[player][s] = torch.einsum(formula, RM_i[s], strategy).view(-1,1)\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def partial_expected_reward(RM, pi):\n",
        "  # Create reward dictionary for each combination of players\n",
        "  r_mean = {}\n",
        "  for player in players_str():  \n",
        "    r_mean[player] = {'1':{}, '2':{}}\n",
        "\n",
        "  # Calculate expected reward for combination of players wrt the policy of one of the players\n",
        "  for s, player in state_player_str_pairs():\n",
        "    other_player_ = other_player(player) # Player used to calculate expected reward\n",
        "    for second_player in players_str():  \n",
        "      N_A = N_A_S[s][second_player]\n",
        "      RM_i = RM[second_player] # Reward matrix for one of the players  \n",
        "      strategy = pi[other_player_][s].view(-1)\n",
        "      formula = 'ij,'+player_dim(other_player_)+'->'+player_dim(player)\n",
        "      r_mean[player][second_player][s] = torch.einsum(formula, RM_i[s], strategy).view(-1,1)\n",
        "  return r_mean\n",
        "\n",
        "\n",
        "def bellman_projection(RM, pi, v):\n",
        "  r_mean = expected_reward(RM, pi)\n",
        "  next_v = torch.zeros((N_S,2)).to('cuda')\n",
        "  for s in S:\n",
        "    next_state_value_matrix = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    next_value_1 = torch.einsum('ijk,i->jk', next_state_value_matrix, strategy_1)\n",
        "    next_v[get_state_index(s),:] = torch.einsum('jk,j->k', next_value_1, strategy_2)\n",
        "  return r_mean + beta * next_v\n",
        "\n",
        "\n",
        "def bellman_partial_projection_other(RM, pi, v):\n",
        "  r_mean = partial_expected_reward_other(RM, pi)\n",
        "  next_v = {'1':{}, '2':{}}\n",
        "  for s in S:\n",
        "    next_state_value_matrix = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    # mean next value when considering the strategy of the other player. Output: array of size m^i(s)\n",
        "    next_v['1'][str(s)] = torch.einsum('ij,j->i', next_state_value_matrix[:,:,0], strategy_2).view(-1,1)\n",
        "    next_v['2'][str(s)] = torch.einsum('ij,i->j', next_state_value_matrix[:,:,1], strategy_1).view(-1,1)\n",
        "  bellman_projection_dic = {'1':{}, '2':{}}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    bellman_projection_dic[str(player_id)][str(s)] = r_mean[str(player_id)][str(s)] + beta * next_v[str(player_id)][str(s)]\n",
        "  return bellman_projection_dic\n",
        "\n",
        "\n",
        "def partial_next_values(pi, v):\n",
        "  # Create next-value dictionary for each player combination\n",
        "  next_v = {'1':{}, '2':{}}\n",
        "  for player in players_str():\n",
        "    next_v[player] = {'1':{}, '2':{}}\n",
        "\n",
        "  # Fill dictionary \n",
        "  for s in S:\n",
        "    next_state_value_matrix = next_value_matrices(s, v)\n",
        "    strategy_1 = pi['1'][str(s)].squeeze(1)\n",
        "    strategy_2 = pi['2'][str(s)].squeeze(1)\n",
        "    \n",
        "    # Calculate mean next value when considering the strategy of one of the players. Output: array of size m^i(s)\n",
        "    for player_id in players():\n",
        "      next_v['1'][str(player_id)][str(s)] = torch.einsum('ij,j->i', next_state_value_matrix[:,:,player_id-1], strategy_2).view(-1,1)\n",
        "      next_v['2'][str(player_id)][str(s)] = torch.einsum('ij,i->j', next_state_value_matrix[:,:,player_id-1], strategy_1).view(-1,1)  \n",
        "  return next_v\n",
        "\n",
        "\n",
        "def bellman_partial_projection(RM, pi, v):\n",
        "  r_mean = partial_expected_reward(RM, pi)\n",
        "  next_v = partial_next_values(pi, v)\n",
        "\n",
        "  bellman_projection_dic = {'1':{}, '2':{}}\n",
        "  for player in players_str():\n",
        "    bellman_projection_dic[player] = {'1':{}, '2':{}}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    for second_player in players_str():\n",
        "      bellman_projection_dic[player][second_player][s] = (\n",
        "          r_mean[player][second_player][s] + beta * next_v[player][second_player][s]\n",
        "          )\n",
        "  return bellman_projection_dic\n",
        "\n",
        "\n",
        "def reward_baselines(RM, pi):\n",
        "  r_mean = expected_reward(RM, pi)\n",
        "  r_baseline = r_mean.mean(0).view(-1,1).detach().cpu().numpy()\n",
        "  return r_baseline"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-uCtqdo35yW"
      },
      "source": [
        "def cost_vector_fixed_policies(pi):\n",
        "  P = transition_matrix(pi)\n",
        "  cost_vector = (1 - beta * P.sum(0)).view(-1,1).detach().cpu().numpy() / N_S\n",
        "  return cost_vector\n",
        "\n",
        "def restriction_matrices_fixed_policies(pi):\n",
        "  transition_matrices = partial_transition_matrices(pi)\n",
        "  restriction_matrices = {'1':[], '2':[]}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    temp_matrix = - beta * transition_matrices[str(s)][str(player_id)]\n",
        "    temp_matrix[:, get_state_index(s)] = temp_matrix[:, get_state_index(s)] + 1\n",
        "    restriction_matrices[str(player_id)].append(temp_matrix)\n",
        "  for player in players_str():\n",
        "    restriction_matrices[player] = -torch.cat(restriction_matrices[player], dim=0).detach().cpu().numpy()\n",
        "  return restriction_matrices\n",
        "\n",
        "def restriction_vectors_fixed_policies(RM, pi, alpha=0.1):\n",
        "  r_mean = partial_expected_reward_other(RM, pi)\n",
        "  restriction_vectors = {'1':[], '2':[]}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    restriction_vectors[player].append(r_mean[player][s].view(-1,1))\n",
        "  for player in players_str():\n",
        "    restriction_vectors[player] = -(torch.cat(restriction_vectors[player], dim=0)+alpha).detach().cpu().numpy()\n",
        "  return restriction_vectors\n",
        "\n",
        "\n",
        "def parameters_fixed_policies(game_, alpha):\n",
        "  pi = game_()[0]\n",
        "  c = cost_vector_fixed_policies(pi)\n",
        "  f0 = reward_baselines(RM, pi)\n",
        "  A_ub = restriction_matrices_fixed_policies(pi)\n",
        "  b_ub = restriction_vectors_fixed_policies(RM, pi, alpha)\n",
        "  return c, f0, A_ub, b_ub\n",
        "\n",
        "\n",
        "def calculate_initial_v(game_, alpha=10):\n",
        "  v0 = np.zeros((N_S,2))\n",
        "  c, f0, A_ub, b_ub = parameters_fixed_policies(game_, alpha=alpha)\n",
        "  for player_id in players():\n",
        "    temp_res = linprog(c, A_ub=A_ub[str(player_id)], b_ub=b_ub[str(player_id)])\n",
        "    v0[:,player_id-1] = temp_res.x\n",
        "  return v0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbMxPbhy3s_p"
      },
      "source": [
        "def calculate_nash_restrictions(pi, v):\n",
        "  q_estimated = bellman_partial_projection_other(RM, pi, v) # Dic. with an array of 'q'-values for each agent \n",
        "  g_nash = {'1':{}, '2':{}}\n",
        "  for s, player_id in state_player_pairs():\n",
        "    g_nash[str(player_id)][str(s)] = q_estimated[str(player_id)][str(s)] - v[get_state_index(s), player_id-1]\n",
        "  return g_nash\n",
        "\n",
        "\n",
        "def calculate_bellman_error(pi, v):\n",
        "  # Calculation of original target function: Bellman approximation error \n",
        "  v_estimated = bellman_projection(RM, pi, v)\n",
        "  f_bellman = (v - v_estimated).sum()\n",
        "  return f_bellman\n",
        "\n",
        "\n",
        "def check_nash_KKT_conditions(pi, v, lambda_nash, tol=1e-8):\n",
        "  with torch.no_grad():\n",
        "    # Calculate restrictions\n",
        "    g_nash = calculate_nash_restrictions(pi, v)\n",
        "\n",
        "    g_nash_satisfied = True\n",
        "    product_zero_satisfied = True\n",
        "    \n",
        "    max_g_nash = -np.infty\n",
        "    max_product_zero = -np.infty\n",
        "    \n",
        "    for player in players_str():\n",
        "      remaining_duals = lambda_nash[player].clone()\n",
        "      for s in S_str:\n",
        "        NA = N_A_S[s][player]\n",
        "        g_nash_satisfied = g_nash_satisfied and torch.all(g_nash[player][s] <= 0)\n",
        "        max_g_nash = max(max_g_nash, g_nash[player][s].max())\n",
        "\n",
        "        lambda_g_nash_product = g_nash[player][s].view(-1) * remaining_duals[:NA,:].view(-1)\n",
        "        product_zero_satisfied = product_zero_satisfied and torch.all(lambda_g_nash_product.abs() <= tol)\n",
        "        max_product_zero = max(max_product_zero, lambda_g_nash_product.abs().max())\n",
        "        remaining_duals = remaining_duals[NA:,:]\n",
        "    return g_nash_satisfied, max_g_nash, product_zero_satisfied, max_product_zero\n",
        "\n",
        "\n",
        "def check_pi_KKT_conditions(pi_vector, pi_sum_vector, duals, tol=1e-8):\n",
        "  with torch.no_grad():\n",
        "    g_pi_plus_satisfied = True\n",
        "    g_pi_one_satisfied = True\n",
        "\n",
        "    product_zero_plus_satisfied = True\n",
        "    product_zero_one_satisfied = True\n",
        "\n",
        "    max_g_pi_plus = -np.infty\n",
        "    max_g_pi_one = -np.infty\n",
        "\n",
        "    max_product_zero_plus = -np.infty\n",
        "    max_product_zero_one = -np.infty\n",
        "\n",
        "    for player in players_str():\n",
        "      g_pi_plus_satisfied = g_pi_plus_satisfied and torch.all(pi_vector[player] >= 0)\n",
        "      g_pi_one_satisfied = g_pi_one_satisfied and torch.all(pi_sum_vector[player] <= 1)\n",
        "\n",
        "      max_g_pi_plus = max(max_g_pi_plus, -pi_vector[player].min().item())\n",
        "      max_g_pi_one = max(max_g_pi_one, (pi_sum_vector[player]-1).min().item())\n",
        "\n",
        "      NAt = N_A_total[player]\n",
        "      NAr = N_A_reduced[player]\n",
        "      lambda_g_pi_plus_product = -pi_vector[player].view(-1) * duals[player][NAt:NAt+NAr,:].view(-1) # TODO: fix for assymetric number of actions\n",
        "      product_zero_plus_satisfied = product_zero_plus_satisfied and torch.all(lambda_g_pi_plus_product.abs() <= tol)\n",
        "      max_product_zero_plus = max(max_product_zero_plus, lambda_g_pi_plus_product.abs().max())\n",
        "      lambda_g_pi_one_product = (pi_sum_vector[player].view(-1)-1) * duals[player][NAt+NAr:,:].view(-1)\n",
        "      product_zero_one_satisfied = product_zero_one_satisfied and torch.all(lambda_g_pi_one_product.abs() <= tol)\n",
        "      max_product_zero_one = max(max_product_zero_one, lambda_g_pi_one_product.abs().max())\n",
        "   \n",
        "    return (g_pi_plus_satisfied, max_g_pi_plus, product_zero_plus_satisfied, max_product_zero_plus, \n",
        "            g_pi_one_satisfied, max_g_pi_one, product_zero_one_satisfied, max_product_zero_one)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMPtyopfi3fC"
      },
      "source": [
        "def bellman_error_gradients(pi, v):\n",
        "  P = transition_matrix(pi)\n",
        "  bellman_error_grad_v = (torch.eye(N_S).to('cuda') - torch.t(P)).sum(1, keepdim=True)\n",
        "\n",
        "  q_individual = bellman_partial_projection(RM, pi, v)\n",
        "  bellman_error_grad_pi = {'1':{}, '2':{}}\n",
        "  for s, player in state_player_str_pairs():\n",
        "    bellman_error_grad_pi[player][s] = 0.0\n",
        "    for second_player in players_str():\n",
        "      bellman_error_grad_pi[player][s] = bellman_error_grad_pi[player][s] - q_individual[player][second_player][s]  \n",
        "  return bellman_error_grad_v, bellman_error_grad_pi\n",
        "\n",
        "def nash_restriction_gradients(pi, v):\n",
        "  P_partial = partial_transition_matrices(pi)\n",
        "  next_value_dic = next_value_dictionary(v)\n",
        "  consistent_RM = player_consistent_reward_matrices()\n",
        "\n",
        "  nash_restriction_grad_v = {'1':{}, '2':{}}\n",
        "  nash_restriction_grad_pi = {'1':{}, '2':{}}\n",
        "\n",
        "  for s, player in state_player_str_pairs():\n",
        "    Delta = torch.zeros_like(P_partial[s][player])\n",
        "    Delta[:, get_state_index(s)] = 1.0\n",
        "    nash_restriction_grad_v[player][s] = beta * P_partial[s][player]  - Delta\n",
        "  \n",
        "    other_player_ = other_player(player)\n",
        "    next_value_matrix = next_value_dic[s][:,:,get_player_id(other_player_)]\n",
        "    r_matrix = consistent_RM[other_player_][s]\n",
        "    if player == '1':\n",
        "      next_value_matrix = torch.t(next_value_matrix)\n",
        "    nash_restriction_grad_pi[player][s] = r_matrix + beta * next_value_matrix\n",
        "  return nash_restriction_grad_v, nash_restriction_grad_pi\n",
        "\n",
        "\n",
        "def grad_bellman_v2vec(grad_f_v, grad_f_pi):\n",
        "  n_vars = 2*N_S + N_A_reduced['1'] + N_A_reduced['2']\n",
        "  grad_f_vector = torch.zeros(n_vars, 1).to('cuda')\n",
        "  \n",
        "  for i in range(0,2):\n",
        "    grad_f_vector[i*N_S:(i+1)*N_S,:] = grad_f_v.clone()\n",
        "  \n",
        "  for player in players_str():\n",
        "    grad_f_pi_list = []\n",
        "    for s in S_str:\n",
        "      if more_than_one_action_in_s(player, s):\n",
        "        grad_f_pi_list.append(grad_f_pi[player][s].clone())  \n",
        "    grad_f_pi_vector = torch.cat(grad_f_pi_list, dim=0)\n",
        "    y0 = 2*N_S + get_player_id(player) * N_A_reduced['1']\n",
        "    yf = 2*N_S + N_A_reduced['1'] + get_player_id(player) * N_A_reduced['2']\n",
        "    grad_f_vector[y0:yf,:] = grad_f_pi_vector\n",
        "\n",
        "  return grad_f_vector\n",
        "\n",
        "def grad_bellman_pi2mat(player, grad_g_pi):\n",
        "  grad_g_pi_list = []\n",
        "  s_not_added_list = []\n",
        "  for s in S_str:\n",
        "    if more_than_one_action_in_s(player, s):\n",
        "      grad = grad_g_pi[player][s].clone()\n",
        "      if len(s_not_added_list) == 0:\n",
        "        grad_g_pi_list.append(grad)\n",
        "      else:\n",
        "        n_null_rows = 0\n",
        "        for s_ in s_not_added_list:\n",
        "          n_null_rows += N_A_S[s_][other_player_]\n",
        "        n_rows = n_null_rows + grad.shape[0]\n",
        "        n_cols = grad.shape[1]\n",
        "        null_rows_and_grad_g_pi = torch.zeros(n_rows, n_cols).to('cuda')\n",
        "        null_rows_and_grad_g_pi[-n_rows+n_null_rows:,:] = grad\n",
        "        grad_g_pi_list.append(null_rows_and_grad_g_pi)\n",
        "        s_not_added_list = []\n",
        "    else:\n",
        "      s_not_added_list.append(s)\n",
        "  if len(s_not_added_list) != 0:\n",
        "    n_null_rows = 0\n",
        "    for s_ in s_not_added_list:\n",
        "      n_null_rows += N_A_S[s_][other_player_]\n",
        "    n_rows = n_null_rows + grad_g_pi_list[-1].shape[0]\n",
        "    n_cols = grad_g_pi_list[-1].shape[1]\n",
        "    null_rows_and_grad_g_pi = torch.zeros(n_rows, n_cols).to('cuda')\n",
        "    null_rows_and_grad_g_pi[:n_rows-n_null_rows,:] = grad_g_pi_list[-1].clone()\n",
        "    grad_g_pi_list[-1] = null_rows_and_grad_g_pi\n",
        "  \n",
        "  J_gmi_pit = torch.block_diag(*grad_g_pi_list)\n",
        "  return J_gmi_pit\n",
        "\n",
        "def build_grad_tensors(grad_f_v, grad_f_pi, grad_g_v, grad_g_pi):\n",
        "\n",
        "  grad_f_vector = grad_bellman_v2vec(grad_f_v, grad_f_pi)\n",
        "    \n",
        "  grad_g_matrix = torch.zeros(n_restrictions, n_vars).to('cuda')\n",
        "  \n",
        "  for player in players_str():\n",
        "    other_player_ = other_player(player)\n",
        "\n",
        "    # fill jacobian with restriction gradients for v\n",
        "    grad_g_v_list = [grad_g_v[player][s].clone() for s in S_str]\n",
        "    J_gi_vi = torch.cat(grad_g_v_list, dim=0)\n",
        "    y0 = get_player_id(player) * N_A_total['1']\n",
        "    yf = N_A_total['1'] + get_player_id(player) * N_A_total['2']\n",
        "    x0 = get_player_id(player) * N_S\n",
        "    xf = x0 + N_S\n",
        "    grad_g_matrix[y0:yf,x0:xf] = J_gi_vi\n",
        "\n",
        "    # fill jacobian with nash restriction gradients for pi \n",
        "    J_gmi_pit = grad_bellman_pi2mat(player, grad_g_pi)\n",
        "    y0 = get_player_id(other_player_) * N_A_total['1']\n",
        "    yf = N_A_total['1'] + get_player_id(other_player_) * N_A_total['2']\n",
        "    x0 = 2*N_S + get_player_id(player) * N_A_reduced['1']\n",
        "    xf = 2*N_S + N_A_reduced['1'] + get_player_id(player) * N_A_reduced['2']\n",
        "    grad_g_matrix[y0:yf,x0:xf] = J_gmi_pit\n",
        "\n",
        "    # fill jacobian with positivity restriction gradients for pi \n",
        "    J_gpi_plus = -torch.eye(N_A_reduced[player]).to('cuda')\n",
        "    y0 = N_A_total['1'] + N_A_total['2'] + get_player_id(player) * N_A_reduced['1']\n",
        "    yf = (N_A_total['1'] + N_A_total['2'] + N_A_reduced['1'] \n",
        "          + get_player_id(player) * N_A_reduced['2'])\n",
        "    grad_g_matrix[y0:yf,x0:xf] = J_gpi_plus\n",
        "\n",
        "    # fill jacobian with unitary sum restriction gradients for pi\n",
        "    grad_gpi_one_list = []\n",
        "    for s in S_str:\n",
        "      NA = N_A_S[s][player]\n",
        "      if more_than_one_action_in_s(player, s):\n",
        "        grad_gpi_one_list.append(torch.ones(1,NA).to('cuda'))\n",
        "\n",
        "    J_gpi_one = torch.block_diag(*grad_gpi_one_list)\n",
        "    y0 = (N_A_total['1'] + N_A_total['2'] + N_A_reduced['1'] \n",
        "          + N_A_reduced['2'] + get_player_id(player) * N_S_reduced['1'])\n",
        "    yf = (N_A_total['1'] + N_A_total['2'] + N_A_reduced['1'] + N_A_reduced['2'] \n",
        "          + N_S_reduced['1'] + get_player_id(player) * N_S_reduced['2'])\n",
        "    grad_g_matrix[y0:yf,x0:xf] = J_gpi_one\n",
        "\n",
        "  return grad_f_vector, grad_g_matrix\n",
        "\n",
        "\n",
        "def g_dics2vec(g_nash, pi_vector, pi_sum_vector):\n",
        "  g_list = []\n",
        "  for player in players_str():\n",
        "    for s in S_str:\n",
        "      g_list.append(g_nash[player][s])\n",
        "  for player in players_str():\n",
        "    g_list.append(-pi_vector[player])\n",
        "  for player in players_str():\n",
        "    g_list.append(pi_sum_vector[player]-1)\n",
        "  g_vector = torch.cat(g_list, dim=0)\n",
        "  return g_vector\n",
        "\n",
        "\n",
        "def calculate_descent_direction(g_vector, grad_f_vector, grad_g_matrix):\n",
        "  g_vector_ineq = mask_inequality_restrictions(g_vector)\n",
        "  g_vector_eq = mask_equality_restrictions(g_vector)\n",
        "  g_diag_matrix = torch.diag(r_value * g_vector_ineq.view(-1))\n",
        "\n",
        "  A_matrix = g_diag_matrix - torch.einsum('ik,jk->ij', grad_g_matrix, grad_g_matrix) \n",
        "  b_vector = torch.einsum('ij,jk->ik', grad_g_matrix, grad_f_vector) - g_vector_eq \n",
        "  duals_0_vector = torch.solve(b_vector, A_matrix)[0]\n",
        "\n",
        "  d0_vector = - grad_f_vector - torch.einsum('ij,ik->jk', grad_g_matrix, duals_0_vector)\n",
        "  norm_2_d0 = d0_vector.pow(2).sum().item() \n",
        "  return d0_vector, norm_2_d0, duals_0_vector, A_matrix, b_vector\n",
        "\n",
        "\n",
        "def calculate_feasible_direction(g_vector, c_vector, duals_0_vector, \n",
        "                                 norm_2_d0, A_matrix, b_vector, rho,\n",
        "                                  grad_f_vector, grad_g_matrix):\n",
        "  b_unitary = torch.solve(torch.ones_like(b_vector), A_matrix)[0]\n",
        "  g_vector_eq = mask_equality_restrictions(g_vector)\n",
        "  dot = (g_vector_eq.view(-1) * b_unitary.view(-1)).sum()\n",
        "\n",
        "  div = (duals_0_vector.sum() + c_vector.sum() - dot).item()\n",
        "  if div > 0:\n",
        "      rho_1 = (1-alpha) / div\n",
        "      if rho_1 < rho:\n",
        "        rho = 0.5 * rho_1\n",
        "\n",
        "  duals_vector = torch.solve(b_vector - rho * norm_2_d0, A_matrix)[0]\n",
        "  d_vector = - grad_f_vector - torch.einsum('ij,ik->jk', grad_g_matrix, duals_vector)\n",
        "  return d_vector, duals_vector, rho\n",
        "\n",
        "\n",
        "def calculate_auxiliary_bellman_error(f, g_vector, c_vector):\n",
        "  return f - (c_vector.view(-1) * g_vector.view(-1)).sum()\n",
        "\n",
        "\n",
        "def feasible_gradient_descent(game_0, f, g_vector, c_vector, d_vector,\n",
        "                              grad_f_vector, grad_g_matrix, d_v, d_pi, \n",
        "                              duals_vector, max_steps=1200, \n",
        "                              verbose=False):\n",
        "  # Calculate loss function\n",
        "  theta = calculate_auxiliary_bellman_error(f, g_vector, c_vector)\n",
        "  \n",
        "  # Calculate required decrement in the loss function\n",
        "  grad_theta_vector = grad_f_vector + torch.einsum('ij,ik->jk', grad_g_matrix, c_vector)\n",
        "  decrement = (d_vector.view(-1) * grad_theta_vector.view(-1)).sum().item()\n",
        "\n",
        "  # Procedure to find feasible step size \n",
        "  found_feasible_step_size = False\n",
        "  step_size = 1.0\n",
        "  n_step = 0\n",
        "  max_steps = max(1, max_steps)\n",
        "  while (not found_feasible_step_size) and (n_step < max_steps):\n",
        "    game_temp = copy_game(game_0)\n",
        "    # Update parameters performing step in feasible descent direction \n",
        "    game_temp.v.data.add_(step_size * d_v)\n",
        "    for s in S_str:\n",
        "      if more_than_one_action_in_s(player='1', s):\n",
        "        game_temp.pi1[s].data.add_(step_size * d_pi['1'][s])\n",
        "      if more_than_one_action_in_s(player='2', s):\n",
        "        game_temp.pi2[s].data.add_(step_size * d_pi['2'][s])\n",
        "\n",
        "    pi_temp, v_temp = game_temp()    \n",
        "    pi_vector_temp = game_temp.pi_vector()\n",
        "    pi_sum_vector_temp = game_temp.pi_sum_vector()\n",
        "    f_temp = calculate_bellman_error(pi_temp, v_temp)\n",
        "    g_nash_temp = calculate_nash_restrictions(pi_temp, v_temp)\n",
        "    g_vector_temp = g_dics2g_vec(g_nash, pi_vector_temp, pi_sum_vector_temp)\n",
        "    theta_temp = calculate_auxiliary_bellman_error(f_temp, g_vector_temp, c_vector)\n",
        "\n",
        "    gamma = gamma_0 * torch.ones_like(duals_vector)\n",
        "    gamma[duals_vector < 0] = 1.0\n",
        "    gamma[-N_S_reduced['1']-N_S_reduced['2'],:] = 0.0\n",
        "\n",
        "    theta_decreased = theta_temp <= theta + eta * step_size * decrement\n",
        "    g_v_valid = torch.all(g_vector_temp[:114] <= (g_vector * gamma)[:114])\n",
        "    g_pi_valid = torch.all(g_vector_temp[114:] <= (g_vector * gamma)[114:])\n",
        "    g_valid = g_v_valid and g_pi_valid\n",
        "    \n",
        "    # Check if the current step size results in a feasible descent direction\n",
        "    if theta_decreased and g_valid:\n",
        "      found_feasible_step_size = True\n",
        "      \n",
        "    step_size /= nu\n",
        "    n_step += 1\n",
        "    if verbose:\n",
        "      print(f_decreased.item(), g_v_valid.item(), g_pi_valid.item())\n",
        "      print(\"Step: {}, Step size: {:.3e}, Found feasible: {}\".format(\n",
        "          n_step, step_size, found_feasible_step_size))\n",
        "  return game_temp, found_feasible_step_size, n_step, f_temp\n",
        "\n",
        "\n",
        "def vec2dic(d_vector, duals_vector):\n",
        "  d_v = torch.zeros((N_S,2)).to('cuda')  \n",
        "  d_pi = {'1':{}, '2':{}}\n",
        "  duals = {}\n",
        "\n",
        "  for i in range(0,2):\n",
        "    d_v[:,i] = d_vector[i*N_S:(i+1)*N_S,:].view(-1)\n",
        "  \n",
        "  N_A_total = {}\n",
        "  N_S_reduced = {}\n",
        "  for player in players_str():\n",
        "    N_A_total[player] = int(N_A_S_tensors[player].sum().item()) \n",
        "    N_S_reduced[player] = N_A_S_tensors_reduced[player].view(-1).shape[0]\n",
        "\n",
        "  for player in players_str():\n",
        "    y0 = 2*N_S + get_player_id(player) * (N_A_total['1'] - N_S)\n",
        "    yf = N_S + N_A_total['1']  + get_player_id(player) * (N_A_total['2'] - N_S) \n",
        "    d_pi_vector = d_vector[y0:yf,:]\n",
        "    n = 0\n",
        "    for s in S_str:\n",
        "      NA = N_A_S[s][player]\n",
        "      if NA > 1:\n",
        "        d_pi[player][s] = d_pi_vector[n:n+NA-1,:]\n",
        "      else:\n",
        "        d_pi[player][s] = None\n",
        "      n = n + NA-1\n",
        "    \n",
        "  for player in players_str():\n",
        "    i = get_player_id(player)\n",
        "    other_player_ = other_player(player)\n",
        "    n_restrictions = 2*N_A_total[player]-N_S+N_S_reduced[player]\n",
        "    duals[player] = torch.zeros(n_restrictions,1).to('cuda')\n",
        "    \n",
        "    y0 = get_player_id(player) * N_A_total['1']\n",
        "    yf = N_A_total['1']  + get_player_id(player) * N_A_total['2'] \n",
        "    duals[player][:N_A_total[player],:] = duals_vector[y0:yf,:]\n",
        "    \n",
        "    y0 = N_A_total['1'] + N_A_total['2'] + get_player_id(player) * (N_A_total['1'] - N_S)\n",
        "    yf = 2*N_A_total['1'] + N_A_total['2'] - N_S + get_player_id(player) * (N_A_total['2'] - N_S) \n",
        "    duals[player][N_A_total[player]:2*N_A_total[player]-N_S,:] = duals_vector[y0:yf,:]\n",
        "\n",
        "    y0 = 2*(N_A_total['1'] + N_A_total['2'] - N_S) + get_player_id(player) * N_S_reduced['1']\n",
        "    yf = 2*(N_A_total['1'] + N_A_total['2'] - N_S) + N_S_reduced['1'] + get_player_id(player) * N_S_reduced['2'] \n",
        "    duals[player][-N_S_reduced[player]:,:] = duals_vector[y0:yf,:]\n",
        "\n",
        "  return d_v, d_pi, duals\n",
        "\n",
        "\n",
        "def update_c_vector(c_vector, duals_0_vector):\n",
        "  new_c_vector = c_vector.clone()\n",
        "  entries_to_update = c_vector < -1.2*duals_0_vector\n",
        "  new_c_vector[entries_to_update] = -2*duals_0_vector[entries_to_update]\n",
        "  new_c_vector = mask_equality_restrictions(new_c_vector)\n",
        "  return new_c_vector\n",
        "\n",
        "\n",
        "def optimize_game(game_0, rho, n_epochs=100, verbose=False):\n",
        "  game_new = copy_game(game_0)\n",
        "  pi_0, v_0 = game_0()\n",
        "  f_0 = calculate_bellman_error(pi_0, v_0)\n",
        "  ttype = game_new.transform_type\n",
        "  c_vector = torch.ones(n_restrictions,1).to('cuda')\n",
        "  c_vector = mask_equality_restrictions(c_vector)\n",
        "  with torch.no_grad():\n",
        "    for epoch in range(0, n_epochs):\n",
        "      pi, v = game_new()\n",
        "      pi_vector = game_new.pi_vector()\n",
        "      pi_sum_vector = game_new.pi_sum_vector()\n",
        "      f = calculate_bellman_error(pi, v)\n",
        "      g_nash = calculate_nash_restrictions(pi, v)\n",
        "      g_vector = g_dics2g_vec(g_nash, pi_vector, pi_sum_vector)\n",
        "      grad_f_v, grad_f_pi = bellman_error_gradients(pi, v)\n",
        "      grad_g_v, grad_g_pi = nash_restriction_gradients(pi, v)\n",
        "      \n",
        "      grad_f_vector, grad_g_matrix = build_grad_tensors(grad_f_v, grad_f_pi, grad_g_v, grad_g_pi)\n",
        "      d0_vector, norm_2_d0, duals_0_vector, A_matrix, b_vector = calculate_descent_direction(g_vector, grad_f_vector, grad_g_matrix)\n",
        "      c_vector = update_c_vector(c_vector, duals_0_vector)\n",
        "\n",
        "      d_vector, duals_vector, rho = calculate_feasible_direction(g_vector, c_vector, duals_0_vector, \n",
        "                                                                 norm_2_d0, A_matrix, b_vector, rho, \n",
        "                                                                 grad_f_vector, grad_g_matrix)\n",
        "      d_v, d_pi, duals = vec2dic(d_vector, duals_vector)\n",
        "\n",
        "      g_nash_satisfied, max_g_nash, product_zero_satisfied, max_product_zero = check_nash_KKT_conditions(pi, v, duals)\n",
        "      if ttype == 'nolast':\n",
        "        pi_KKT = check_pi_KKT_conditions(pi_vector, pi_sum_vector, duals)\n",
        "        g_pi_plus_satisfied, max_g_pi_plus, product_zero_plus_satisfied, max_product_zero_plus = pi_KKT[:4] \n",
        "        g_pi_one_satisfied, max_g_pi_one, product_zero_one_satisfied, max_product_zero_one = pi_KKT[4:]\n",
        "      game_temp, found_feasible_step_size, n_step, f_new = feasible_gradient_descent(game_new, f, g_vector, c_vector, d_vector, \n",
        "                                                                                     grad_f_vector, grad_g_matrix, d_v, d_pi, \n",
        "                                                                                     duals_vector, verbose=verbose) # TODO: check if passing wrong grad_f\n",
        "      if found_feasible_step_size:\n",
        "        game_new = game_temp\n",
        "        delta_f = (f_new - f_0) / f_0 * 100\n",
        "        print('Epoch: {}, f: {:.3e}, delta f: {:.3e}%, nash satisfied: {}, max g: {:.3e}, dual nash satisfied: {}, max product: {:.3e}, rho: {:.3e}, norm2 d0:{:.3e}'.format(\n",
        "            epoch, f_new.item(), delta_f.item(), g_nash_satisfied, max_g_nash.item(), product_zero_satisfied, max_product_zero.item(), rho, norm_2_d0.item()))\n",
        "        if ttype == 'nolast':\n",
        "          print('Epoch: {}, >0 satisfied: {}, max g>0: {:.3e}, dual >0 satisfied: {}, max product >0: {:.3e}, =1 satisfied: {}, max g=1: {:.3e}, dual =1 satisfied: {}, max product =1: {:.3e}'.format(\n",
        "            epoch, g_pi_plus_satisfied, max_g_pi_plus.item(), product_zero_plus_satisfied, max_product_zero_plus.item(), g_pi_one_satisfied, max_g_pi_one.item(), product_zero_one_satisfied, max_product_zero_one.item()))\n",
        "      else:\n",
        "        break\n",
        "        \n",
        "  return game_new, rho     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbiikizT9KyT"
      },
      "source": [
        "#**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLlRZTiQAgWJ"
      },
      "source": [
        "game_b = nolast_game().to('cuda')\n",
        "\n",
        "v0 = calculate_initial_v(game_b, alpha=0.01)\n",
        "v0 = torch.FloatTensor(v0).to('cuda')\n",
        "with torch.no_grad():\n",
        "  game_b.v.data.add_(v0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYaXbFwlYldO"
      },
      "source": [
        "alpha = 0.5\n",
        "gamma_0 = 0.5\n",
        "eta = 0.01\n",
        "nu = 1/0.98\n",
        "rho_0 = 1.0\n",
        "rho = rho_0\n",
        "r_value = 1.0\n",
        "c = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GpIbXdaVkt9"
      },
      "source": [
        "game_b2, rho = optimize_game(game_b, rho, n_epochs=100, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}